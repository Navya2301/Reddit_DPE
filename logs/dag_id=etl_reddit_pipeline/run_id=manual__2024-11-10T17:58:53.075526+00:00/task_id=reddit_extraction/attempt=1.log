[2024-11-10T17:58:55.321+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-11-10T17:58:53.075526+00:00 [queued]>
[2024-11-10T17:58:55.329+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-11-10T17:58:53.075526+00:00 [queued]>
[2024-11-10T17:58:55.330+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-11-10T17:58:55.338+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-11-10 17:58:53.075526+00:00
[2024-11-10T17:58:55.346+0000] {standard_task_runner.py:57} INFO - Started process 66 to run task
[2024-11-10T17:58:55.350+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-11-10T17:58:53.075526+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpxbz8sin1']
[2024-11-10T17:58:55.352+0000] {standard_task_runner.py:85} INFO - Job 6: Subtask reddit_extraction
[2024-11-10T17:58:55.416+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-11-10T17:58:53.075526+00:00 [running]> on host 97f6bbe1811b
[2024-11-10T17:58:55.476+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Navya Racha' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-11-10T17:58:53.075526+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-10T17:58:53.075526+00:00'
[2024-11-10T17:58:55.480+0000] {logging_mixin.py:151} WARNING - Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.
[2024-11-10T17:58:55.483+0000] {logging_mixin.py:151} INFO - Connected to Reddit!
[2024-11-10T17:58:56.098+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I want to thank this community for putting pressure on me to not be so greedy and share my knowledge more freely.\n\nLaunch video with all the details is here: [https://youtu.be/myhe0LXpCeo](https://youtu.be/myhe0LXpCeo)  \nMore details of how to join will be added to [https://www.github.com/DataExpert-io/data-engineer-handbook](https://www.github.com/DataExpert-io/data-engineer-handbook) soon!\n\nStarting on November 15th, I'll be publishing a new education video nearly every day until the end of the year as an end-of-2024 gift!\n\nThings we'll cover:  \n\\- Data modeling (fact data modeling, one big table, STRUCTS/ARRAYs, dimensional modeling)\n\n\\- Data quality patterns with Airflow like write-audit-publish\n\n\\- Unit and end-to-end testing PySpark jobs with Chispa\n\n\\- Writing Apache Flink jobs that connect to Kafka and do complex windowing\n\n\\- Data visualization with Tableau\n\n\\- Data pipeline maintenance (how to create good runbooks)\n\n\\- Analytical Patterns with Postgres (such as Facebook growth accounting)\n\n\\- Advanced window functions with Postgres and SQL\n\nThe content of these videos is from the boot camp I delivered in July 2023.\n\nIt will be six weeks of in depth content and I'm excited to deliver the value to y'all.", 'author_fullname': 't2_2xnlhe', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Launching a free six-week data engineering boot camp on YouTube on November 15th!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnor9c', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 226, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 226, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731200196.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I want to thank this community for putting pressure on me to not be so greedy and share my knowledge more freely.</p>\n\n<p>Launch video with all the details is here: <a href="https://youtu.be/myhe0LXpCeo">https://youtu.be/myhe0LXpCeo</a><br/>\nMore details of how to join will be added to <a href="https://www.github.com/DataExpert-io/data-engineer-handbook">https://www.github.com/DataExpert-io/data-engineer-handbook</a> soon!</p>\n\n<p>Starting on November 15th, I&#39;ll be publishing a new education video nearly every day until the end of the year as an end-of-2024 gift!</p>\n\n<p>Things we&#39;ll cover:<br/>\n- Data modeling (fact data modeling, one big table, STRUCTS/ARRAYs, dimensional modeling)</p>\n\n<p>- Data quality patterns with Airflow like write-audit-publish</p>\n\n<p>- Unit and end-to-end testing PySpark jobs with Chispa</p>\n\n<p>- Writing Apache Flink jobs that connect to Kafka and do complex windowing</p>\n\n<p>- Data visualization with Tableau</p>\n\n<p>- Data pipeline maintenance (how to create good runbooks)</p>\n\n<p>- Analytical Patterns with Postgres (such as Facebook growth accounting)</p>\n\n<p>- Advanced window functions with Postgres and SQL</p>\n\n<p>The content of these videos is from the boot camp I delivered in July 2023.</p>\n\n<p>It will be six weeks of in depth content and I&#39;m excited to deliver the value to y&#39;all.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?auto=webp&s=b0adf7d2276d6a58828e73eafe09842c80780be2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?width=108&crop=smart&auto=webp&s=f04f252a26818db41a554cd4150ea00fb371b6b0', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?width=216&crop=smart&auto=webp&s=d9684c27d7001da10b1b1dd44ce5e237059aa5f6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?width=320&crop=smart&auto=webp&s=4504d7c2526166102859cb61bd39c7fef9c1e527', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'hNjQunzaVA63XjM8vSu890db6RFHDnzbJTTbsUcf-Ws'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnor9c', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='eczachly'), 'discussion_type': None, 'num_comments': 80, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnor9c/launching_a_free_sixweek_data_engineering_boot/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnor9c/launching_a_free_sixweek_data_engineering_boot/', 'subreddit_subscribers': 227819, 'created_utc': 1731200196.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.099+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Wrapping up my series of getting into Data Engineering. Two images attached, three core expertise and roadmap. You may have to check the initial article here to understand my perspective: https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&utm_campaign=post&utm_medium=web\n\nData Analyst can naturally move by focusing on overlapping areas and grow and make more $$$.\n\nEach time I shared roadmap for SWE or DS or now DA, they all focus on the core areas to make it easy transition.\n\nRoadmaps are hard to come up with, so I made some choices and wrote about here: https://www.junaideffendi.com/p/transition-data-analyst-to-data-engineer?r=cqjft&utm_campaign=post&utm_medium=web\n\nIf you have something in mind, comment please.', 'author_fullname': 't2_dhgy4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'is_gallery': True, 'title': 'Analyst to Engineer ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 81, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'scti5cnhx00e1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 131, 'x': 108, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=108&crop=smart&auto=webp&s=0c782a904dd73cf85cb93f7a1d71b07bb9a980da'}, {'y': 262, 'x': 216, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=216&crop=smart&auto=webp&s=b191a47688d6f208327773777c318e43d3218abe'}, {'y': 388, 'x': 320, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=320&crop=smart&auto=webp&s=869d83b1e77d8ab8558b853e00a53984427af252'}, {'y': 777, 'x': 640, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=640&crop=smart&auto=webp&s=b431fd971942894b1194e06617d0efa648da0489'}, {'y': 1166, 'x': 960, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=960&crop=smart&auto=webp&s=914779bb304eddb8ac0d6f79529cd3f4d7b819dd'}, {'y': 1312, 'x': 1080, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=1080&crop=smart&auto=webp&s=43d8d7687d45fc7e1a512dec747f921d97869236'}], 's': {'y': 1546, 'x': 1272, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=1272&format=pjpg&auto=webp&s=74363aae3a88aeeacf17031aad159c52df32f004'}, 'id': 'scti5cnhx00e1'}, 't52hs6ihx00e1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 62, 'x': 108, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=108&crop=smart&auto=webp&s=9cae61004721cfedcdd870dc389248ce5dd74d53'}, {'y': 125, 'x': 216, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=216&crop=smart&auto=webp&s=8a25512250a981a0d06dac62558b40d20eb1b538'}, {'y': 185, 'x': 320, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=320&crop=smart&auto=webp&s=c741ab4492d4eca31e00f37585557a3ba675c478'}, {'y': 371, 'x': 640, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=640&crop=smart&auto=webp&s=27dab8d36bb3fd277f1fac72f856263a2d46bf5e'}, {'y': 556, 'x': 960, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=960&crop=smart&auto=webp&s=3916aba11d5c6d257014252abb902b8d1026cc1b'}, {'y': 626, 'x': 1080, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=1080&crop=smart&auto=webp&s=24a98ffcbacae4492176253cc8d544c0b35a7e86'}], 's': {'y': 738, 'x': 1272, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=1272&format=pjpg&auto=webp&s=0ff1f689535c3089fe0a94e3b919f157c6a7c3ea'}, 'id': 't52hs6ihx00e1'}}, 'name': 't3_1gnv2oy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'ups': 77, 'domain': 'reddit.com', 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'gallery_data': {'items': [{'caption': '', 'media_id': 't52hs6ihx00e1', 'id': 548987668}, {'caption': '', 'media_id': 'scti5cnhx00e1', 'id': 548987669}]}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 77, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/nVEIuQJOD--nVzD3ICW--beqDFjV3IiGXlrT6kG387c.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731222553.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'total_awards_received': 0, 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Wrapping up my series of getting into Data Engineering. Two images attached, three core expertise and roadmap. You may have to check the initial article here to understand my perspective: <a href="https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web">https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web</a></p>\n\n<p>Data Analyst can naturally move by focusing on overlapping areas and grow and make more $$$.</p>\n\n<p>Each time I shared roadmap for SWE or DS or now DA, they all focus on the core areas to make it easy transition.</p>\n\n<p>Roadmaps are hard to come up with, so I made some choices and wrote about here: <a href="https://www.junaideffendi.com/p/transition-data-analyst-to-data-engineer?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web">https://www.junaideffendi.com/p/transition-data-analyst-to-data-engineer?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web</a></p>\n\n<p>If you have something in mind, comment please.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.reddit.com/gallery/1gnv2oy', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnv2oy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mjfnd'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnv2oy/analyst_to_engineer/', 'stickied': False, 'url': 'https://www.reddit.com/gallery/1gnv2oy', 'subreddit_subscribers': 227819, 'created_utc': 1731222553.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.100+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey guys,\n\nI know this has been discussed to hell and back, but most in-depth discussions I could find were from at least a year ago so I\'m raising this up again.\n\nThe company I work at uses Fivetran for ELT from a bunch of sources to our Snowflake warehouse. Honestly, we have a variety of sources but nothing as important as the PostgreSQL databases of all our internal services.\n\nWhile Fivetran rarely has issues (but not never), we feel the cost of the contract is not worth the service we\'re getting back as their customer: support is shitty and often just dismisses us without helping, their "logarithmic scale pricing" always seems to be in their benefit and not ours, we\'re consistently growing our contract with them year-over-year and always find ourselves in overage, and those are just the highlights.\n\n  \nI started looking into alternatives and found Airbyte. On paper, the OSS deployment looks like everything I dreamed of, but reading a bit here - many people mentioned it was not robust enough for production.\n\nI\'m in contact with Rivery, but have also heard a mixed bag of opinions from colleagues.\n\nI\'ve explored Meltano several years ago, and have a bad memory of them but it might not be based on anything relevant anymore.\n\ndlt also came up, but I know very little except that I\'ll need to handle orchestration (?).\n\n  \nAre these the best options I have for robust, reliable, production-ready ELT solutions? What did I get wrong or miss? Please help.\n\n', 'author_fullname': 't2_o5ws9', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tired of paying Fivetran, what are reliable production-ready alternatives?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnipx1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 54, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 54, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731182916.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey guys,</p>\n\n<p>I know this has been discussed to hell and back, but most in-depth discussions I could find were from at least a year ago so I&#39;m raising this up again.</p>\n\n<p>The company I work at uses Fivetran for ELT from a bunch of sources to our Snowflake warehouse. Honestly, we have a variety of sources but nothing as important as the PostgreSQL databases of all our internal services.</p>\n\n<p>While Fivetran rarely has issues (but not never), we feel the cost of the contract is not worth the service we&#39;re getting back as their customer: support is shitty and often just dismisses us without helping, their &quot;logarithmic scale pricing&quot; always seems to be in their benefit and not ours, we&#39;re consistently growing our contract with them year-over-year and always find ourselves in overage, and those are just the highlights.</p>\n\n<p>I started looking into alternatives and found Airbyte. On paper, the OSS deployment looks like everything I dreamed of, but reading a bit here - many people mentioned it was not robust enough for production.</p>\n\n<p>I&#39;m in contact with Rivery, but have also heard a mixed bag of opinions from colleagues.</p>\n\n<p>I&#39;ve explored Meltano several years ago, and have a bad memory of them but it might not be based on anything relevant anymore.</p>\n\n<p>dlt also came up, but I know very little except that I&#39;ll need to handle orchestration (?).</p>\n\n<p>Are these the best options I have for robust, reliable, production-ready ELT solutions? What did I get wrong or miss? Please help.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnipx1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='IdoNisso'), 'discussion_type': None, 'num_comments': 73, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1gnipx1/tired_of_paying_fivetran_what_are_reliable/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnipx1/tired_of_paying_fivetran_what_are_reliable/', 'subreddit_subscribers': 227819, 'created_utc': 1731182916.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.100+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I do my visualization in power bi I pull from SQL data base doing most of my transformations before I get to power query. So I do case statements, trims, joins, destincts, and loops. \n\nI'm curious, how do you use SQL? I feel like y'all's SQL is at  whole other level, like I don't even know how much I don't know. ", 'author_fullname': 't2_6n7xko94', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "I'm a data analyst, how is how I use SQL different than how you use it? ", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnsq5o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 18, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731213447.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I do my visualization in power bi I pull from SQL data base doing most of my transformations before I get to power query. So I do case statements, trims, joins, destincts, and loops. </p>\n\n<p>I&#39;m curious, how do you use SQL? I feel like y&#39;all&#39;s SQL is at  whole other level, like I don&#39;t even know how much I don&#39;t know. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1gnsq5o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='whiskey_rue'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnsq5o/im_a_data_analyst_how_is_how_i_use_sql_different/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnsq5o/im_a_data_analyst_how_is_how_i_use_sql_different/', 'subreddit_subscribers': 227819, 'created_utc': 1731213447.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.101+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have a phone screen coming for a Meta DE IC3 role in 4 weeks. From what I've seen online and in Meta's preparation material. They give you a schema with 3-5 tables and no sample records. To successfully pass the SQL section of the phone screen, I would have to solve atleast 3 SQL mediums in 25 minutes.\n\nMy average time on StrataScratch Medium (Meta Tagged) is around 10-12 minutes. In an ideal situation, that time has to be under 8-9 minutes for each question.\n\nMy question is, how do I structure my problem before I start coding. I seem to get 'lost in the sauce' or lose focus if I keep reading the question for long. Is there a structure I can follow that would cut down my querying time?\n\nMy current gameplan:\n\n1. Identify tables\n2. Identify relationships needed for question\n3. Identify Grouping, Aggregations etc\n4. Implement.\n\nI am able to get past the first 3 steps but the 4th step gets difficult where I feel like I don't know where to start. And once I see the solution, I feel so stupid because 80-90% of the time I was on the right track and just didn't have the light bulb moment.\n\n  \nTLDR - What mental pattern should I use to solve SQL medium/hard under 8 minutes?", 'author_fullname': 't2_129bsj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do I build intuition for Complex SQL questions?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnnjd5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731196523.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have a phone screen coming for a Meta DE IC3 role in 4 weeks. From what I&#39;ve seen online and in Meta&#39;s preparation material. They give you a schema with 3-5 tables and no sample records. To successfully pass the SQL section of the phone screen, I would have to solve atleast 3 SQL mediums in 25 minutes.</p>\n\n<p>My average time on StrataScratch Medium (Meta Tagged) is around 10-12 minutes. In an ideal situation, that time has to be under 8-9 minutes for each question.</p>\n\n<p>My question is, how do I structure my problem before I start coding. I seem to get &#39;lost in the sauce&#39; or lose focus if I keep reading the question for long. Is there a structure I can follow that would cut down my querying time?</p>\n\n<p>My current gameplan:</p>\n\n<ol>\n<li>Identify tables</li>\n<li>Identify relationships needed for question</li>\n<li>Identify Grouping, Aggregations etc</li>\n<li>Implement.</li>\n</ol>\n\n<p>I am able to get past the first 3 steps but the 4th step gets difficult where I feel like I don&#39;t know where to start. And once I see the solution, I feel so stupid because 80-90% of the time I was on the right track and just didn&#39;t have the light bulb moment.</p>\n\n<p>TLDR - What mental pattern should I use to solve SQL medium/hard under 8 minutes?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnnjd5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Tam27_'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1gnnjd5/how_do_i_build_intuition_for_complex_sql_questions/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnnjd5/how_do_i_build_intuition_for_complex_sql_questions/', 'subreddit_subscribers': 227819, 'created_utc': 1731196523.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.101+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I think I might be completely misunderstanding the logic behind DuckDB, I would appreciate if anyone could tell me if it\'s even possible/valid use case to use DuckDB as an "OLAP engine" for analytical queries on top of Postgres?\n\n\nI have a Flask application that is collecting data in the background and I need to show some charts on the data, I have long heard about DuckDB so today I thought I would give it a try - the results were promising, a query that took 30+ seconds to run was done in  3 (that\'s good enough for my purposes). \n\nI am using the \'***\' extension and the ATTACH  statement, everything works fine if there is only one query running at a time, but if I do 2 requests at ~ same time, one of them crashes. In the latest implementation I was using duckdb_engnine and I\'m getting the following error:\n\n> TransactionContext Error: Catalog write-write conflict on create with "***"\n\nI tried :memory: and on disk, does not make a difference.\n\n\nAm I trying to fit a square peg into a round hole?', 'author_fullname': 't2_9d1jjuxh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Using DuckDB in a Web Application to run on top of Postgres - help out a duckdb newbie', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gni1g4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731181062.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I think I might be completely misunderstanding the logic behind DuckDB, I would appreciate if anyone could tell me if it&#39;s even possible/valid use case to use DuckDB as an &quot;OLAP engine&quot; for analytical queries on top of Postgres?</p>\n\n<p>I have a Flask application that is collecting data in the background and I need to show some charts on the data, I have long heard about DuckDB so today I thought I would give it a try - the results were promising, a query that took 30+ seconds to run was done in  3 (that&#39;s good enough for my purposes). </p>\n\n<p>I am using the &#39;***&#39; extension and the ATTACH  statement, everything works fine if there is only one query running at a time, but if I do 2 requests at ~ same time, one of them crashes. In the latest implementation I was using duckdb_engnine and I&#39;m getting the following error:</p>\n\n<blockquote>\n<p>TransactionContext Error: Catalog write-write conflict on create with &quot;***&quot;</p>\n</blockquote>\n\n<p>I tried :memory: and on disk, does not make a difference.</p>\n\n<p>Am I trying to fit a square peg into a round hole?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gni1g4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='boggle_thy_mind'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gni1g4/using_duckdb_in_a_web_application_to_run_on_top/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gni1g4/using_duckdb_in_a_web_application_to_run_on_top/', 'subreddit_subscribers': 227819, 'created_utc': 1731181062.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.102+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I am building a data platform following medallion architecture on Microsoft Fabric. The data ingestion is being done using Fabric Data Factory and I am saving data in its original formats such as JSON, CSV, Parquet (data from relational databases is saved as parquet files). The folder strcutre is as <system>/<table or endpoint>/yyyy/mm/dd/job\\_id/files\n\nThe problem I'm facing is related to data sources (especially Rest APIs) where there are no clues for incremental loads (no updateDate, createDate, or increasing numeric columns etc.). Loading full data everytime is going to explode storage size, as the ingestion process is intended to run very frequently (potentially every 5-10 minutes).\n\nHow have you tackled this problem in the past as a data architect?", 'author_fullname': 't2_421mrc99', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bronze layer and full table loads', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnervi', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731172263.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am building a data platform following medallion architecture on Microsoft Fabric. The data ingestion is being done using Fabric Data Factory and I am saving data in its original formats such as JSON, CSV, Parquet (data from relational databases is saved as parquet files). The folder strcutre is as &lt;system&gt;/&lt;table or endpoint&gt;/yyyy/mm/dd/job_id/files</p>\n\n<p>The problem I&#39;m facing is related to data sources (especially Rest APIs) where there are no clues for incremental loads (no updateDate, createDate, or increasing numeric columns etc.). Loading full data everytime is going to explode storage size, as the ingestion process is intended to run very frequently (potentially every 5-10 minutes).</p>\n\n<p>How have you tackled this problem in the past as a data architect?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnervi', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='khalid_amin'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnervi/bronze_layer_and_full_table_loads/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnervi/bronze_layer_and_full_table_loads/', 'subreddit_subscribers': 227819, 'created_utc': 1731172263.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.103+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am looking for an orchestrator for my usecase and came across Apache Airflow. But I am not sure if it is the right choice. Here are the essential requirements -\n\n1. The system is supposed to serve 100K - 1M requests per day.\n2. Each request requires downstream calls to different external dependencies which are dynamically decided at runtime. The calls to these dependencies are structured like a DAG. Lets call these dependency calls as ‘jobs’.\n3. The dependencies process their jobs asynchronously and return response via SNS. The average turnaround time is 1 minute.\n4. The dependencies throw errors indicating that their job limit is reached. In these cases, we have to queue the jobs for that dependency until we receive a response from them indicating that capacity is now available.\n5. We are constrained on the job processing capacities of our dependencies and want maximum utilization. Hence, we want to schedule the next job as soon as we receive a response from that particular dependency. In other words, we want to minimize latency between job scheduling.\n6. We should have the capability to retry failed tasks / jobs / DAGsand monitor the reasons behind their failure.\n\nBonus -\n1. The system would have to keep 100K+ requests in queue at anytime due to the nature of our dependencies. So, it would be great if we can process these requests in order so that a request is not starved because of random scheduling.\n\nI have designed a solution using Lambdas with a MySQL DB to schedule the jobs and process them in order. But it would be great to understand if Airflow can be used as a tool for our usecase. \n\nFrom what I understand, I might have to create a Dynamic DAG at runtime for each of my requests with each of my dependency calls being subtasks. How good is Airflow at keeping 100K - 1M DAGs? \n\nAssuming that a Lambda receives the SNS response from the dependencies, can it go modify a DAG’s task indicating that it is now ready to move forward? And also trigger a retry to serially schedule new jobs for that specific dependency?\n\nFor the ordering logic, I read that DAGs can have dependencies on each other. Is there no other way to schedule tasks?\n\nHeres the scheduling logic I want to implement -\nIf a dependency has available capacity, pick the earliest created DAG which has pending job for that depenency and process it.', 'author_fullname': 't2_9b3176xo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is Airflow the right choice for running 100K - 1M dynamic workflows everyday?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnwoj2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731229619.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am looking for an orchestrator for my usecase and came across Apache Airflow. But I am not sure if it is the right choice. Here are the essential requirements -</p>\n\n<ol>\n<li>The system is supposed to serve 100K - 1M requests per day.</li>\n<li>Each request requires downstream calls to different external dependencies which are dynamically decided at runtime. The calls to these dependencies are structured like a DAG. Lets call these dependency calls as ‘jobs’.</li>\n<li>The dependencies process their jobs asynchronously and return response via SNS. The average turnaround time is 1 minute.</li>\n<li>The dependencies throw errors indicating that their job limit is reached. In these cases, we have to queue the jobs for that dependency until we receive a response from them indicating that capacity is now available.</li>\n<li>We are constrained on the job processing capacities of our dependencies and want maximum utilization. Hence, we want to schedule the next job as soon as we receive a response from that particular dependency. In other words, we want to minimize latency between job scheduling.</li>\n<li>We should have the capability to retry failed tasks / jobs / DAGsand monitor the reasons behind their failure.</li>\n</ol>\n\n<p>Bonus -\n1. The system would have to keep 100K+ requests in queue at anytime due to the nature of our dependencies. So, it would be great if we can process these requests in order so that a request is not starved because of random scheduling.</p>\n\n<p>I have designed a solution using Lambdas with a MySQL DB to schedule the jobs and process them in order. But it would be great to understand if Airflow can be used as a tool for our usecase. </p>\n\n<p>From what I understand, I might have to create a Dynamic DAG at runtime for each of my requests with each of my dependency calls being subtasks. How good is Airflow at keeping 100K - 1M DAGs? </p>\n\n<p>Assuming that a Lambda receives the SNS response from the dependencies, can it go modify a DAG’s task indicating that it is now ready to move forward? And also trigger a retry to serially schedule new jobs for that specific dependency?</p>\n\n<p>For the ordering logic, I read that DAGs can have dependencies on each other. Is there no other way to schedule tasks?</p>\n\n<p>Heres the scheduling logic I want to implement -\nIf a dependency has available capacity, pick the earliest created DAG which has pending job for that depenency and process it.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnwoj2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Tricky-Button-197'), 'discussion_type': None, 'num_comments': 23, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnwoj2/is_airflow_the_right_choice_for_running_100k_1m/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnwoj2/is_airflow_the_right_choice_for_running_100k_1m/', 'subreddit_subscribers': 227819, 'created_utc': 1731229619.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.104+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi !\nI've been a DE for 5 years now, for 2 différent companies. I've always worked on the 'backend' side: ETL/ELT, a bit of devops, modelization, sometimes making datamarts when analysts couldnt do it but thats pretty much it. I've never or very rarely been involved directly with any business process.\nI've read here that what makes a good DE is to be more 'business' oriented.\n\nAs I'm starting a new job in 2 months, I would like to implement some changes from the start. \n\nCan someone give me some tips about this ? What does this mean in practice ? Is there any book/ressource that would help with this ?\n\nThanks", 'author_fullname': 't2_ymuaigmkq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "How do I get more 'business' oriented ?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go2tqz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731251726.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi !\nI&#39;ve been a DE for 5 years now, for 2 différent companies. I&#39;ve always worked on the &#39;backend&#39; side: ETL/ELT, a bit of devops, modelization, sometimes making datamarts when analysts couldnt do it but thats pretty much it. I&#39;ve never or very rarely been involved directly with any business process.\nI&#39;ve read here that what makes a good DE is to be more &#39;business&#39; oriented.</p>\n\n<p>As I&#39;m starting a new job in 2 months, I would like to implement some changes from the start. </p>\n\n<p>Can someone give me some tips about this ? What does this mean in practice ? Is there any book/ressource that would help with this ?</p>\n\n<p>Thanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1go2tqz', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Fit-Needleworker6411'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go2tqz/how_do_i_get_more_business_oriented/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go2tqz/how_do_i_get_more_business_oriented/', 'subreddit_subscribers': 227819, 'created_utc': 1731251726.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.104+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '\nHi. I\'m an Architect on Microsoft\'s Fabric team and help drive the Real-time Intelligence platform pieces. A big theme of us is creating a more type-safe and productive environment for working with streaming data through broad support for schematized event payloads and CloudEvents. Our Eventstreams feature is an implementation of Azure Event Hubs (and thus also a Kafka API) embedded inside Fabric and the initiatives CNCF xRegistry and CNCF CloudEvents that we invest time in aim at event streaming in general.\n\nAvrotize is one of our useable and useful prototypes, a Rosetta Stone for data structure definitions, allowing you to convert between numerous data and database schema formats and to generate data transfer object code for different programming languages. \n\nIt is, for instance, a well-documented and predictable converter and code generator for data structures originally defined in JSON Schema (of arbitrary complexity).\n\nThe tool leans on the Apache Avro-derived Avrotize Schema as its schema model, extending Avro with several annotations. A formal spec is in the repo. The rationale for picking Avro is, simply, that any code-generator must resolve the chaos that is JSON Schema\'s $ref/anyOf/allOf/oneOf and unrestricted type unions and enums into type graph before emitting code. What I do with this tool is to capture that type graph in Avro Schema, which is a better foundation for code generation as it is always self-contained, limits the value space for identifiers, supports namespaces, and has a richer and extensible type system. The fact that you can drive a binary serializer with it is just a nice byproduct.\n\nData schema formats: Avro, JSON Schema, XML Schema (XSD), Protocol Buffers 2 and 3, ASN.1, Apache Parquet \nProgramming languages: Python, C#, Java, TypeScript, JavaScript, Rust, Go, C++\nSQL Databases: MySQL, MariaDB, PostgreSQL, SQL Server, Oracle, SQLite, BigQuery, Snowflake, Redshift, DB2\nOther databases: KQL/Kusto, MongoDB, Cassandra, Redis, Elasticsearch, DynamoDB, CosmosDB\n\nMind that the tool is not emitting code that does data conversion from/to all these data encodings and DBs. It converts the data structure declarations. If you want to work with GTFS-RT data, it\'s going to do a good job converting the Protobuf structures to Avro and onwards into JSON Schema, taking all the enums and doc comments along for the ride.\n\nHowever, the generated data transfer objects can obviously be used with your favorite ORM tool and the code generators emit annotations for JSON and Avro serializers (plus XML in C#)\n\nFeedback and collaboration welcome.\n\n(VS Code Extension available as "Avrotize" in the Marketplace)', 'author_fullname': 't2_6ggr7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Avrotize: A "Rosetta stone" to convert data(-base) schemas to/from/via Apache Avro Schema', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnwx7n', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/nd8RnZ4T0thB9WkLlcfLEulv1T7PfcCp_g2f3PC2xNU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731230687.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'github.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi. I&#39;m an Architect on Microsoft&#39;s Fabric team and help drive the Real-time Intelligence platform pieces. A big theme of us is creating a more type-safe and productive environment for working with streaming data through broad support for schematized event payloads and CloudEvents. Our Eventstreams feature is an implementation of Azure Event Hubs (and thus also a Kafka API) embedded inside Fabric and the initiatives CNCF xRegistry and CNCF CloudEvents that we invest time in aim at event streaming in general.</p>\n\n<p>Avrotize is one of our useable and useful prototypes, a Rosetta Stone for data structure definitions, allowing you to convert between numerous data and database schema formats and to generate data transfer object code for different programming languages. </p>\n\n<p>It is, for instance, a well-documented and predictable converter and code generator for data structures originally defined in JSON Schema (of arbitrary complexity).</p>\n\n<p>The tool leans on the Apache Avro-derived Avrotize Schema as its schema model, extending Avro with several annotations. A formal spec is in the repo. The rationale for picking Avro is, simply, that any code-generator must resolve the chaos that is JSON Schema&#39;s $ref/anyOf/allOf/oneOf and unrestricted type unions and enums into type graph before emitting code. What I do with this tool is to capture that type graph in Avro Schema, which is a better foundation for code generation as it is always self-contained, limits the value space for identifiers, supports namespaces, and has a richer and extensible type system. The fact that you can drive a binary serializer with it is just a nice byproduct.</p>\n\n<p>Data schema formats: Avro, JSON Schema, XML Schema (XSD), Protocol Buffers 2 and 3, ASN.1, Apache Parquet \nProgramming languages: Python, C#, Java, TypeScript, JavaScript, Rust, Go, C++\nSQL Databases: MySQL, MariaDB, PostgreSQL, SQL Server, Oracle, SQLite, BigQuery, Snowflake, Redshift, DB2\nOther databases: KQL/Kusto, MongoDB, Cassandra, Redis, Elasticsearch, DynamoDB, CosmosDB</p>\n\n<p>Mind that the tool is not emitting code that does data conversion from/to all these data encodings and DBs. It converts the data structure declarations. If you want to work with GTFS-RT data, it&#39;s going to do a good job converting the Protobuf structures to Avro and onwards into JSON Schema, taking all the enums and doc comments along for the ride.</p>\n\n<p>However, the generated data transfer objects can obviously be used with your favorite ORM tool and the code generators emit annotations for JSON and Avro serializers (plus XML in C#)</p>\n\n<p>Feedback and collaboration welcome.</p>\n\n<p>(VS Code Extension available as &quot;Avrotize&quot; in the Marketplace)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://github.com/clemensv/avrotize', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?auto=webp&s=7af3938d27bb6988ecd06399ee71707d6ffe2905', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=108&crop=smart&auto=webp&s=79e157aa58458b06a25664c1d4df02d639d1fec6', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=216&crop=smart&auto=webp&s=e254e612a75d5035279b4994196e6f5eec950f61', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=320&crop=smart&auto=webp&s=b3f89fcaf30cc1015e3ecd38ade27f12b0178c2e', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=640&crop=smart&auto=webp&s=3043a3bc5cb6837b527e3eb92ad44647b5e0a2dd', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=960&crop=smart&auto=webp&s=1933e4f091cd58944a4939468ff0c540f569d3ea', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=1080&crop=smart&auto=webp&s=8584e161b6960e940a99f66561954f521b27f7ea', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 't9OiITrH4cBTmSil5LlZp-czQk-gzwc7A6XZQeDlveg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1gnwx7n', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='clemensv'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnwx7n/avrotize_a_rosetta_stone_to_convert_database/', 'stickied': False, 'url': 'https://github.com/clemensv/avrotize', 'subreddit_subscribers': 227819, 'created_utc': 1731230687.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.105+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Just the title.  ', 'author_fullname': 't2_rh7carej1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Why is neither distinct nor limit encouraged in Bigquery data warehouse? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnyqhx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.61, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731238442.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Just the title.  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnyqhx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='accountForCareer'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnyqhx/why_is_neither_distinct_nor_limit_encouraged_in/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnyqhx/why_is_neither_distinct_nor_limit_encouraged_in/', 'subreddit_subscribers': 227819, 'created_utc': 1731238442.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.105+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Building warehousing solutions involves many engineering challenges, like dimensional modelling, generation of surrogate keys, SCD dimensions, etc. In this end-to-end tutorial, I follow Kimball's warehousing methodology and demonstrate the practical implementation of these tasks using the example of the Synapse Warehouse which is part of Microsoft Fabric. I also explain some of Synapse Warehouse's limitations, which stem from its distributed nature, and show coding tricks to help overcome them. Check out here: [https://youtu.be/Sv4zRnmfWJc](https://youtu.be/Sv4zRnmfWJc)", 'author_fullname': 't2_8isdv2tf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to build scalable cloud warehouse with Microsoft Fabric', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnnbck', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731195855.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Building warehousing solutions involves many engineering challenges, like dimensional modelling, generation of surrogate keys, SCD dimensions, etc. In this end-to-end tutorial, I follow Kimball&#39;s warehousing methodology and demonstrate the practical implementation of these tasks using the example of the Synapse Warehouse which is part of Microsoft Fabric. I also explain some of Synapse Warehouse&#39;s limitations, which stem from its distributed nature, and show coding tricks to help overcome them. Check out here: <a href="https://youtu.be/Sv4zRnmfWJc">https://youtu.be/Sv4zRnmfWJc</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?auto=webp&s=adc6f69a86e233a8924ecd7eeb869a05c493553a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?width=108&crop=smart&auto=webp&s=4ec2260e8f8d0654c8d35706a625e171b7f5eaee', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?width=216&crop=smart&auto=webp&s=1134d83d01b06052834649c7f487acf7fd0bfe79', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?width=320&crop=smart&auto=webp&s=40fb3ca6286a05516ddab47750f95f346533bf4b', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'AafQK5Zs5kLk7g2EtamJ5ncnzJXax-O4QQ8Exeluvpk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnnbck', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Nice_Substance_6594'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnnbck/how_to_build_scalable_cloud_warehouse_with/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnnbck/how_to_build_scalable_cloud_warehouse_with/', 'subreddit_subscribers': 227819, 'created_utc': 1731195855.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.106+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi Everyone!\n\nI’m working on a project where I need to generate a realistic dataset to test a Cloud Economics Dashboard. The challenge is making sure that relationships between tables are consistent (e.g., foreign keys align) and that the values reflect real-world usage patterns—especially for columns that are used in calculations, like costs or usage hours.\n\nI’d love to hear about:\n\n* **Approaches** you use to create realistic, testable datasets where relationships and constraints are consistent.\n* **Best practices** for simulating real-world variability and trends (e.g., costs peaking in certain months, higher usage for certain resources, etc.).\n* **Open-source tools** that you’ve found helpful for this type of data generation, especially ones that support complex relationships between tables.\n\nAny advice, tools, or resources would be awesome—thanks in advance!', 'author_fullname': 't2_dwtcg8jr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best Practices for Generating Realistic Test Datasets with Consistent Relationships? Any Open-Source Tools?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gny7dj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731236294.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi Everyone!</p>\n\n<p>I’m working on a project where I need to generate a realistic dataset to test a Cloud Economics Dashboard. The challenge is making sure that relationships between tables are consistent (e.g., foreign keys align) and that the values reflect real-world usage patterns—especially for columns that are used in calculations, like costs or usage hours.</p>\n\n<p>I’d love to hear about:</p>\n\n<ul>\n<li><strong>Approaches</strong> you use to create realistic, testable datasets where relationships and constraints are consistent.</li>\n<li><strong>Best practices</strong> for simulating real-world variability and trends (e.g., costs peaking in certain months, higher usage for certain resources, etc.).</li>\n<li><strong>Open-source tools</strong> that you’ve found helpful for this type of data generation, especially ones that support complex relationships between tables.</li>\n</ul>\n\n<p>Any advice, tools, or resources would be awesome—thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gny7dj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Remote-Community239'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gny7dj/best_practices_for_generating_realistic_test/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gny7dj/best_practices_for_generating_realistic_test/', 'subreddit_subscribers': 227819, 'created_utc': 1731236294.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.106+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm working with **Azure Synapse dedicated SQL pools**, and I'm looking for ideas on the fastest way to identify which columns of ODS tables are used in the next layers, specifically in the **TDM** and **DWH** schemas. For context, **ODS** \\-> **TDM->**  **DWH** are schemas in our database.\n\nFor example, we have an ODS table called `ODS.SFDC_ACCOUNT` with around 90 columns. I want to find out how many of these columns are actually used in the TDM or DWH layers—perhaps only 50 of them are utilized. This information would help us streamline our two different Datawarehouse processes as we work on merging common tables.\n\n**Does anyone have suggestions or best practices for efficiently identifying column usage across schemas in Azure Synapse?** Any tools, SQL queries, or approaches that could help with this would be greatly appreciated.\n\nThanks in advance!", 'author_fullname': 't2_iiiqo30a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to Identify Used Columns Across Schemas in Azure Synapse Dedicated SQL Pools?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnx3i5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731231458.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m working with <strong>Azure Synapse dedicated SQL pools</strong>, and I&#39;m looking for ideas on the fastest way to identify which columns of ODS tables are used in the next layers, specifically in the <strong>TDM</strong> and <strong>DWH</strong> schemas. For context, <strong>ODS</strong> -&gt; <strong>TDM-&gt;</strong>  <strong>DWH</strong> are schemas in our database.</p>\n\n<p>For example, we have an ODS table called <code>ODS.SFDC_ACCOUNT</code> with around 90 columns. I want to find out how many of these columns are actually used in the TDM or DWH layers—perhaps only 50 of them are utilized. This information would help us streamline our two different Datawarehouse processes as we work on merging common tables.</p>\n\n<p><strong>Does anyone have suggestions or best practices for efficiently identifying column usage across schemas in Azure Synapse?</strong> Any tools, SQL queries, or approaches that could help with this would be greatly appreciated.</p>\n\n<p>Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnx3i5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='BOOBINDERxKK'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnx3i5/how_to_identify_used_columns_across_schemas_in/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnx3i5/how_to_identify_used_columns_across_schemas_in/', 'subreddit_subscribers': 227819, 'created_utc': 1731231458.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.107+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi, does anybody know how to test Snowplow analytics locally end-to-end. \n\nI tried the Snowplow micro but it doesn’t include ingestion of data to a data warehouse.\n\nI want to test:\n- event generation with their SDK\n- validation with Iglu server\n- data ingestion to something like BigQuery\n- their Dbt models\n\n\nIs there an example project that works for this? Everything I found is unmaintained.\n\nThank you in advance!', 'author_fullname': 't2_gnytqihqi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Snowplow Example', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1go45aw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731255278.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, does anybody know how to test Snowplow analytics locally end-to-end. </p>\n\n<p>I tried the Snowplow micro but it doesn’t include ingestion of data to a data warehouse.</p>\n\n<p>I want to test:\n- event generation with their SDK\n- validation with Iglu server\n- data ingestion to something like BigQuery\n- their Dbt models</p>\n\n<p>Is there an example project that works for this? Everything I found is unmaintained.</p>\n\n<p>Thank you in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1go45aw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MitzuIstvan'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go45aw/snowplow_example/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go45aw/snowplow_example/', 'subreddit_subscribers': 227819, 'created_utc': 1731255278.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.107+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello everyone,\n\nI’m a student researching renewable energy, specifically working on a system to convert mechanical energy from ocean waves into electricity. My objective is to design a system that maintains a tangential alignment with the ocean wave surface at all times (meaning the angle between the system and the wave surface is always zero). This alignment should optimize energy transfer.\n\nI’m looking for advice on the best way to determine the ideal shape for this system. One idea I have is to create a Python program that simulates different shapes and tests how well they maintain a tangential alignment with waves in a simulated environment. Additionally, I’d like to explore if I could use AI to automatically generate and test shapes, ultimately helping me find the most effective design. While I have some Python experience, so any guidance would be helpful.\n\nThank you for your help!', 'author_fullname': 't2_94xvey5m', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Researching Energy: How to Find the Best Shape Using AI', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnxskd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.61, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731234546.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone,</p>\n\n<p>I’m a student researching renewable energy, specifically working on a system to convert mechanical energy from ocean waves into electricity. My objective is to design a system that maintains a tangential alignment with the ocean wave surface at all times (meaning the angle between the system and the wave surface is always zero). This alignment should optimize energy transfer.</p>\n\n<p>I’m looking for advice on the best way to determine the ideal shape for this system. One idea I have is to create a Python program that simulates different shapes and tests how well they maintain a tangential alignment with waves in a simulated environment. Additionally, I’d like to explore if I could use AI to automatically generate and test shapes, ultimately helping me find the most effective design. While I have some Python experience, so any guidance would be helpful.</p>\n\n<p>Thank you for your help!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnxskd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='WhereasGlum9389'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnxskd/researching_energy_how_to_find_the_best_shape/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnxskd/researching_energy_how_to_find_the_best_shape/', 'subreddit_subscribers': 227819, 'created_utc': 1731234546.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.108+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "We're currently using Snowflake and it's very costly and we're considering migrating to an open data stack such as with Apache Iceberg. If your company was thinking about moving away from a cloud data warehouse, whether for costs or not, to another data platform, I'd love to hear about your process.\n\nHere are some questions I have in mind:\n\n1. What was your previous/current data stack?\n2. What stack did you move to or planning to move to?\n3. How do you envision the transition, or what did it look like?\n4. What was the primary reason for migrating to the other data platform?\n\nThanks a lot.", 'author_fullname': 't2_1lhjq7k7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Migrating from costly cloud data warehouses', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1go51af', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731257627.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We&#39;re currently using Snowflake and it&#39;s very costly and we&#39;re considering migrating to an open data stack such as with Apache Iceberg. If your company was thinking about moving away from a cloud data warehouse, whether for costs or not, to another data platform, I&#39;d love to hear about your process.</p>\n\n<p>Here are some questions I have in mind:</p>\n\n<ol>\n<li>What was your previous/current data stack?</li>\n<li>What stack did you move to or planning to move to?</li>\n<li>How do you envision the transition, or what did it look like?</li>\n<li>What was the primary reason for migrating to the other data platform?</li>\n</ol>\n\n<p>Thanks a lot.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1go51af', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='elongl'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go51af/migrating_from_costly_cloud_data_warehouses/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go51af/migrating_from_costly_cloud_data_warehouses/', 'subreddit_subscribers': 227819, 'created_utc': 1731257627.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.108+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'For sake of keeping the post short, imagine you have around 30 tables in a database. \n\nNow imagine you have a few different databases and we need to extract this data and deposit it in a staging Data Lake for further transformations downstream. For this question I am only concerned on the best way to handle the ETL on this step, for 30+ tables on 5+ databases. Tables should be identical except for some slight differences in data types or extra columns sometimes.\n\nI need to be able to "re-run" the full ETL for a single table in one database. \n\nI am relatively new working with Airflow by the way, and learned SubDAGs are being phased out. Are SubDAGs the best option? The general ETL process can be reutilized for all database and table combinations.', 'author_fullname': 't2_74z7k', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Any ideas on how to design Airflow DAGs/Tasks for ETL?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1go4w1y', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731257249.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>For sake of keeping the post short, imagine you have around 30 tables in a database. </p>\n\n<p>Now imagine you have a few different databases and we need to extract this data and deposit it in a staging Data Lake for further transformations downstream. For this question I am only concerned on the best way to handle the ETL on this step, for 30+ tables on 5+ databases. Tables should be identical except for some slight differences in data types or extra columns sometimes.</p>\n\n<p>I need to be able to &quot;re-run&quot; the full ETL for a single table in one database. </p>\n\n<p>I am relatively new working with Airflow by the way, and learned SubDAGs are being phased out. Are SubDAGs the best option? The general ETL process can be reutilized for all database and table combinations.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1go4w1y', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LateDay'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go4w1y/any_ideas_on_how_to_design_airflow_dagstasks_for/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go4w1y/any_ideas_on_how_to_design_airflow_dagstasks_for/', 'subreddit_subscribers': 227819, 'created_utc': 1731257249.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.108+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_qvzmu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Grab Employs LLMs for Conversational Data Discovery with GPT-4, Glean and Slack', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnzbpt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/MfYZGEgCsIdIQNyY3sCAvfOYWQ_dBPgIJR4ZBSrjiRw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731240729.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'infoq.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.infoq.com/news/2024/11/grab-data-discovery-llm-slack/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?auto=webp&s=c058b65b443d4709ea749c694a3c1ac7609058f9', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=108&crop=smart&auto=webp&s=a2bf77d283cf6f027ccce213c674714b19fc296a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=216&crop=smart&auto=webp&s=05db1902d6564d2f833d197baf3c03ab0d009e3a', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=320&crop=smart&auto=webp&s=bbc3bf11a479939c979454ad69878454968bdb3e', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=640&crop=smart&auto=webp&s=aca9a69c989c0b38f6494525dffd658078c804b8', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=960&crop=smart&auto=webp&s=25e8e6ead51fdbcc82216cbf6b874df895ad90a8', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=1080&crop=smart&auto=webp&s=808237aecdf7af17ae4a31d6708d2dbda6c6b222', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'VVBvaiBXStZ2s9QUipZmD1amxR31BOOZKBBZ90UPSkk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnzbpt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='rgancarz'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnzbpt/grab_employs_llms_for_conversational_data/', 'stickied': False, 'url': 'https://www.infoq.com/news/2024/11/grab-data-discovery-llm-slack/', 'subreddit_subscribers': 227819, 'created_utc': 1731240729.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.109+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'In data systems, telemetry and reference data play complementary roles, with telemetry providing real-time updates (e.g., vehicle locations, sensor readings) and reference data supplying the essential context (e.g., route details, sensor placements). While telemetry data updates frequently and reference data less so, both types need accurate temporal alignment for reliable historical analysis. Telemetry data is only meaningful when contextualized by reference data that is temporally consistent. If a bus\'s location is tracked in real-time, the route configuration at the time of tracking is essential for understanding the bus\'s context. The blog post discusses the "how to" of treating reference data as streams.', 'author_fullname': 't2_6ggr7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Streamifying Reference Data for Temporal Consistency with Telemetry Events', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 80, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnx02a', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/k0fWXEb_pi1imIAftJI5HZUIEWqSedbHlSXi2Eo3j8s.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731231042.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'vasters.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In data systems, telemetry and reference data play complementary roles, with telemetry providing real-time updates (e.g., vehicle locations, sensor readings) and reference data supplying the essential context (e.g., route details, sensor placements). While telemetry data updates frequently and reference data less so, both types need accurate temporal alignment for reliable historical analysis. Telemetry data is only meaningful when contextualized by reference data that is temporally consistent. If a bus&#39;s location is tracked in real-time, the route configuration at the time of tracking is essential for understanding the bus&#39;s context. The blog post discusses the &quot;how to&quot; of treating reference data as streams.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://vasters.com/clemens/2024/10/30/streamifying-reference-data-for-temporal-consistency-with-telemetry-events', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?auto=webp&s=0ae0df57132eda87a9e643c3e3762703fedeef0a', 'width': 1792, 'height': 1024}, 'resolutions': [{'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=108&crop=smart&auto=webp&s=d7f1bdb3e86eb5e8542ad6e9444c189e6457a2f6', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=216&crop=smart&auto=webp&s=e95296dea4104bf021b8d9cf19f0823c5344858f', 'width': 216, 'height': 123}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=320&crop=smart&auto=webp&s=cefd0a45db6dd9f29f680f6d5c4f29229c0ef93e', 'width': 320, 'height': 182}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=640&crop=smart&auto=webp&s=9fcfa31aee32af1e244b52f06f975af8155c9d72', 'width': 640, 'height': 365}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=960&crop=smart&auto=webp&s=f1f1882dec911adb05abf56631da202d6b679029', 'width': 960, 'height': 548}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=1080&crop=smart&auto=webp&s=baf1607044705ae37939ac58271fac91ba3ac549', 'width': 1080, 'height': 617}], 'variants': {}, 'id': 'BuZg2uTSrpUIVq2xhvajZPXSfYBFACmk-PEUStAjswE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnx02a', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='clemensv'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnx02a/streamifying_reference_data_for_temporal/', 'stickied': False, 'url': 'https://vasters.com/clemens/2024/10/30/streamifying-reference-data-for-temporal-consistency-with-telemetry-events', 'subreddit_subscribers': 227819, 'created_utc': 1731231042.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.109+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c504160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone,\n\nI’m working on my Master’s thesis and would really appreciate your help! I’m conducting a survey on AI usage, trust, and employee performance, and I’m looking for participants who use AI tools (like ChatGPT, Grammarly, or similar) in their work.\n\nThe survey is anonymous and should take no more than 5 minutes to complete. Your input would be incredibly valuable for my research.\n\nHere’s the link: https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_bdqdnmVSh2PfTZs\n\nThanks so much in advance for your support!', 'author_fullname': 't2_a2bsxwsy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineers Feedback Needed!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go2a96', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/ZkC6CWHn2ZNHLn7ZNXvMJe7vdJSrHMjiCLL4TblDi-s.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731250212.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I’m working on my Master’s thesis and would really appreciate your help! I’m conducting a survey on AI usage, trust, and employee performance, and I’m looking for participants who use AI tools (like ChatGPT, Grammarly, or similar) in their work.</p>\n\n<p>The survey is anonymous and should take no more than 5 minutes to complete. Your input would be incredibly valuable for my research.</p>\n\n<p>Here’s the link: <a href="https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_bdqdnmVSh2PfTZs">https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_bdqdnmVSh2PfTZs</a></p>\n\n<p>Thanks so much in advance for your support!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/bgzla0gq730e1.jpeg', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?auto=webp&s=60e48f8830eb848717c58b85ea4affba94027388', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=108&crop=smart&auto=webp&s=d5505bec2fe474e13724bf596a25679bad4705ae', 'width': 108, 'height': 56}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=216&crop=smart&auto=webp&s=f0367124d6b4f6d62d49d5a64ec325ec5c7acbd5', 'width': 216, 'height': 113}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=320&crop=smart&auto=webp&s=b4662593fef952ef5abc4655addf51ffdf22c89a', 'width': 320, 'height': 167}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=640&crop=smart&auto=webp&s=eba6f21e12bc993c71611935baa375cc53a484cb', 'width': 640, 'height': 334}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=960&crop=smart&auto=webp&s=2c05c8923d39103438e57b6481344916211b3990', 'width': 960, 'height': 502}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=1080&crop=smart&auto=webp&s=b618cff4b47a8f3d40195119ed2312ff00e8457f', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'ARlU1690rNEE90gHpifbhDj9JpG312F30ETCTebF9rM'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1go2a96', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Top_Sheepherder_2929'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go2a96/data_engineers_feedback_needed/', 'stickied': False, 'url': 'https://i.redd.it/bgzla0gq730e1.jpeg', 'subreddit_subscribers': 227819, 'created_utc': 1731250212.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-10T17:58:56.162+0000] {python.py:194} INFO - Done. Returned value was: /opt/airflow/data/output/reddit_20241110.csv
[2024-11-10T17:58:56.186+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20241110T175853, start_date=20241110T175855, end_date=20241110T175856
[2024-11-10T17:58:56.208+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-11-10T17:58:56.226+0000] {taskinstance.py:2778} INFO - 1 downstream tasks scheduled from follow-on schedule check
