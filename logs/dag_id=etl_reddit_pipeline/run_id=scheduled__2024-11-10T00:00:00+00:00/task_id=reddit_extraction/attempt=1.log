[2024-11-11T00:00:04.380+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction scheduled__2024-11-10T00:00:00+00:00 [queued]>
[2024-11-11T00:00:04.484+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction scheduled__2024-11-10T00:00:00+00:00 [queued]>
[2024-11-11T00:00:04.485+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2024-11-11T00:00:04.535+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-11-10 00:00:00+00:00
[2024-11-11T00:00:04.573+0000] {standard_task_runner.py:57} INFO - Started process 98 to run task
[2024-11-11T00:00:04.602+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'scheduled__2024-11-10T00:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpab0cusj_']
[2024-11-11T00:00:04.613+0000] {standard_task_runner.py:85} INFO - Job 12: Subtask reddit_extraction
[2024-11-11T00:00:04.975+0000] {task_command.py:416} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction scheduled__2024-11-10T00:00:00+00:00 [running]> on host 97f6bbe1811b
[2024-11-11T00:00:05.165+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Navya Racha' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-11-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-10T00:00:00+00:00'
[2024-11-11T00:00:05.461+0000] {logging_mixin.py:151} WARNING - Version 7.7.1 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.
[2024-11-11T00:00:05.464+0000] {logging_mixin.py:151} INFO - Connected to Reddit!
[2024-11-11T00:00:06.204+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I want to thank this community for putting pressure on me to not be so greedy and share my knowledge more freely.\n\nLaunch video with all the details is here: [https://youtu.be/myhe0LXpCeo](https://youtu.be/myhe0LXpCeo)  \nMore details of how to join will be added to [https://www.github.com/DataExpert-io/data-engineer-handbook](https://www.github.com/DataExpert-io/data-engineer-handbook) soon!\n\nStarting on November 15th, I'll be publishing a new education video nearly every day until the end of the year as an end-of-2024 gift!\n\nThings we'll cover:  \n\\- Data modeling (fact data modeling, one big table, STRUCTS/ARRAYs, dimensional modeling)\n\n\\- Data quality patterns with Airflow like write-audit-publish\n\n\\- Unit and end-to-end testing PySpark jobs with Chispa\n\n\\- Writing Apache Flink jobs that connect to Kafka and do complex windowing\n\n\\- Data visualization with Tableau\n\n\\- Data pipeline maintenance (how to create good runbooks)\n\n\\- Analytical Patterns with Postgres (such as Facebook growth accounting)\n\n\\- Advanced window functions with Postgres and SQL\n\nThe content of these videos is from the boot camp I delivered in July 2023.\n\nIt will be six weeks of in depth content and I'm excited to deliver the value to y'all.", 'author_fullname': 't2_2xnlhe', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Launching a free six-week data engineering boot camp on YouTube on November 15th!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnor9c', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 240, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 240, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731200196.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I want to thank this community for putting pressure on me to not be so greedy and share my knowledge more freely.</p>\n\n<p>Launch video with all the details is here: <a href="https://youtu.be/myhe0LXpCeo">https://youtu.be/myhe0LXpCeo</a><br/>\nMore details of how to join will be added to <a href="https://www.github.com/DataExpert-io/data-engineer-handbook">https://www.github.com/DataExpert-io/data-engineer-handbook</a> soon!</p>\n\n<p>Starting on November 15th, I&#39;ll be publishing a new education video nearly every day until the end of the year as an end-of-2024 gift!</p>\n\n<p>Things we&#39;ll cover:<br/>\n- Data modeling (fact data modeling, one big table, STRUCTS/ARRAYs, dimensional modeling)</p>\n\n<p>- Data quality patterns with Airflow like write-audit-publish</p>\n\n<p>- Unit and end-to-end testing PySpark jobs with Chispa</p>\n\n<p>- Writing Apache Flink jobs that connect to Kafka and do complex windowing</p>\n\n<p>- Data visualization with Tableau</p>\n\n<p>- Data pipeline maintenance (how to create good runbooks)</p>\n\n<p>- Analytical Patterns with Postgres (such as Facebook growth accounting)</p>\n\n<p>- Advanced window functions with Postgres and SQL</p>\n\n<p>The content of these videos is from the boot camp I delivered in July 2023.</p>\n\n<p>It will be six weeks of in depth content and I&#39;m excited to deliver the value to y&#39;all.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?auto=webp&s=b0adf7d2276d6a58828e73eafe09842c80780be2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?width=108&crop=smart&auto=webp&s=f04f252a26818db41a554cd4150ea00fb371b6b0', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?width=216&crop=smart&auto=webp&s=d9684c27d7001da10b1b1dd44ce5e237059aa5f6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/cEF8BdFIknC2quBmBs3oJbB3NvlKYXFn6fAxL7tbHgE.jpg?width=320&crop=smart&auto=webp&s=4504d7c2526166102859cb61bd39c7fef9c1e527', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'hNjQunzaVA63XjM8vSu890db6RFHDnzbJTTbsUcf-Ws'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnor9c', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='eczachly'), 'discussion_type': None, 'num_comments': 85, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnor9c/launching_a_free_sixweek_data_engineering_boot/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnor9c/launching_a_free_sixweek_data_engineering_boot/', 'subreddit_subscribers': 227915, 'created_utc': 1731200196.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.207+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Wrapping up my series of getting into Data Engineering. Two images attached, three core expertise and roadmap. You may have to check the initial article here to understand my perspective: https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&utm_campaign=post&utm_medium=web\n\nData Analyst can naturally move by focusing on overlapping areas and grow and make more $$$.\n\nEach time I shared roadmap for SWE or DS or now DA, they all focus on the core areas to make it easy transition.\n\nRoadmaps are hard to come up with, so I made some choices and wrote about here: https://www.junaideffendi.com/p/transition-data-analyst-to-data-engineer?r=cqjft&utm_campaign=post&utm_medium=web\n\nIf you have something in mind, comment please.', 'author_fullname': 't2_dhgy4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'is_gallery': True, 'title': 'Analyst to Engineer ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 81, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'scti5cnhx00e1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 131, 'x': 108, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=108&crop=smart&auto=webp&s=0c782a904dd73cf85cb93f7a1d71b07bb9a980da'}, {'y': 262, 'x': 216, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=216&crop=smart&auto=webp&s=b191a47688d6f208327773777c318e43d3218abe'}, {'y': 388, 'x': 320, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=320&crop=smart&auto=webp&s=869d83b1e77d8ab8558b853e00a53984427af252'}, {'y': 777, 'x': 640, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=640&crop=smart&auto=webp&s=b431fd971942894b1194e06617d0efa648da0489'}, {'y': 1166, 'x': 960, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=960&crop=smart&auto=webp&s=914779bb304eddb8ac0d6f79529cd3f4d7b819dd'}, {'y': 1312, 'x': 1080, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=1080&crop=smart&auto=webp&s=43d8d7687d45fc7e1a512dec747f921d97869236'}], 's': {'y': 1546, 'x': 1272, 'u': 'https://preview.redd.it/scti5cnhx00e1.jpg?width=1272&format=pjpg&auto=webp&s=74363aae3a88aeeacf17031aad159c52df32f004'}, 'id': 'scti5cnhx00e1'}, 't52hs6ihx00e1': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 62, 'x': 108, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=108&crop=smart&auto=webp&s=9cae61004721cfedcdd870dc389248ce5dd74d53'}, {'y': 125, 'x': 216, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=216&crop=smart&auto=webp&s=8a25512250a981a0d06dac62558b40d20eb1b538'}, {'y': 185, 'x': 320, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=320&crop=smart&auto=webp&s=c741ab4492d4eca31e00f37585557a3ba675c478'}, {'y': 371, 'x': 640, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=640&crop=smart&auto=webp&s=27dab8d36bb3fd277f1fac72f856263a2d46bf5e'}, {'y': 556, 'x': 960, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=960&crop=smart&auto=webp&s=3916aba11d5c6d257014252abb902b8d1026cc1b'}, {'y': 626, 'x': 1080, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=1080&crop=smart&auto=webp&s=24a98ffcbacae4492176253cc8d544c0b35a7e86'}], 's': {'y': 738, 'x': 1272, 'u': 'https://preview.redd.it/t52hs6ihx00e1.jpg?width=1272&format=pjpg&auto=webp&s=0ff1f689535c3089fe0a94e3b919f157c6a7c3ea'}, 'id': 't52hs6ihx00e1'}}, 'name': 't3_1gnv2oy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'ups': 94, 'domain': 'reddit.com', 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'gallery_data': {'items': [{'caption': '', 'media_id': 't52hs6ihx00e1', 'id': 548987668}, {'caption': '', 'media_id': 'scti5cnhx00e1', 'id': 548987669}]}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 94, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/nVEIuQJOD--nVzD3ICW--beqDFjV3IiGXlrT6kG387c.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731222553.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'total_awards_received': 0, 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Wrapping up my series of getting into Data Engineering. Two images attached, three core expertise and roadmap. You may have to check the initial article here to understand my perspective: <a href="https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web">https://www.junaideffendi.com/p/types-of-data-engineers?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web</a></p>\n\n<p>Data Analyst can naturally move by focusing on overlapping areas and grow and make more $$$.</p>\n\n<p>Each time I shared roadmap for SWE or DS or now DA, they all focus on the core areas to make it easy transition.</p>\n\n<p>Roadmaps are hard to come up with, so I made some choices and wrote about here: <a href="https://www.junaideffendi.com/p/transition-data-analyst-to-data-engineer?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web">https://www.junaideffendi.com/p/transition-data-analyst-to-data-engineer?r=cqjft&amp;utm_campaign=post&amp;utm_medium=web</a></p>\n\n<p>If you have something in mind, comment please.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.reddit.com/gallery/1gnv2oy', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnv2oy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mjfnd'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnv2oy/analyst_to_engineer/', 'stickied': False, 'url': 'https://www.reddit.com/gallery/1gnv2oy', 'subreddit_subscribers': 227915, 'created_utc': 1731222553.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.209+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'This plan is shaped by 4+ years of experience, analyzing over 100 job descriptions, industry insights, and guidance from advisors at McGill during my studies. Here’s a structured four-month path to accelerate your path in Data Engineering.\n\nhttps://preview.redd.it/22kswpt2d40e1.png?width=1131&format=png&auto=webp&s=7ae5c70e4bf22d69233bdef1b5381ab4ac3dcbd0\n\n**Month 1: Foundations**\n\n* **DBMS & SQL:** Basics of database concepts, querying, and design.\n* **Python:** Focus on Python essentials, including libraries like Pandas and NumPy.\n* **Linux:** Basic commands and navigation.\n* **DSA:** Data structures and algorithms, especially for big tech roles.\n\n**Month 2: Key Concepts & Tools**\n\n* **Data Concepts:** Topics such as Data Lake, Data Mart, Fabric, and Mesh.\n* **Data Governance:** Management, security, and ethics in data.\n* **Spark:** Introductory concepts with Apache Spark.\n* **Distributed Systems:** Overview of Hadoop, Hive, and MPP systems.\n* **Cloud Services:** Options such as AWS, GCP, or Azure.\n\n**Month 3: Advanced Topics**\n\n* **Orchestration:** Basics of workflow orchestration with tools like Apache Airflow.\n* **Compute:** Databricks, Snowflake, or equivalents like AWS EMR.\n* **Containers:** Introduction to Docker and Kubernetes.\n* **CI/CD:** Tools such as Jenkins and SonarQube.\n* **Streaming:** Fundamentals of Kafka.\n* **ETL/ELT:** Tools like dbt and Talend, along with architecture basics.\n* **Terraform:** Code-based infrastructure setup.\n\n**Month 4: Projects & Portfolio**\n\nBuild a project portfolio to showcase skills. Examples include:\n\n* **Bank Data Warehouse**\n* **Fraud Detection ETL**\n* **Reddit Review Tracker**\n* **Retail Analytics**\n* **Trip Data Transformation**\n* **YouTube Clone**\n\n**Certifications**\n\n* **AWS Certifications:** Cloud Practitioner, Solutions Architect Associate, Data Engineer Associate\n* **Databricks:** Data Engineer Associate\n* **Apache Airflow:** Airflow Fundamentals\n\n**Showcase Your Work**\n\n* Document projects on GitHub, post on LinkedIn, and network with target companies.\n\nYour feedback is appreciated to fine tune this plan!\n\n➡️ Full breakdown of more details and learning resources available in the video: [https://youtu.be/5b4CIon\\_1pY](https://youtu.be/5b4CIon_1pY)  \n➡️ Excel sheet with data:\xa0[https://docs.google.com/spreadsheets/d/1zB6wocrgxNgjWwo6Jkezje0SgJ3PXMIoCEyJwdY-nLU/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1zB6wocrgxNgjWwo6Jkezje0SgJ3PXMIoCEyJwdY-nLU/edit?usp=sharing)', 'author_fullname': 't2_c3x6xj5p', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '4 Month Data Engineering Study Plan - Based on Market Demand', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'22kswpt2d40e1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 61, 'x': 108, 'u': 'https://preview.redd.it/22kswpt2d40e1.png?width=108&crop=smart&auto=webp&s=e82fce0b4af3e582e2483e2c26de74ed1b789b45'}, {'y': 122, 'x': 216, 'u': 'https://preview.redd.it/22kswpt2d40e1.png?width=216&crop=smart&auto=webp&s=8599190cf631e32baedc91730f9c06b067b33205'}, {'y': 182, 'x': 320, 'u': 'https://preview.redd.it/22kswpt2d40e1.png?width=320&crop=smart&auto=webp&s=e8e73e063c6f90b329698a2f224dbf8ecdd9ebac'}, {'y': 364, 'x': 640, 'u': 'https://preview.redd.it/22kswpt2d40e1.png?width=640&crop=smart&auto=webp&s=b864b1bc676a88bf1fa97035128c25c4d09c1345'}, {'y': 546, 'x': 960, 'u': 'https://preview.redd.it/22kswpt2d40e1.png?width=960&crop=smart&auto=webp&s=15f658041669030399551e8a7ba22fbe6d21d9af'}, {'y': 614, 'x': 1080, 'u': 'https://preview.redd.it/22kswpt2d40e1.png?width=1080&crop=smart&auto=webp&s=2c1f47223e57b6959e7a1065165ca7a3315b2445'}], 's': {'y': 644, 'x': 1131, 'u': 'https://preview.redd.it/22kswpt2d40e1.png?width=1131&format=png&auto=webp&s=7ae5c70e4bf22d69233bdef1b5381ab4ac3dcbd0'}, 'id': '22kswpt2d40e1'}}, 'name': 't3_1go7jq9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'ups': 108, 'total_awards_received': 0, 'media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/5b4CIon_1pY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Data Engineer ROADMAP: 4 Month Learning Path to Launch Your Career!"></iframe>', 'width': 356, 'scrolling': False, 'height': 200}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Data Engineer ROADMAP: 4 Month Learning Path to Launch Your Career!', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/5b4CIon_1pY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Data Engineer ROADMAP: 4 Month Learning Path to Launch Your Career!"></iframe>', 'author_name': 'Analytics Vector', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/5b4CIon_1pY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@Analytics-Vector'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '<iframe width="356" height="200" src="https://www.youtube.com/embed/5b4CIon_1pY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Data Engineer ROADMAP: 4 Month Learning Path to Launch Your Career!"></iframe>', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/1go7jq9', 'height': 200}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 108, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/poOVbjSwA-XkEF7GVsDc431CxJMp2pLzNsEjbf6jCoI.jpg', 'edited': 1731265619.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'subreddit_type': 'public', 'created': 1731264081.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>This plan is shaped by 4+ years of experience, analyzing over 100 job descriptions, industry insights, and guidance from advisors at McGill during my studies. Here’s a structured four-month path to accelerate your path in Data Engineering.</p>\n\n<p><a href="https://preview.redd.it/22kswpt2d40e1.png?width=1131&amp;format=png&amp;auto=webp&amp;s=7ae5c70e4bf22d69233bdef1b5381ab4ac3dcbd0">https://preview.redd.it/22kswpt2d40e1.png?width=1131&amp;format=png&amp;auto=webp&amp;s=7ae5c70e4bf22d69233bdef1b5381ab4ac3dcbd0</a></p>\n\n<p><strong>Month 1: Foundations</strong></p>\n\n<ul>\n<li><strong>DBMS &amp; SQL:</strong> Basics of database concepts, querying, and design.</li>\n<li><strong>Python:</strong> Focus on Python essentials, including libraries like Pandas and NumPy.</li>\n<li><strong>Linux:</strong> Basic commands and navigation.</li>\n<li><strong>DSA:</strong> Data structures and algorithms, especially for big tech roles.</li>\n</ul>\n\n<p><strong>Month 2: Key Concepts &amp; Tools</strong></p>\n\n<ul>\n<li><strong>Data Concepts:</strong> Topics such as Data Lake, Data Mart, Fabric, and Mesh.</li>\n<li><strong>Data Governance:</strong> Management, security, and ethics in data.</li>\n<li><strong>Spark:</strong> Introductory concepts with Apache Spark.</li>\n<li><strong>Distributed Systems:</strong> Overview of Hadoop, Hive, and MPP systems.</li>\n<li><strong>Cloud Services:</strong> Options such as AWS, GCP, or Azure.</li>\n</ul>\n\n<p><strong>Month 3: Advanced Topics</strong></p>\n\n<ul>\n<li><strong>Orchestration:</strong> Basics of workflow orchestration with tools like Apache Airflow.</li>\n<li><strong>Compute:</strong> Databricks, Snowflake, or equivalents like AWS EMR.</li>\n<li><strong>Containers:</strong> Introduction to Docker and Kubernetes.</li>\n<li><strong>CI/CD:</strong> Tools such as Jenkins and SonarQube.</li>\n<li><strong>Streaming:</strong> Fundamentals of Kafka.</li>\n<li><strong>ETL/ELT:</strong> Tools like dbt and Talend, along with architecture basics.</li>\n<li><strong>Terraform:</strong> Code-based infrastructure setup.</li>\n</ul>\n\n<p><strong>Month 4: Projects &amp; Portfolio</strong></p>\n\n<p>Build a project portfolio to showcase skills. Examples include:</p>\n\n<ul>\n<li><strong>Bank Data Warehouse</strong></li>\n<li><strong>Fraud Detection ETL</strong></li>\n<li><strong>Reddit Review Tracker</strong></li>\n<li><strong>Retail Analytics</strong></li>\n<li><strong>Trip Data Transformation</strong></li>\n<li><strong>YouTube Clone</strong></li>\n</ul>\n\n<p><strong>Certifications</strong></p>\n\n<ul>\n<li><strong>AWS Certifications:</strong> Cloud Practitioner, Solutions Architect Associate, Data Engineer Associate</li>\n<li><strong>Databricks:</strong> Data Engineer Associate</li>\n<li><strong>Apache Airflow:</strong> Airflow Fundamentals</li>\n</ul>\n\n<p><strong>Showcase Your Work</strong></p>\n\n<ul>\n<li>Document projects on GitHub, post on LinkedIn, and network with target companies.</li>\n</ul>\n\n<p>Your feedback is appreciated to fine tune this plan!</p>\n\n<p>➡️ Full breakdown of more details and learning resources available in the video: <a href="https://youtu.be/5b4CIon_1pY">https://youtu.be/5b4CIon_1pY</a><br/>\n➡️ Excel sheet with data:\xa0<a href="https://docs.google.com/spreadsheets/d/1zB6wocrgxNgjWwo6Jkezje0SgJ3PXMIoCEyJwdY-nLU/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1zB6wocrgxNgjWwo6Jkezje0SgJ3PXMIoCEyJwdY-nLU/edit?usp=sharing</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/JaW6Rnh2-hu3XpDLmcjj5o4Ec3cfsMOl-PA_fkqxcbQ.jpg?auto=webp&s=8908f37347c68e2dc3cc5ff27caeb31158a372ce', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/JaW6Rnh2-hu3XpDLmcjj5o4Ec3cfsMOl-PA_fkqxcbQ.jpg?width=108&crop=smart&auto=webp&s=a0c71b7229bda2665410079e9425f1204d9eb780', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/JaW6Rnh2-hu3XpDLmcjj5o4Ec3cfsMOl-PA_fkqxcbQ.jpg?width=216&crop=smart&auto=webp&s=0fd072928d2a9bc0f51c6ac0c0186738036da27c', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/JaW6Rnh2-hu3XpDLmcjj5o4Ec3cfsMOl-PA_fkqxcbQ.jpg?width=320&crop=smart&auto=webp&s=eb09e28415c9d082040b290898166ef98e5e7166', 'width': 320, 'height': 240}], 'variants': {}, 'id': '9RvpQe8lzUXNqUim4uaC3CX7Z_34TaurAmMpAVA--Wg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1go7jq9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cryptoyash'), 'discussion_type': None, 'num_comments': 19, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go7jq9/4_month_data_engineering_study_plan_based_on/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go7jq9/4_month_data_engineering_study_plan_based_on/', 'subreddit_subscribers': 227915, 'created_utc': 1731264081.0, 'num_crossposts': 0, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Data Engineer ROADMAP: 4 Month Learning Path to Launch Your Career!', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '<iframe width="356" height="200" src="https://www.youtube.com/embed/5b4CIon_1pY?feature=oembed&enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen title="Data Engineer ROADMAP: 4 Month Learning Path to Launch Your Career!"></iframe>', 'author_name': 'Analytics Vector', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/5b4CIon_1pY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@Analytics-Vector'}}, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.209+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "We're currently using Snowflake and it's very costly and we're considering migrating to an open data stack such as with Apache Iceberg. If your company was thinking about moving away from a cloud data warehouse, whether for costs or not, to another data platform, I'd love to hear about your process.\n\nHere are some questions I have in mind:\n\n1. What was your previous/current data stack?\n2. What stack did you move to or planning to move to?\n3. How do you envision the transition, or what did it look like?\n4. What was the primary reason for migrating to the other data platform?\n\nThanks a lot.", 'author_fullname': 't2_1lhjq7k7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Migrating from costly cloud data warehouses', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go51af', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.89, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 22, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 22, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731257627.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>We&#39;re currently using Snowflake and it&#39;s very costly and we&#39;re considering migrating to an open data stack such as with Apache Iceberg. If your company was thinking about moving away from a cloud data warehouse, whether for costs or not, to another data platform, I&#39;d love to hear about your process.</p>\n\n<p>Here are some questions I have in mind:</p>\n\n<ol>\n<li>What was your previous/current data stack?</li>\n<li>What stack did you move to or planning to move to?</li>\n<li>How do you envision the transition, or what did it look like?</li>\n<li>What was the primary reason for migrating to the other data platform?</li>\n</ol>\n\n<p>Thanks a lot.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1go51af', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='elongl'), 'discussion_type': None, 'num_comments': 25, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go51af/migrating_from_costly_cloud_data_warehouses/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go51af/migrating_from_costly_cloud_data_warehouses/', 'subreddit_subscribers': 227915, 'created_utc': 1731257627.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.210+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I do my visualization in power bi I pull from SQL data base doing most of my transformations before I get to power query. So I do case statements, trims, joins, destincts, and loops. \n\nI'm curious, how do you use SQL? I feel like y'all's SQL is at  whole other level, like I don't even know how much I don't know. ", 'author_fullname': 't2_6n7xko94', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "I'm a data analyst, how is how I use SQL different than how you use it? ", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnsq5o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 23, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 23, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731213447.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I do my visualization in power bi I pull from SQL data base doing most of my transformations before I get to power query. So I do case statements, trims, joins, destincts, and loops. </p>\n\n<p>I&#39;m curious, how do you use SQL? I feel like y&#39;all&#39;s SQL is at  whole other level, like I don&#39;t even know how much I don&#39;t know. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1gnsq5o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='whiskey_rue'), 'discussion_type': None, 'num_comments': 15, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnsq5o/im_a_data_analyst_how_is_how_i_use_sql_different/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnsq5o/im_a_data_analyst_how_is_how_i_use_sql_different/', 'subreddit_subscribers': 227915, 'created_utc': 1731213447.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.210+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have a phone screen coming for a Meta DE IC3 role in 4 weeks. From what I've seen online and in Meta's preparation material. They give you a schema with 3-5 tables and no sample records. To successfully pass the SQL section of the phone screen, I would have to solve atleast 3 SQL mediums in 25 minutes.\n\nMy average time on StrataScratch Medium (Meta Tagged) is around 10-12 minutes. In an ideal situation, that time has to be under 8-9 minutes for each question.\n\nMy question is, how do I structure my problem before I start coding. I seem to get 'lost in the sauce' or lose focus if I keep reading the question for long. Is there a structure I can follow that would cut down my querying time?\n\nMy current gameplan:\n\n1. Identify tables\n2. Identify relationships needed for question\n3. Identify Grouping, Aggregations etc\n4. Implement.\n\nI am able to get past the first 3 steps but the 4th step gets difficult where I feel like I don't know where to start. And once I see the solution, I feel so stupid because 80-90% of the time I was on the right track and just didn't have the light bulb moment.\n\n  \nTLDR - What mental pattern should I use to solve SQL medium/hard under 8 minutes?", 'author_fullname': 't2_129bsj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do I build intuition for Complex SQL questions?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnnjd5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731196523.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have a phone screen coming for a Meta DE IC3 role in 4 weeks. From what I&#39;ve seen online and in Meta&#39;s preparation material. They give you a schema with 3-5 tables and no sample records. To successfully pass the SQL section of the phone screen, I would have to solve atleast 3 SQL mediums in 25 minutes.</p>\n\n<p>My average time on StrataScratch Medium (Meta Tagged) is around 10-12 minutes. In an ideal situation, that time has to be under 8-9 minutes for each question.</p>\n\n<p>My question is, how do I structure my problem before I start coding. I seem to get &#39;lost in the sauce&#39; or lose focus if I keep reading the question for long. Is there a structure I can follow that would cut down my querying time?</p>\n\n<p>My current gameplan:</p>\n\n<ol>\n<li>Identify tables</li>\n<li>Identify relationships needed for question</li>\n<li>Identify Grouping, Aggregations etc</li>\n<li>Implement.</li>\n</ol>\n\n<p>I am able to get past the first 3 steps but the 4th step gets difficult where I feel like I don&#39;t know where to start. And once I see the solution, I feel so stupid because 80-90% of the time I was on the right track and just didn&#39;t have the light bulb moment.</p>\n\n<p>TLDR - What mental pattern should I use to solve SQL medium/hard under 8 minutes?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnnjd5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Tam27_'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1gnnjd5/how_do_i_build_intuition_for_complex_sql_questions/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnnjd5/how_do_i_build_intuition_for_complex_sql_questions/', 'subreddit_subscribers': 227915, 'created_utc': 1731196523.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.211+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am looking for an orchestrator for my usecase and came across Apache Airflow. But I am not sure if it is the right choice. Here are the essential requirements -\n\n1. The system is supposed to serve 100K - 1M requests per day.\n2. Each request requires downstream calls to different external dependencies which are dynamically decided at runtime. The calls to these dependencies are structured like a DAG. Lets call these dependency calls as ‘jobs’.\n3. The dependencies process their jobs asynchronously and return response via SNS. The average turnaround time is 1 minute.\n4. The dependencies throw errors indicating that their job limit is reached. In these cases, we have to queue the jobs for that dependency until we receive a response from them indicating that capacity is now available.\n5. We are constrained on the job processing capacities of our dependencies and want maximum utilization. Hence, we want to schedule the next job as soon as we receive a response from that particular dependency. In other words, we want to minimize latency between job scheduling.\n6. We should have the capability to retry failed tasks / jobs / DAGsand monitor the reasons behind their failure.\n\nBonus -\n1. The system would have to keep 100K+ requests in queue at anytime due to the nature of our dependencies. So, it would be great if we can process these requests in order so that a request is not starved because of random scheduling.\n\nI have designed a solution using Lambdas with a MySQL DB to schedule the jobs and process them in order. But it would be great to understand if Airflow can be used as a tool for our usecase. \n\nFrom what I understand, I might have to create a Dynamic DAG at runtime for each of my requests with each of my dependency calls being subtasks. How good is Airflow at keeping 100K - 1M DAGs? \n\nAssuming that a Lambda receives the SNS response from the dependencies, can it go modify a DAG’s task indicating that it is now ready to move forward? And also trigger a retry to serially schedule new jobs for that specific dependency?\n\nFor the ordering logic, I read that DAGs can have dependencies on each other. Is there no other way to schedule tasks?\n\nHeres the scheduling logic I want to implement -\nIf a dependency has available capacity, pick the earliest created DAG which has pending job for that depenency and process it.', 'author_fullname': 't2_9b3176xo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is Airflow the right choice for running 100K - 1M dynamic workflows everyday?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnwoj2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 21, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 21, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731229619.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am looking for an orchestrator for my usecase and came across Apache Airflow. But I am not sure if it is the right choice. Here are the essential requirements -</p>\n\n<ol>\n<li>The system is supposed to serve 100K - 1M requests per day.</li>\n<li>Each request requires downstream calls to different external dependencies which are dynamically decided at runtime. The calls to these dependencies are structured like a DAG. Lets call these dependency calls as ‘jobs’.</li>\n<li>The dependencies process their jobs asynchronously and return response via SNS. The average turnaround time is 1 minute.</li>\n<li>The dependencies throw errors indicating that their job limit is reached. In these cases, we have to queue the jobs for that dependency until we receive a response from them indicating that capacity is now available.</li>\n<li>We are constrained on the job processing capacities of our dependencies and want maximum utilization. Hence, we want to schedule the next job as soon as we receive a response from that particular dependency. In other words, we want to minimize latency between job scheduling.</li>\n<li>We should have the capability to retry failed tasks / jobs / DAGsand monitor the reasons behind their failure.</li>\n</ol>\n\n<p>Bonus -\n1. The system would have to keep 100K+ requests in queue at anytime due to the nature of our dependencies. So, it would be great if we can process these requests in order so that a request is not starved because of random scheduling.</p>\n\n<p>I have designed a solution using Lambdas with a MySQL DB to schedule the jobs and process them in order. But it would be great to understand if Airflow can be used as a tool for our usecase. </p>\n\n<p>From what I understand, I might have to create a Dynamic DAG at runtime for each of my requests with each of my dependency calls being subtasks. How good is Airflow at keeping 100K - 1M DAGs? </p>\n\n<p>Assuming that a Lambda receives the SNS response from the dependencies, can it go modify a DAG’s task indicating that it is now ready to move forward? And also trigger a retry to serially schedule new jobs for that specific dependency?</p>\n\n<p>For the ordering logic, I read that DAGs can have dependencies on each other. Is there no other way to schedule tasks?</p>\n\n<p>Heres the scheduling logic I want to implement -\nIf a dependency has available capacity, pick the earliest created DAG which has pending job for that depenency and process it.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnwoj2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Tricky-Button-197'), 'discussion_type': None, 'num_comments': 29, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnwoj2/is_airflow_the_right_choice_for_running_100k_1m/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnwoj2/is_airflow_the_right_choice_for_running_100k_1m/', 'subreddit_subscribers': 227915, 'created_utc': 1731229619.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.211+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi !\nI've been a DE for 5 years now, for 2 différent companies. I've always worked on the 'backend' side: ETL/ELT, a bit of devops, modelization, sometimes making datamarts when analysts couldnt do it but thats pretty much it. I've never or very rarely been involved directly with any business process.\nI've read here that what makes a good DE is to be more 'business' oriented.\n\nAs I'm starting a new job in 2 months, I would like to implement some changes from the start. \n\nCan someone give me some tips about this ? What does this mean in practice ? Is there any book/ressource that would help with this ?\n\nThanks", 'author_fullname': 't2_ymuaigmkq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "How do I get more 'business' oriented ?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go2tqz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.9, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731251726.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi !\nI&#39;ve been a DE for 5 years now, for 2 différent companies. I&#39;ve always worked on the &#39;backend&#39; side: ETL/ELT, a bit of devops, modelization, sometimes making datamarts when analysts couldnt do it but thats pretty much it. I&#39;ve never or very rarely been involved directly with any business process.\nI&#39;ve read here that what makes a good DE is to be more &#39;business&#39; oriented.</p>\n\n<p>As I&#39;m starting a new job in 2 months, I would like to implement some changes from the start. </p>\n\n<p>Can someone give me some tips about this ? What does this mean in practice ? Is there any book/ressource that would help with this ?</p>\n\n<p>Thanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1go2tqz', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Fit-Needleworker6411'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go2tqz/how_do_i_get_more_business_oriented/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go2tqz/how_do_i_get_more_business_oriented/', 'subreddit_subscribers': 227915, 'created_utc': 1731251726.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.212+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '\nHi. I\'m an Architect on Microsoft\'s Fabric team and help drive the Real-time Intelligence platform pieces. A big theme of us is creating a more type-safe and productive environment for working with streaming data through broad support for schematized event payloads and CloudEvents. Our Eventstreams feature is an implementation of Azure Event Hubs (and thus also a Kafka API) embedded inside Fabric and the initiatives CNCF xRegistry and CNCF CloudEvents that we invest time in aim at event streaming in general.\n\nAvrotize is one of our useable and useful prototypes, a Rosetta Stone for data structure definitions, allowing you to convert between numerous data and database schema formats and to generate data transfer object code for different programming languages. \n\nIt is, for instance, a well-documented and predictable converter and code generator for data structures originally defined in JSON Schema (of arbitrary complexity).\n\nThe tool leans on the Apache Avro-derived Avrotize Schema as its schema model, extending Avro with several annotations. A formal spec is in the repo. The rationale for picking Avro is, simply, that any code-generator must resolve the chaos that is JSON Schema\'s $ref/anyOf/allOf/oneOf and unrestricted type unions and enums into type graph before emitting code. What I do with this tool is to capture that type graph in Avro Schema, which is a better foundation for code generation as it is always self-contained, limits the value space for identifiers, supports namespaces, and has a richer and extensible type system. The fact that you can drive a binary serializer with it is just a nice byproduct.\n\nData schema formats: Avro, JSON Schema, XML Schema (XSD), Protocol Buffers 2 and 3, ASN.1, Apache Parquet \nProgramming languages: Python, C#, Java, TypeScript, JavaScript, Rust, Go, C++\nSQL Databases: MySQL, MariaDB, PostgreSQL, SQL Server, Oracle, SQLite, BigQuery, Snowflake, Redshift, DB2\nOther databases: KQL/Kusto, MongoDB, Cassandra, Redis, Elasticsearch, DynamoDB, CosmosDB\n\nMind that the tool is not emitting code that does data conversion from/to all these data encodings and DBs. It converts the data structure declarations. If you want to work with GTFS-RT data, it\'s going to do a good job converting the Protobuf structures to Avro and onwards into JSON Schema, taking all the enums and doc comments along for the ride.\n\nHowever, the generated data transfer objects can obviously be used with your favorite ORM tool and the code generators emit annotations for JSON and Avro serializers (plus XML in C#)\n\nFeedback and collaboration welcome.\n\n(VS Code Extension available as "Avrotize" in the Marketplace)', 'author_fullname': 't2_6ggr7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Avrotize: A "Rosetta stone" to convert data(-base) schemas to/from/via Apache Avro Schema', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnwx7n', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/nd8RnZ4T0thB9WkLlcfLEulv1T7PfcCp_g2f3PC2xNU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731230687.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'github.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi. I&#39;m an Architect on Microsoft&#39;s Fabric team and help drive the Real-time Intelligence platform pieces. A big theme of us is creating a more type-safe and productive environment for working with streaming data through broad support for schematized event payloads and CloudEvents. Our Eventstreams feature is an implementation of Azure Event Hubs (and thus also a Kafka API) embedded inside Fabric and the initiatives CNCF xRegistry and CNCF CloudEvents that we invest time in aim at event streaming in general.</p>\n\n<p>Avrotize is one of our useable and useful prototypes, a Rosetta Stone for data structure definitions, allowing you to convert between numerous data and database schema formats and to generate data transfer object code for different programming languages. </p>\n\n<p>It is, for instance, a well-documented and predictable converter and code generator for data structures originally defined in JSON Schema (of arbitrary complexity).</p>\n\n<p>The tool leans on the Apache Avro-derived Avrotize Schema as its schema model, extending Avro with several annotations. A formal spec is in the repo. The rationale for picking Avro is, simply, that any code-generator must resolve the chaos that is JSON Schema&#39;s $ref/anyOf/allOf/oneOf and unrestricted type unions and enums into type graph before emitting code. What I do with this tool is to capture that type graph in Avro Schema, which is a better foundation for code generation as it is always self-contained, limits the value space for identifiers, supports namespaces, and has a richer and extensible type system. The fact that you can drive a binary serializer with it is just a nice byproduct.</p>\n\n<p>Data schema formats: Avro, JSON Schema, XML Schema (XSD), Protocol Buffers 2 and 3, ASN.1, Apache Parquet \nProgramming languages: Python, C#, Java, TypeScript, JavaScript, Rust, Go, C++\nSQL Databases: MySQL, MariaDB, PostgreSQL, SQL Server, Oracle, SQLite, BigQuery, Snowflake, Redshift, DB2\nOther databases: KQL/Kusto, MongoDB, Cassandra, Redis, Elasticsearch, DynamoDB, CosmosDB</p>\n\n<p>Mind that the tool is not emitting code that does data conversion from/to all these data encodings and DBs. It converts the data structure declarations. If you want to work with GTFS-RT data, it&#39;s going to do a good job converting the Protobuf structures to Avro and onwards into JSON Schema, taking all the enums and doc comments along for the ride.</p>\n\n<p>However, the generated data transfer objects can obviously be used with your favorite ORM tool and the code generators emit annotations for JSON and Avro serializers (plus XML in C#)</p>\n\n<p>Feedback and collaboration welcome.</p>\n\n<p>(VS Code Extension available as &quot;Avrotize&quot; in the Marketplace)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://github.com/clemensv/avrotize', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?auto=webp&s=7af3938d27bb6988ecd06399ee71707d6ffe2905', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=108&crop=smart&auto=webp&s=79e157aa58458b06a25664c1d4df02d639d1fec6', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=216&crop=smart&auto=webp&s=e254e612a75d5035279b4994196e6f5eec950f61', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=320&crop=smart&auto=webp&s=b3f89fcaf30cc1015e3ecd38ade27f12b0178c2e', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=640&crop=smart&auto=webp&s=3043a3bc5cb6837b527e3eb92ad44647b5e0a2dd', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=960&crop=smart&auto=webp&s=1933e4f091cd58944a4939468ff0c540f569d3ea', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/I-D9YlD3rDy3zcLSCdBj-lgipxfUU79si760HTtOeOA.jpg?width=1080&crop=smart&auto=webp&s=8584e161b6960e940a99f66561954f521b27f7ea', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 't9OiITrH4cBTmSil5LlZp-czQk-gzwc7A6XZQeDlveg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1gnwx7n', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='clemensv'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnwx7n/avrotize_a_rosetta_stone_to_convert_database/', 'stickied': False, 'url': 'https://github.com/clemensv/avrotize', 'subreddit_subscribers': 227915, 'created_utc': 1731230687.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.212+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Just the title.  ', 'author_fullname': 't2_rh7carej1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Why is neither distinct nor limit encouraged in Bigquery data warehouse? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnyqhx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.74, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731238442.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Just the title.  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnyqhx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='accountForCareer'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnyqhx/why_is_neither_distinct_nor_limit_encouraged_in/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnyqhx/why_is_neither_distinct_nor_limit_encouraged_in/', 'subreddit_subscribers': 227915, 'created_utc': 1731238442.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.212+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi Everyone!\n\nI’m working on a project where I need to generate a realistic dataset to test a Cloud Economics Dashboard. The challenge is making sure that relationships between tables are consistent (e.g., foreign keys align) and that the values reflect real-world usage patterns—especially for columns that are used in calculations, like costs or usage hours.\n\nI’d love to hear about:\n\n* **Approaches** you use to create realistic, testable datasets where relationships and constraints are consistent.\n* **Best practices** for simulating real-world variability and trends (e.g., costs peaking in certain months, higher usage for certain resources, etc.).\n* **Open-source tools** that you’ve found helpful for this type of data generation, especially ones that support complex relationships between tables.\n\nAny advice, tools, or resources would be awesome—thanks in advance!', 'author_fullname': 't2_dwtcg8jr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best Practices for Generating Realistic Test Datasets with Consistent Relationships? Any Open-Source Tools?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gny7dj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731236294.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi Everyone!</p>\n\n<p>I’m working on a project where I need to generate a realistic dataset to test a Cloud Economics Dashboard. The challenge is making sure that relationships between tables are consistent (e.g., foreign keys align) and that the values reflect real-world usage patterns—especially for columns that are used in calculations, like costs or usage hours.</p>\n\n<p>I’d love to hear about:</p>\n\n<ul>\n<li><strong>Approaches</strong> you use to create realistic, testable datasets where relationships and constraints are consistent.</li>\n<li><strong>Best practices</strong> for simulating real-world variability and trends (e.g., costs peaking in certain months, higher usage for certain resources, etc.).</li>\n<li><strong>Open-source tools</strong> that you’ve found helpful for this type of data generation, especially ones that support complex relationships between tables.</li>\n</ul>\n\n<p>Any advice, tools, or resources would be awesome—thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gny7dj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Remote-Community239'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gny7dj/best_practices_for_generating_realistic_test/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gny7dj/best_practices_for_generating_realistic_test/', 'subreddit_subscribers': 227915, 'created_utc': 1731236294.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.213+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm working with **Azure Synapse dedicated SQL pools**, and I'm looking for ideas on the fastest way to identify which columns of ODS tables are used in the next layers, specifically in the **TDM** and **DWH** schemas. For context, **ODS** \\-> **TDM->**  **DWH** are schemas in our database.\n\nFor example, we have an ODS table called `ODS.SFDC_ACCOUNT` with around 90 columns. I want to find out how many of these columns are actually used in the TDM or DWH layers—perhaps only 50 of them are utilized. This information would help us streamline our two different Datawarehouse processes as we work on merging common tables.\n\n**Does anyone have suggestions or best practices for efficiently identifying column usage across schemas in Azure Synapse?** Any tools, SQL queries, or approaches that could help with this would be greatly appreciated.\n\nThanks in advance!", 'author_fullname': 't2_iiiqo30a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to Identify Used Columns Across Schemas in Azure Synapse Dedicated SQL Pools?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnx3i5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731231458.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m working with <strong>Azure Synapse dedicated SQL pools</strong>, and I&#39;m looking for ideas on the fastest way to identify which columns of ODS tables are used in the next layers, specifically in the <strong>TDM</strong> and <strong>DWH</strong> schemas. For context, <strong>ODS</strong> -&gt; <strong>TDM-&gt;</strong>  <strong>DWH</strong> are schemas in our database.</p>\n\n<p>For example, we have an ODS table called <code>ODS.SFDC_ACCOUNT</code> with around 90 columns. I want to find out how many of these columns are actually used in the TDM or DWH layers—perhaps only 50 of them are utilized. This information would help us streamline our two different Datawarehouse processes as we work on merging common tables.</p>\n\n<p><strong>Does anyone have suggestions or best practices for efficiently identifying column usage across schemas in Azure Synapse?</strong> Any tools, SQL queries, or approaches that could help with this would be greatly appreciated.</p>\n\n<p>Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnx3i5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='BOOBINDERxKK'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnx3i5/how_to_identify_used_columns_across_schemas_in/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnx3i5/how_to_identify_used_columns_across_schemas_in/', 'subreddit_subscribers': 227915, 'created_utc': 1731231458.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.213+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Building warehousing solutions involves many engineering challenges, like dimensional modelling, generation of surrogate keys, SCD dimensions, etc. In this end-to-end tutorial, I follow Kimball's warehousing methodology and demonstrate the practical implementation of these tasks using the example of the Synapse Warehouse which is part of Microsoft Fabric. I also explain some of Synapse Warehouse's limitations, which stem from its distributed nature, and show coding tricks to help overcome them. Check out here: [https://youtu.be/Sv4zRnmfWJc](https://youtu.be/Sv4zRnmfWJc)", 'author_fullname': 't2_8isdv2tf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to build scalable cloud warehouse with Microsoft Fabric', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnnbck', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731195855.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Building warehousing solutions involves many engineering challenges, like dimensional modelling, generation of surrogate keys, SCD dimensions, etc. In this end-to-end tutorial, I follow Kimball&#39;s warehousing methodology and demonstrate the practical implementation of these tasks using the example of the Synapse Warehouse which is part of Microsoft Fabric. I also explain some of Synapse Warehouse&#39;s limitations, which stem from its distributed nature, and show coding tricks to help overcome them. Check out here: <a href="https://youtu.be/Sv4zRnmfWJc">https://youtu.be/Sv4zRnmfWJc</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?auto=webp&s=adc6f69a86e233a8924ecd7eeb869a05c493553a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?width=108&crop=smart&auto=webp&s=4ec2260e8f8d0654c8d35706a625e171b7f5eaee', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?width=216&crop=smart&auto=webp&s=1134d83d01b06052834649c7f487acf7fd0bfe79', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/MazNoeLGY1jM5CAipVXJjYGTJTwCb_yH-Ul1_SXAgQE.jpg?width=320&crop=smart&auto=webp&s=40fb3ca6286a05516ddab47750f95f346533bf4b', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'AafQK5Zs5kLk7g2EtamJ5ncnzJXax-O4QQ8Exeluvpk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnnbck', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Nice_Substance_6594'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnnbck/how_to_build_scalable_cloud_warehouse_with/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnnbck/how_to_build_scalable_cloud_warehouse_with/', 'subreddit_subscribers': 227915, 'created_utc': 1731195855.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.214+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'For sake of keeping the post short, imagine you have around 30 tables in a database. \n\nNow imagine you have a few different databases and we need to extract this data and deposit it in a staging Data Lake for further transformations downstream. For this question I am only concerned on the best way to handle the ETL on this step, for 30+ tables on 5+ databases. Tables should be identical except for some slight differences in data types or extra columns sometimes.\n\nI need to be able to "re-run" the full ETL for a single table in one database. \n\nI am relatively new working with Airflow by the way, and learned SubDAGs are being phased out. Are SubDAGs the best option? The general ETL process can be reutilized for all database and table combinations.', 'author_fullname': 't2_74z7k', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Any ideas on how to design Airflow DAGs/Tasks for ETL?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go4w1y', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731257249.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>For sake of keeping the post short, imagine you have around 30 tables in a database. </p>\n\n<p>Now imagine you have a few different databases and we need to extract this data and deposit it in a staging Data Lake for further transformations downstream. For this question I am only concerned on the best way to handle the ETL on this step, for 30+ tables on 5+ databases. Tables should be identical except for some slight differences in data types or extra columns sometimes.</p>\n\n<p>I need to be able to &quot;re-run&quot; the full ETL for a single table in one database. </p>\n\n<p>I am relatively new working with Airflow by the way, and learned SubDAGs are being phased out. Are SubDAGs the best option? The general ETL process can be reutilized for all database and table combinations.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1go4w1y', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='LateDay'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go4w1y/any_ideas_on_how_to_design_airflow_dagstasks_for/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go4w1y/any_ideas_on_how_to_design_airflow_dagstasks_for/', 'subreddit_subscribers': 227915, 'created_utc': 1731257249.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.215+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi, does anybody know how to test Snowplow analytics locally end-to-end. \n\nI tried the Snowplow micro but it doesn’t include ingestion of data to a data warehouse.\n\nI want to test:\n- event generation with their SDK\n- validation with Iglu server\n- data ingestion to something like BigQuery\n- their Dbt models\n\n\nIs there an example project that works for this? Everything I found is unmaintained.\n\nThank you in advance!', 'author_fullname': 't2_gnytqihqi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Snowplow Example', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go45aw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731255278.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, does anybody know how to test Snowplow analytics locally end-to-end. </p>\n\n<p>I tried the Snowplow micro but it doesn’t include ingestion of data to a data warehouse.</p>\n\n<p>I want to test:\n- event generation with their SDK\n- validation with Iglu server\n- data ingestion to something like BigQuery\n- their Dbt models</p>\n\n<p>Is there an example project that works for this? Everything I found is unmaintained.</p>\n\n<p>Thank you in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1go45aw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MitzuIstvan'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go45aw/snowplow_example/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go45aw/snowplow_example/', 'subreddit_subscribers': 227915, 'created_utc': 1731255278.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.215+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_qvzmu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Grab Employs LLMs for Conversational Data Discovery with GPT-4, Glean and Slack', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnzbpt', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/MfYZGEgCsIdIQNyY3sCAvfOYWQ_dBPgIJR4ZBSrjiRw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731240729.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'infoq.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.infoq.com/news/2024/11/grab-data-discovery-llm-slack/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?auto=webp&s=c058b65b443d4709ea749c694a3c1ac7609058f9', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=108&crop=smart&auto=webp&s=a2bf77d283cf6f027ccce213c674714b19fc296a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=216&crop=smart&auto=webp&s=05db1902d6564d2f833d197baf3c03ab0d009e3a', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=320&crop=smart&auto=webp&s=bbc3bf11a479939c979454ad69878454968bdb3e', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=640&crop=smart&auto=webp&s=aca9a69c989c0b38f6494525dffd658078c804b8', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=960&crop=smart&auto=webp&s=25e8e6ead51fdbcc82216cbf6b874df895ad90a8', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/WM7cWQpzj9-Yvj09i6KayTtkzt1YOCrliGPoPYGgLzk.jpg?width=1080&crop=smart&auto=webp&s=808237aecdf7af17ae4a31d6708d2dbda6c6b222', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'VVBvaiBXStZ2s9QUipZmD1amxR31BOOZKBBZ90UPSkk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnzbpt', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='rgancarz'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnzbpt/grab_employs_llms_for_conversational_data/', 'stickied': False, 'url': 'https://www.infoq.com/news/2024/11/grab-data-discovery-llm-slack/', 'subreddit_subscribers': 227915, 'created_utc': 1731240729.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.215+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello everyone,\n\nI’m a student researching renewable energy, specifically working on a system to convert mechanical energy from ocean waves into electricity. My objective is to design a system that maintains a tangential alignment with the ocean wave surface at all times (meaning the angle between the system and the wave surface is always zero). This alignment should optimize energy transfer.\n\nI’m looking for advice on the best way to determine the ideal shape for this system. One idea I have is to create a Python program that simulates different shapes and tests how well they maintain a tangential alignment with waves in a simulated environment. Additionally, I’d like to explore if I could use AI to automatically generate and test shapes, ultimately helping me find the most effective design. While I have some Python experience, so any guidance would be helpful.\n\nThank you for your help!', 'author_fullname': 't2_94xvey5m', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Researching Energy: How to Find the Best Shape Using AI', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnxskd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.61, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731234546.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone,</p>\n\n<p>I’m a student researching renewable energy, specifically working on a system to convert mechanical energy from ocean waves into electricity. My objective is to design a system that maintains a tangential alignment with the ocean wave surface at all times (meaning the angle between the system and the wave surface is always zero). This alignment should optimize energy transfer.</p>\n\n<p>I’m looking for advice on the best way to determine the ideal shape for this system. One idea I have is to create a Python program that simulates different shapes and tests how well they maintain a tangential alignment with waves in a simulated environment. Additionally, I’d like to explore if I could use AI to automatically generate and test shapes, ultimately helping me find the most effective design. While I have some Python experience, so any guidance would be helpful.</p>\n\n<p>Thank you for your help!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1gnxskd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='WhereasGlum9389'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnxskd/researching_energy_how_to_find_the_best_shape/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gnxskd/researching_energy_how_to_find_the_best_shape/', 'subreddit_subscribers': 227915, 'created_utc': 1731234546.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.216+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'As the title: We offer Perplexity AI PRO voucher codes for one year plan.   \n\nTo Order: https://cheapgpts.store/Perplexity\n\nPayments accepted:  \n\n- PayPal. (100% Buyer protected)  \n- Revolut.', 'author_fullname': 't2_i0t1ebp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Perplexity AI PRO - 1 YEAR PLAN OFFER - 75% OFF', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1goddcd', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.13, 'author_flair_background_color': None, 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/W3M6uqlOID1nrke94VQBwhsUrVfyVK6CS25rphz41ZU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731279202.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>As the title: We offer Perplexity AI PRO voucher codes for one year plan.   </p>\n\n<p>To Order: <a href="https://cheapgpts.store/Perplexity">https://cheapgpts.store/Perplexity</a></p>\n\n<p>Payments accepted:  </p>\n\n<ul>\n<li>PayPal. (100% Buyer protected)<br/></li>\n<li>Revolut.</li>\n</ul>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/scpc44vvl50e1.jpeg', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/scpc44vvl50e1.jpeg?auto=webp&s=64344400d18c2e46a1e3b3c409649c2e7b73dd0e', 'width': 2000, 'height': 2000}, 'resolutions': [{'url': 'https://preview.redd.it/scpc44vvl50e1.jpeg?width=108&crop=smart&auto=webp&s=610583125ab6167c0f8f44815f28e6baff853247', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/scpc44vvl50e1.jpeg?width=216&crop=smart&auto=webp&s=acd67238c1fe982282f09bcbc9a1c1d265748e1f', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/scpc44vvl50e1.jpeg?width=320&crop=smart&auto=webp&s=7db944f316987149c8602f45db9c268c7698cc38', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/scpc44vvl50e1.jpeg?width=640&crop=smart&auto=webp&s=820c7065b1fc109b4efa80d29e2d9308fe5fb851', 'width': 640, 'height': 640}, {'url': 'https://preview.redd.it/scpc44vvl50e1.jpeg?width=960&crop=smart&auto=webp&s=1aee3de27630d0abcb2167fce7399bf358541b4c', 'width': 960, 'height': 960}, {'url': 'https://preview.redd.it/scpc44vvl50e1.jpeg?width=1080&crop=smart&auto=webp&s=900c4137d5ca618ba99c812d811272dfaebfd3a6', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': '-rA_mYqKxyG-Ejr0PebmVogbyfI4wywNSERsR3Wj6Xc'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1goddcd', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MReus11R'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1goddcd/perplexity_ai_pro_1_year_plan_offer_75_off/', 'stickied': False, 'url': 'https://i.redd.it/scpc44vvl50e1.jpeg', 'subreddit_subscribers': 227915, 'created_utc': 1731279202.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.217+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I work as a Data Engineer with around 14 to 15 years of professional experience My current title at work is Data Engineer which has been bothering me for a while as I should really be at least Senior Data Engineer based on the value I add to the team and my experience but there isn’t any room for growth in the team. In the last couple of months I’ve been approached by two very good firms with senior roles and they have both offered me those roles and I need to decide which way to go.\n\nOption 1 industry - automotive title - Senior Data Engineer salary bump - 10K Work - Will be working with young and smart people and data sets are pretty interesting and will be using cutting edge tech - I’m very excited for this opportunity. This is individual contribution role where I’ll also doing mentoring and coaching\n\nOption 2 industry - financial services title - Principal Data Engineer salary bump - 40K Work - might not be as interesting as option 1. Tech stack might not be cutting edge and the datasets volumes isn’t massive - may have red tapes and limited freedom to innovation due to the nature of business(compliance and heavily regulated) - This is individual contribution role where I’ll also doing mentoring and coaching\n\nI’m inclining towards option 1 but if I do that, then it means I’ll be losing 30K which isn’t a small number.\n\nI want to take your view in terms of which option you think would be beneficial for me long term? I feel like going with option 1 would give me better market value if I were to leave them in 2 to 3 years but if I go with option 2 I may not have a great market value as the role lacks good tech stack and provide rigid working environment.\n\nIn 2 to 3 years, ideally I would like to be working as a lead within data engineering space where I’ll be leading teams so the sort of titles I would be interested in would be head of engineering or Data lead or Head of data.\n\nAny thoughts would be really helpful.', 'author_fullname': 't2_116yuq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Which way to go? Data Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go626j', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731260271.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I work as a Data Engineer with around 14 to 15 years of professional experience My current title at work is Data Engineer which has been bothering me for a while as I should really be at least Senior Data Engineer based on the value I add to the team and my experience but there isn’t any room for growth in the team. In the last couple of months I’ve been approached by two very good firms with senior roles and they have both offered me those roles and I need to decide which way to go.</p>\n\n<p>Option 1 industry - automotive title - Senior Data Engineer salary bump - 10K Work - Will be working with young and smart people and data sets are pretty interesting and will be using cutting edge tech - I’m very excited for this opportunity. This is individual contribution role where I’ll also doing mentoring and coaching</p>\n\n<p>Option 2 industry - financial services title - Principal Data Engineer salary bump - 40K Work - might not be as interesting as option 1. Tech stack might not be cutting edge and the datasets volumes isn’t massive - may have red tapes and limited freedom to innovation due to the nature of business(compliance and heavily regulated) - This is individual contribution role where I’ll also doing mentoring and coaching</p>\n\n<p>I’m inclining towards option 1 but if I do that, then it means I’ll be losing 30K which isn’t a small number.</p>\n\n<p>I want to take your view in terms of which option you think would be beneficial for me long term? I feel like going with option 1 would give me better market value if I were to leave them in 2 to 3 years but if I go with option 2 I may not have a great market value as the role lacks good tech stack and provide rigid working environment.</p>\n\n<p>In 2 to 3 years, ideally I would like to be working as a lead within data engineering space where I’ll be leading teams so the sort of titles I would be interested in would be head of engineering or Data lead or Head of data.</p>\n\n<p>Any thoughts would be really helpful.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1go626j', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='lethal_drake'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go626j/which_way_to_go_data_engineering/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1go626j/which_way_to_go_data_engineering/', 'subreddit_subscribers': 227915, 'created_utc': 1731260271.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.217+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'In data systems, telemetry and reference data play complementary roles, with telemetry providing real-time updates (e.g., vehicle locations, sensor readings) and reference data supplying the essential context (e.g., route details, sensor placements). While telemetry data updates frequently and reference data less so, both types need accurate temporal alignment for reliable historical analysis. Telemetry data is only meaningful when contextualized by reference data that is temporally consistent. If a bus\'s location is tracked in real-time, the route configuration at the time of tracking is essential for understanding the bus\'s context. The blog post discusses the "how to" of treating reference data as streams.', 'author_fullname': 't2_6ggr7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Streamifying Reference Data for Temporal Consistency with Telemetry Events', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 80, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gnx02a', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/k0fWXEb_pi1imIAftJI5HZUIEWqSedbHlSXi2Eo3j8s.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731231042.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'vasters.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>In data systems, telemetry and reference data play complementary roles, with telemetry providing real-time updates (e.g., vehicle locations, sensor readings) and reference data supplying the essential context (e.g., route details, sensor placements). While telemetry data updates frequently and reference data less so, both types need accurate temporal alignment for reliable historical analysis. Telemetry data is only meaningful when contextualized by reference data that is temporally consistent. If a bus&#39;s location is tracked in real-time, the route configuration at the time of tracking is essential for understanding the bus&#39;s context. The blog post discusses the &quot;how to&quot; of treating reference data as streams.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://vasters.com/clemens/2024/10/30/streamifying-reference-data-for-temporal-consistency-with-telemetry-events', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?auto=webp&s=0ae0df57132eda87a9e643c3e3762703fedeef0a', 'width': 1792, 'height': 1024}, 'resolutions': [{'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=108&crop=smart&auto=webp&s=d7f1bdb3e86eb5e8542ad6e9444c189e6457a2f6', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=216&crop=smart&auto=webp&s=e95296dea4104bf021b8d9cf19f0823c5344858f', 'width': 216, 'height': 123}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=320&crop=smart&auto=webp&s=cefd0a45db6dd9f29f680f6d5c4f29229c0ef93e', 'width': 320, 'height': 182}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=640&crop=smart&auto=webp&s=9fcfa31aee32af1e244b52f06f975af8155c9d72', 'width': 640, 'height': 365}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=960&crop=smart&auto=webp&s=f1f1882dec911adb05abf56631da202d6b679029', 'width': 960, 'height': 548}, {'url': 'https://external-preview.redd.it/8IUDODHx7h_Uc7TnLjfVIH-a96sGLuLHy7uG3L-9iaQ.jpg?width=1080&crop=smart&auto=webp&s=baf1607044705ae37939ac58271fac91ba3ac549', 'width': 1080, 'height': 617}], 'variants': {}, 'id': 'BuZg2uTSrpUIVq2xhvajZPXSfYBFACmk-PEUStAjswE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1gnx02a', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='clemensv'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gnx02a/streamifying_reference_data_for_temporal/', 'stickied': False, 'url': 'https://vasters.com/clemens/2024/10/30/streamifying-reference-data-for-temporal-consistency-with-telemetry-events', 'subreddit_subscribers': 227915, 'created_utc': 1731231042.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.218+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'This will sound stupid I know. But I\'m genuinely trying to understand workflow orchestrators at this point. \n\nI have been using airflow for a few years in my role, been playing with Prefect. Heard about Dagster but haven\'t tried it yet. I have also worked at a company with an in-house workflow orchestrator tool that sort of was inspired by Prefect 1.0. \n\nAfter using and building etl pipelines I\'m trying to understand why? What is it that workflow orchestrators allow me to do that writing in pure python with standards enforcement at code review time couldn\'t achieve?\n\nIs there any good articles that discuss why these tools were created? I mean it sounds to me that a tool like Airflow was created at Airbnb as a "standards enforcement" to get the whole company around and was then shared to the open source community. But why all these other tools over just some general guidelines around using python\'s std lib (or insert any language) and the third party libraries to connect to services required for data processing. ', 'author_fullname': 't2_tmmwdoh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Why [do we really need] workflow orchestrators?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gob928', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.38, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1731273610.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>This will sound stupid I know. But I&#39;m genuinely trying to understand workflow orchestrators at this point. </p>\n\n<p>I have been using airflow for a few years in my role, been playing with Prefect. Heard about Dagster but haven&#39;t tried it yet. I have also worked at a company with an in-house workflow orchestrator tool that sort of was inspired by Prefect 1.0. </p>\n\n<p>After using and building etl pipelines I&#39;m trying to understand why? What is it that workflow orchestrators allow me to do that writing in pure python with standards enforcement at code review time couldn&#39;t achieve?</p>\n\n<p>Is there any good articles that discuss why these tools were created? I mean it sounds to me that a tool like Airflow was created at Airbnb as a &quot;standards enforcement&quot; to get the whole company around and was then shared to the open source community. But why all these other tools over just some general guidelines around using python&#39;s std lib (or insert any language) and the third party libraries to connect to services required for data processing. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1gob928', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='hfzvc'), 'discussion_type': None, 'num_comments': 14, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1gob928/why_do_we_really_need_workflow_orchestrators/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1gob928/why_do_we_really_need_workflow_orchestrators/', 'subreddit_subscribers': 227915, 'created_utc': 1731273610.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.218+0000] {logging_mixin.py:151} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6c506160>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone,\n\nI’m working on my Master’s thesis and would really appreciate your help! I’m conducting a survey on AI usage, trust, and employee performance, and I’m looking for participants who use AI tools (like ChatGPT, Grammarly, or similar) in their work.\n\nThe survey is anonymous and should take no more than 5 minutes to complete. Your input would be incredibly valuable for my research.\n\nHere’s the link: https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_bdqdnmVSh2PfTZs\n\nThanks so much in advance for your support!', 'author_fullname': 't2_a2bsxwsy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineers Feedback Needed!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1go2a96', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/ZkC6CWHn2ZNHLn7ZNXvMJe7vdJSrHMjiCLL4TblDi-s.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1731250212.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I’m working on my Master’s thesis and would really appreciate your help! I’m conducting a survey on AI usage, trust, and employee performance, and I’m looking for participants who use AI tools (like ChatGPT, Grammarly, or similar) in their work.</p>\n\n<p>The survey is anonymous and should take no more than 5 minutes to complete. Your input would be incredibly valuable for my research.</p>\n\n<p>Here’s the link: <a href="https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_bdqdnmVSh2PfTZs">https://maastrichtuniversity.eu.qualtrics.com/jfe/form/SV_bdqdnmVSh2PfTZs</a></p>\n\n<p>Thanks so much in advance for your support!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/bgzla0gq730e1.jpeg', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?auto=webp&s=60e48f8830eb848717c58b85ea4affba94027388', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=108&crop=smart&auto=webp&s=d5505bec2fe474e13724bf596a25679bad4705ae', 'width': 108, 'height': 56}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=216&crop=smart&auto=webp&s=f0367124d6b4f6d62d49d5a64ec325ec5c7acbd5', 'width': 216, 'height': 113}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=320&crop=smart&auto=webp&s=b4662593fef952ef5abc4655addf51ffdf22c89a', 'width': 320, 'height': 167}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=640&crop=smart&auto=webp&s=eba6f21e12bc993c71611935baa375cc53a484cb', 'width': 640, 'height': 334}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=960&crop=smart&auto=webp&s=2c05c8923d39103438e57b6481344916211b3990', 'width': 960, 'height': 502}, {'url': 'https://preview.redd.it/bgzla0gq730e1.jpeg?width=1080&crop=smart&auto=webp&s=b618cff4b47a8f3d40195119ed2312ff00e8457f', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'ARlU1690rNEE90gHpifbhDj9JpG312F30ETCTebF9rM'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1go2a96', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Top_Sheepherder_2929'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1go2a96/data_engineers_feedback_needed/', 'stickied': False, 'url': 'https://i.redd.it/bgzla0gq730e1.jpeg', 'subreddit_subscribers': 227915, 'created_utc': 1731250212.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-11-11T00:00:06.310+0000] {python.py:194} INFO - Done. Returned value was: /opt/airflow/data/output/reddit_20241111.csv
[2024-11-11T00:00:06.357+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20241110T000000, start_date=20241111T000004, end_date=20241111T000006
[2024-11-11T00:00:06.436+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-11-11T00:00:06.514+0000] {taskinstance.py:2778} INFO - 1 downstream tasks scheduled from follow-on schedule check
