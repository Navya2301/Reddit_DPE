[2024-11-03T00:08:06.439+0000] {processor.py:186} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:08:06.441+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:08:06.457+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:08:06.454+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:08:06.513+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:08:06.510+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    def connect_reddit(client_id, client_secret, user_agent) -> Reddit :
NameError: name 'Reddit' is not defined
[2024-11-03T00:08:06.513+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:08:06.541+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.113 seconds
[2024-11-03T00:08:36.918+0000] {processor.py:186} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:08:36.920+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:08:36.925+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:08:36.925+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:08:36.997+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:08:36.994+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    def connect_reddit(client_id, client_secret, user_agent) -> Reddit :
NameError: name 'Reddit' is not defined
[2024-11-03T00:08:36.998+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:08:37.028+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.120 seconds
[2024-11-03T00:09:07.380+0000] {processor.py:186} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:09:07.391+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:09:07.399+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:09:07.398+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:09:07.467+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:09:07.463+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    def connect_reddit(client_id, client_secret, user_agent) -> Reddit :
NameError: name 'Reddit' is not defined
[2024-11-03T00:09:07.467+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:09:07.500+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.139 seconds
[2024-11-03T00:09:57.471+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:09:57.477+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:09:57.491+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:09:57.490+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:09:57.680+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:09:57.677+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:09:57.681+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:09:57.737+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.290 seconds
[2024-11-03T00:10:27.960+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:10:27.962+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:10:27.967+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:10:27.966+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:10:28.039+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:10:28.037+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:10:28.039+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:10:28.066+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.116 seconds
[2024-11-03T00:10:58.470+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:10:58.472+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:10:58.477+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:10:58.476+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:10:58.534+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:10:58.533+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:10:58.535+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:10:58.559+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.119 seconds
[2024-11-03T00:11:28.962+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:11:28.964+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:11:28.969+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:11:28.968+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:11:29.021+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:11:29.019+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:11:29.022+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:11:29.060+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.113 seconds
[2024-11-03T00:11:59.450+0000] {processor.py:186} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:11:59.452+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:11:59.475+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:11:59.471+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:11:59.716+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:11:59.712+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:11:59.718+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:11:59.729+0000] {processor.py:211} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.9/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 928, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/api_internal/internal_api_call.py", line 139, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 648, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(ParseImportError.filename).all()]
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.9/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-11-03T00:12:31.752+0000] {processor.py:186} INFO - Started process (PID=28) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:12:31.757+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:12:31.768+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:12:31.767+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:12:31.856+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:12:31.854+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:12:31.857+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:12:31.883+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.149 seconds
[2024-11-03T00:13:02.244+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:13:02.247+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:13:02.253+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:13:02.252+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:13:02.321+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:13:02.318+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:13:02.322+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:13:02.385+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.171 seconds
[2024-11-03T00:13:32.776+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:13:32.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:13:32.783+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:13:32.782+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:13:32.852+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:13:32.851+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:13:32.853+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:13:32.884+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.135 seconds
[2024-11-03T00:14:03.312+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:14:03.314+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:14:03.320+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:14:03.319+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:14:03.393+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:14:03.389+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:14:03.393+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:14:03.427+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.133 seconds
[2024-11-03T00:14:33.811+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:14:33.813+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:14:33.818+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:14:33.817+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:14:33.880+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:14:33.878+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:14:33.880+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:14:33.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.119 seconds
[2024-11-03T00:15:04.335+0000] {processor.py:186} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:15:04.337+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:15:04.343+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:15:04.342+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:15:04.409+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:15:04.408+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:15:04.410+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:15:04.462+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.143 seconds
[2024-11-03T00:15:34.753+0000] {processor.py:186} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:15:34.755+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:15:34.761+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:15:34.760+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:15:34.815+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:15:34.813+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:15:34.815+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:15:34.845+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.110 seconds
[2024-11-03T00:16:05.325+0000] {processor.py:186} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:16:05.331+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:16:05.348+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:16:05.348+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:16:05.403+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:16:05.401+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ImportError: cannot import name 'reddit_pipeline' from 'pipelines.reddit_pipeline' (/opt/airflow/pipelines/reddit_pipeline.py)
[2024-11-03T00:16:05.404+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:16:05.435+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.133 seconds
[2024-11-03T00:16:35.804+0000] {processor.py:186} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:16:35.806+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:16:35.812+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:16:35.811+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:16:35.899+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:16:35.898+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 47, in <module>
    python_callable = upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-11-03T00:16:35.900+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:16:35.930+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.144 seconds
[2024-11-03T00:17:10.544+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:17:10.547+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:17:10.552+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:17:10.551+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:17:10.669+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:17:10.664+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 47, in <module>
    python_callable = upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-11-03T00:17:10.670+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:17:10.699+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.169 seconds
[2024-11-03T00:17:41.093+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:17:41.095+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:17:41.101+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:17:41.100+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:17:41.164+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:17:41.162+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 47, in <module>
    python_callable = upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-11-03T00:17:41.164+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:17:41.193+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.114 seconds
[2024-11-03T00:18:11.650+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:11.651+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:18:11.656+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:11.656+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:11.742+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:11.741+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 47, in <module>
    python_callable = upload_s3_pipeline,
NameError: name 'upload_s3_pipeline' is not defined
[2024-11-03T00:18:11.743+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:11.771+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.133 seconds
[2024-11-03T00:18:20.863+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:20.865+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:18:20.870+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:20.869+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:20.943+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:21.175+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.174+0000] {override.py:1907} INFO - Created Permission View: can read on DAG:etl_reddit_pipeline
[2024-11-03T00:18:21.182+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.182+0000] {override.py:1907} INFO - Created Permission View: can delete on DAG:etl_reddit_pipeline
[2024-11-03T00:18:21.186+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.186+0000] {override.py:1907} INFO - Created Permission View: can edit on DAG:etl_reddit_pipeline
[2024-11-03T00:18:21.190+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.190+0000] {override.py:1907} INFO - Created Permission View: menu access on DAG Run:etl_reddit_pipeline
[2024-11-03T00:18:21.193+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.193+0000] {override.py:1907} INFO - Created Permission View: can read on DAG Run:etl_reddit_pipeline
[2024-11-03T00:18:21.197+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.196+0000] {override.py:1907} INFO - Created Permission View: can delete on DAG Run:etl_reddit_pipeline
[2024-11-03T00:18:21.201+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.200+0000] {override.py:1907} INFO - Created Permission View: can create on DAG Run:etl_reddit_pipeline
[2024-11-03T00:18:21.201+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.201+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:18:21.213+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.213+0000] {dag.py:3262} INFO - Creating ORM DAG for etl_reddit_pipeline
[2024-11-03T00:18:21.223+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:21.223+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-02 00:00:00+00:00, run_after=2024-11-03 00:00:00+00:00
[2024-11-03T00:18:21.240+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.417 seconds
[2024-11-03T00:18:50.826+0000] {processor.py:186} INFO - Started process (PID=28) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:50.829+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:18:50.835+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:50.834+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:50.962+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:18:50.995+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:50.994+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:18:51.038+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:18:51.037+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-02 00:00:00+00:00, run_after=2024-11-03 00:00:00+00:00
[2024-11-03T00:18:51.086+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.279 seconds
[2024-11-03T00:19:21.306+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:19:21.308+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:19:21.315+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:19:21.314+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:19:21.389+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:19:21.445+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:19:21.445+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:19:21.471+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:19:21.471+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-02 00:00:00+00:00, run_after=2024-11-03 00:00:00+00:00
[2024-11-03T00:19:21.486+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.210 seconds
[2024-11-03T00:19:51.922+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:19:51.924+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:19:51.929+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:19:51.928+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:19:52.016+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:19:52.069+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:19:52.069+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:19:52.095+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:19:52.095+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:19:52.110+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.215 seconds
[2024-11-03T00:20:22.395+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:20:22.398+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:20:22.404+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:20:22.403+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:20:22.472+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:20:22.561+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:20:22.560+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:20:22.585+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:20:22.585+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:20:22.599+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.218 seconds
[2024-11-03T00:20:53.003+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:20:53.005+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:20:53.009+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:20:53.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:20:53.074+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:20:53.126+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:20:53.126+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:20:53.156+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:20:53.156+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:20:53.174+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.198 seconds
[2024-11-03T00:21:23.468+0000] {processor.py:186} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:21:23.470+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:21:23.475+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:21:23.474+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:21:23.548+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:21:23.581+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:21:23.581+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:21:23.604+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:21:23.603+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:21:23.618+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.164 seconds
[2024-11-03T00:21:53.988+0000] {processor.py:186} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:21:53.990+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:21:53.994+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:21:53.993+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:21:54.052+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:21:54.087+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:21:54.087+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:21:54.112+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:21:54.111+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:21:54.128+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.153 seconds
[2024-11-03T00:22:24.436+0000] {processor.py:186} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:22:24.437+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:22:24.441+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:22:24.441+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:22:24.497+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:22:24.543+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:22:24.543+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:22:24.567+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:22:24.567+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:22:24.581+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.160 seconds
[2024-11-03T00:22:55.031+0000] {processor.py:186} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:22:55.034+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:22:55.043+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:22:55.043+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:22:55.125+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:22:55.189+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:22:55.188+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:22:55.218+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:22:55.218+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:22:55.237+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.225 seconds
[2024-11-03T00:23:25.572+0000] {processor.py:186} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:23:25.574+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:23:25.579+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:23:25.578+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:23:25.641+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:23:25.676+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:23:25.675+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:23:25.707+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:23:25.707+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:23:25.725+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.188 seconds
[2024-11-03T00:23:56.262+0000] {processor.py:186} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:23:56.264+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:23:56.269+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:23:56.268+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:23:56.344+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:23:56.382+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:23:56.382+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:23:56.414+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:23:56.413+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:23:56.429+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.187 seconds
[2024-11-03T00:24:26.777+0000] {processor.py:186} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:24:26.778+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:24:26.783+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:24:26.782+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:24:26.857+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:24:26.894+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:24:26.894+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:24:26.921+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:24:26.920+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:24:26.937+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.173 seconds
[2024-11-03T00:24:57.239+0000] {processor.py:186} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:24:57.241+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:24:57.246+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:24:57.245+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:24:57.318+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:24:57.353+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:24:57.352+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:24:57.378+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:24:57.378+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:24:57.393+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.166 seconds
[2024-11-03T00:25:27.751+0000] {processor.py:186} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:25:27.753+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:25:27.757+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:25:27.756+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:25:27.819+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:25:27.853+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:25:27.853+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:25:27.893+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:25:27.893+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:25:27.911+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.205 seconds
[2024-11-03T00:25:58.160+0000] {processor.py:186} INFO - Started process (PID=42) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:25:58.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:25:58.166+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:25:58.165+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:25:58.266+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:25:58.307+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:25:58.307+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:25:58.335+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:25:58.335+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:25:58.351+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.206 seconds
[2024-11-03T00:26:55.550+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:26:55.553+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:26:55.563+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:26:55.562+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:26:55.808+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:26:55.911+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:26:55.910+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:26:56.018+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:26:56.017+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:26:56.056+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.524 seconds
[2024-11-03T00:27:26.323+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:27:26.329+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:27:26.343+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:27:26.342+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:27:26.492+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:27:26.630+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:27:26.629+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:27:26.733+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:27:26.732+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:27:26.780+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-11-03T00:27:56.937+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:27:56.939+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:27:56.946+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:27:56.945+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:27:57.020+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:27:57.054+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:27:57.053+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:27:57.079+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:27:57.079+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:27:57.094+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.170 seconds
[2024-11-03T00:28:27.412+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:28:27.413+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:28:27.419+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:28:27.418+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:28:27.508+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:28:27.543+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:28:27.543+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:28:27.592+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:28:27.591+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:28:27.614+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.215 seconds
[2024-11-03T00:28:57.808+0000] {processor.py:186} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:28:57.812+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:28:57.823+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:28:57.823+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:28:57.891+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:28:57.929+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:28:57.928+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:28:57.955+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:28:57.955+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:28:57.979+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.191 seconds
[2024-11-03T00:29:28.436+0000] {processor.py:186} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:29:28.438+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:29:28.444+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:29:28.443+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:29:28.511+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:29:28.549+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:29:28.548+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:29:28.574+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:29:28.574+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:29:28.597+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.189 seconds
[2024-11-03T00:29:58.903+0000] {processor.py:186} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:29:58.904+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:29:58.909+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:29:58.909+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:29:58.992+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:29:59.031+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:29:59.031+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:29:59.066+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:29:59.065+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:29:59.086+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.197 seconds
[2024-11-03T00:30:29.467+0000] {processor.py:186} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:30:29.469+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:30:29.474+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:30:29.474+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:30:29.536+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:30:29.573+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:30:29.573+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:30:29.620+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:30:29.620+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:30:29.646+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.214 seconds
[2024-11-03T00:31:00.000+0000] {processor.py:186} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:31:00.002+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:31:00.010+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:31:00.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:31:00.087+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:31:00.129+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:31:00.129+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:31:00.154+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:31:00.154+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:31:00.169+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.218 seconds
[2024-11-03T00:31:30.476+0000] {processor.py:186} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:31:30.478+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:31:30.484+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:31:30.483+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:31:30.586+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:31:30.644+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:31:30.644+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:31:30.674+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:31:30.674+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:31:30.688+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.228 seconds
[2024-11-03T00:32:01.046+0000] {processor.py:186} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:32:01.048+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:32:01.054+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:32:01.053+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:32:01.140+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:32:01.197+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:32:01.197+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:32:01.228+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:32:01.228+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:32:01.246+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.218 seconds
[2024-11-03T00:32:31.619+0000] {processor.py:186} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:32:31.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:32:31.631+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:32:31.630+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:32:31.714+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:32:31.768+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:32:31.768+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:32:31.796+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:32:31.796+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:32:31.817+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.216 seconds
[2024-11-03T00:33:02.191+0000] {processor.py:186} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:33:02.193+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:33:02.199+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:33:02.199+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:33:02.279+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:33:02.325+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:33:02.324+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:33:02.349+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:33:02.349+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:33:02.363+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.185 seconds
[2024-11-03T00:33:32.609+0000] {processor.py:186} INFO - Started process (PID=42) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:33:32.611+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:33:32.634+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:33:32.634+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:33:32.711+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:33:32.762+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:33:32.761+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:33:32.795+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:33:32.795+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:33:32.844+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.251 seconds
[2024-11-03T00:34:03.332+0000] {processor.py:186} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:34:03.334+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:34:03.338+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:34:03.338+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:34:03.417+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:34:03.459+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:34:03.459+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:34:03.492+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:34:03.491+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:34:03.526+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.206 seconds
[2024-11-03T00:34:33.808+0000] {processor.py:186} INFO - Started process (PID=44) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:34:33.810+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:34:33.817+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:34:33.815+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:34:33.910+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:34:33.970+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:34:33.969+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:34:33.998+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:34:33.997+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:34:34.014+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.231 seconds
[2024-11-03T00:35:04.415+0000] {processor.py:186} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:35:04.416+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:35:04.421+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:35:04.421+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:35:04.511+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:35:04.556+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:35:04.556+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:35:04.586+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:35:04.585+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:35:04.601+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.199 seconds
[2024-11-03T00:35:34.839+0000] {processor.py:186} INFO - Started process (PID=46) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:35:34.840+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:35:34.845+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:35:34.844+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:35:34.943+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:35:34.984+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:35:34.984+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:35:35.012+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:35:35.012+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:35:35.031+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.206 seconds
[2024-11-03T00:36:05.445+0000] {processor.py:186} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:36:05.447+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:36:05.454+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:36:05.454+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:36:05.543+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:36:05.595+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:36:05.595+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:36:05.661+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:36:05.661+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:36:05.680+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.254 seconds
[2024-11-03T00:36:35.953+0000] {processor.py:186} INFO - Started process (PID=48) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:36:35.955+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:36:35.962+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:36:35.961+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:36:36.032+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:36:36.093+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:36:36.092+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:36:36.123+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:36:36.122+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:36:36.139+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.219 seconds
[2024-11-03T00:37:06.539+0000] {processor.py:186} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:37:06.541+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:37:06.546+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:37:06.545+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:37:06.619+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:37:06.654+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:37:06.653+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:37:06.693+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:37:06.692+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:37:06.710+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.180 seconds
[2024-11-03T00:37:36.965+0000] {processor.py:186} INFO - Started process (PID=50) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:37:36.968+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:37:36.983+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:37:36.982+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:37:37.060+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:37:37.118+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:37:37.117+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:37:37.146+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:37:37.146+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:37:37.164+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.222 seconds
[2024-11-03T00:38:07.481+0000] {processor.py:186} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:38:07.482+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:38:07.488+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:38:07.488+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:38:07.578+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:38:07.625+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:38:07.625+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:38:07.657+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:38:07.657+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:38:07.687+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.221 seconds
[2024-11-03T00:38:37.839+0000] {processor.py:186} INFO - Started process (PID=52) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:38:37.841+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:38:37.846+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:38:37.846+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:38:37.945+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:38:38.001+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:38:38.000+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:38:38.030+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:38:38.030+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:38:38.046+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.221 seconds
[2024-11-03T00:39:08.288+0000] {processor.py:186} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:39:08.290+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:39:08.297+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:39:08.296+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:39:08.391+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:39:08.437+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:39:08.436+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:39:08.488+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:39:08.488+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:39:08.505+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.233 seconds
[2024-11-03T00:39:38.988+0000] {processor.py:186} INFO - Started process (PID=54) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:39:38.990+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:39:38.996+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:39:38.995+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:39:39.061+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:39:39.121+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:39:39.120+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:39:39.158+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:39:39.158+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:39:39.178+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.225 seconds
[2024-11-03T00:40:09.429+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:40:09.431+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:40:09.436+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:40:09.435+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:40:09.501+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:40:09.552+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:40:09.552+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:40:09.585+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:40:09.585+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:40:09.603+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.188 seconds
[2024-11-03T00:40:40.031+0000] {processor.py:186} INFO - Started process (PID=56) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:40:40.033+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:40:40.040+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:40:40.039+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:40:40.106+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:40:40.141+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:40:40.141+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:40:40.188+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:40:40.188+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:40:40.207+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.193 seconds
[2024-11-03T00:41:10.522+0000] {processor.py:186} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:41:10.524+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:41:10.530+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:41:10.529+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:41:10.590+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:41:10.629+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:41:10.629+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:41:10.693+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:41:10.693+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:41:10.712+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.221 seconds
[2024-11-03T00:41:40.986+0000] {processor.py:186} INFO - Started process (PID=58) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:41:40.988+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:41:40.994+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:41:40.993+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:41:41.080+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:41:41.125+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:41:41.125+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:41:41.156+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:41:41.156+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:41:41.182+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.213 seconds
[2024-11-03T00:42:11.548+0000] {processor.py:186} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:42:11.550+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:42:11.562+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:42:11.561+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:42:11.644+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:42:11.688+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:42:11.688+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:42:11.732+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:42:11.732+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:42:11.750+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.227 seconds
[2024-11-03T00:42:42.118+0000] {processor.py:186} INFO - Started process (PID=60) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:42:42.120+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:42:42.127+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:42:42.126+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:42:42.192+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:42:42.259+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:42:42.259+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:42:42.305+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:42:42.305+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:42:42.336+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.258 seconds
[2024-11-03T00:43:12.648+0000] {processor.py:186} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:43:12.650+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:43:12.659+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:43:12.658+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:43:12.740+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:43:12.776+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:43:12.776+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:43:12.818+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:43:12.818+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:43:12.840+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.218 seconds
[2024-11-03T00:43:43.190+0000] {processor.py:186} INFO - Started process (PID=62) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:43:43.192+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:43:43.196+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:43:43.196+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:43:43.255+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:43:43.327+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:43:43.327+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:43:43.360+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:43:43.360+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:43:43.383+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.208 seconds
[2024-11-03T00:44:13.732+0000] {processor.py:186} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:44:13.734+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:44:13.741+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:44:13.740+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:44:13.807+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:44:13.854+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:44:13.854+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:44:13.891+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:44:13.890+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:44:13.907+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.206 seconds
[2024-11-03T00:44:44.248+0000] {processor.py:186} INFO - Started process (PID=64) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:44:44.250+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:44:44.256+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:44:44.256+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:44:44.332+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:44:44.390+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:44:44.390+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:44:44.427+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:44:44.427+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:44:44.445+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.216 seconds
[2024-11-03T00:45:14.758+0000] {processor.py:186} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:45:14.760+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:45:14.766+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:45:14.766+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:45:14.833+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:45:14.896+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:45:14.896+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:45:14.929+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:45:14.929+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:45:14.949+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.226 seconds
[2024-11-03T00:45:45.303+0000] {processor.py:186} INFO - Started process (PID=66) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:45:45.306+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:45:45.313+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:45:45.311+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:45:45.379+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:45:45.433+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:45:45.433+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:45:45.466+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:45:45.465+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:45:45.487+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.202 seconds
[2024-11-03T00:46:15.711+0000] {processor.py:186} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:46:15.713+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:46:15.722+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:46:15.721+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:46:15.813+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:46:15.857+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:46:15.857+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:46:15.887+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:46:15.886+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:46:15.903+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.207 seconds
[2024-11-03T00:46:46.341+0000] {processor.py:186} INFO - Started process (PID=68) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:46:46.343+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:46:46.356+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:46:46.352+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:46:46.418+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:46:46.452+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:46:46.452+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:46:46.481+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:46:46.481+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:46:46.518+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.191 seconds
[2024-11-03T00:47:16.878+0000] {processor.py:186} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:47:16.880+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:47:16.885+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:47:16.885+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:47:16.965+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:47:17.009+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:47:17.008+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:47:17.039+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:47:17.039+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:47:17.056+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.196 seconds
[2024-11-03T00:47:47.417+0000] {processor.py:186} INFO - Started process (PID=70) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:47:47.419+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:47:47.426+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:47:47.425+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:47:47.496+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:47:47.565+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:47:47.565+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:47:47.594+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:47:47.594+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:47:47.609+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.233 seconds
[2024-11-03T00:48:17.853+0000] {processor.py:186} INFO - Started process (PID=71) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:48:17.854+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:48:17.859+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:48:17.859+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:48:17.924+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:48:17.957+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:48:17.957+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:48:17.986+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:48:17.986+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:48:18.005+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.168 seconds
[2024-11-03T00:48:48.374+0000] {processor.py:186} INFO - Started process (PID=72) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:48:48.376+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:48:48.384+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:48:48.384+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:48:48.441+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:48:48.477+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:48:48.477+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:48:48.518+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:48:48.518+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:48:48.549+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.211 seconds
[2024-11-03T00:49:18.778+0000] {processor.py:186} INFO - Started process (PID=73) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:49:18.780+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:49:18.786+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:49:18.786+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:49:18.854+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:49:18.914+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:49:18.914+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:49:18.948+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:49:18.947+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:49:18.965+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.210 seconds
[2024-11-03T00:49:49.463+0000] {processor.py:186} INFO - Started process (PID=74) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:49:49.464+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:49:49.470+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:49:49.469+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:49:49.566+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:49:49.617+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:49:49.616+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:49:49.648+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:49:49.648+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:49:49.671+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.230 seconds
[2024-11-03T00:50:19.900+0000] {processor.py:186} INFO - Started process (PID=75) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:50:19.902+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:50:19.928+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:50:19.927+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:50:19.995+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:50:20.034+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:50:20.034+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:50:20.065+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:50:20.065+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:50:20.083+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.199 seconds
[2024-11-03T00:50:50.479+0000] {processor.py:186} INFO - Started process (PID=76) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:50:50.480+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:50:50.485+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:50:50.485+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:50:50.543+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:50:50.599+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:50:50.599+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:50:50.646+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:50:50.646+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:50:50.666+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.219 seconds
[2024-11-03T00:51:20.905+0000] {processor.py:186} INFO - Started process (PID=77) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:51:20.907+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:51:20.913+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:51:20.912+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:51:20.995+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:51:21.041+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:51:21.040+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:51:21.072+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:51:21.072+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:51:21.088+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.199 seconds
[2024-11-03T00:51:51.495+0000] {processor.py:186} INFO - Started process (PID=78) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:51:51.497+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:51:51.503+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:51:51.502+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:51:51.565+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:51:51.642+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:51:51.642+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:51:51.680+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:51:51.680+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:51:51.697+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.228 seconds
[2024-11-03T00:52:21.981+0000] {processor.py:186} INFO - Started process (PID=79) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:52:21.983+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:52:21.989+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:52:21.988+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:52:22.063+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:52:22.131+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:52:22.130+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:52:22.164+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:52:22.164+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:52:22.182+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.253 seconds
[2024-11-03T00:52:52.591+0000] {processor.py:186} INFO - Started process (PID=80) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:52:52.593+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:52:52.602+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:52:52.601+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:52:52.670+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:52:52.704+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:52:52.704+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:52:52.752+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:52:52.752+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:52:52.770+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.216 seconds
[2024-11-03T00:53:23.070+0000] {processor.py:186} INFO - Started process (PID=81) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:53:23.071+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:53:23.075+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:53:23.074+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:53:23.130+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:53:23.159+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:53:23.159+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:53:23.186+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:53:23.186+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:53:23.203+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.146 seconds
[2024-11-03T00:53:53.621+0000] {processor.py:186} INFO - Started process (PID=82) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:53:53.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:53:53.629+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:53:53.629+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:53:53.693+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:53:53.736+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:53:53.735+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:53:53.791+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:53:53.790+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:53:53.808+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.226 seconds
[2024-11-03T00:54:24.080+0000] {processor.py:186} INFO - Started process (PID=83) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:54:24.081+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:54:24.088+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:54:24.087+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:54:24.157+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:54:24.193+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:54:24.193+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:54:24.222+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:54:24.222+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:54:24.241+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.177 seconds
[2024-11-03T00:54:54.893+0000] {processor.py:186} INFO - Started process (PID=84) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:54:54.895+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:54:54.902+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:54:54.901+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:54:55.053+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:54:55.117+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:54:55.116+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:54:55.163+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:54:55.163+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:54:55.269+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.419 seconds
[2024-11-03T00:55:25.661+0000] {processor.py:186} INFO - Started process (PID=85) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:55:25.663+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:55:25.669+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:55:25.669+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:55:25.822+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:55:25.899+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:55:25.898+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:55:25.949+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:55:25.949+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:55:26.001+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.353 seconds
[2024-11-03T00:55:56.460+0000] {processor.py:186} INFO - Started process (PID=86) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:55:56.462+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:55:56.468+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:55:56.468+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:55:56.567+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:55:56.644+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:55:56.643+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:55:56.724+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:55:56.724+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:55:56.742+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.327 seconds
[2024-11-03T00:56:26.879+0000] {processor.py:186} INFO - Started process (PID=87) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:56:26.881+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:56:26.886+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:56:26.886+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:56:26.987+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:56:27.031+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:56:27.030+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:56:27.070+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:56:27.070+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:56:27.100+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.237 seconds
[2024-11-03T00:56:57.704+0000] {processor.py:186} INFO - Started process (PID=88) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:56:57.705+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:56:57.710+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:56:57.710+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:56:57.788+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:56:57.831+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:56:57.831+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:56:57.861+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:56:57.861+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:56:57.878+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.187 seconds
[2024-11-03T00:57:28.245+0000] {processor.py:186} INFO - Started process (PID=89) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:57:28.247+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:57:28.253+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:57:28.253+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:57:28.373+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:57:28.422+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:57:28.422+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:57:28.455+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:57:28.454+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:57:28.481+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.256 seconds
[2024-11-03T00:57:58.626+0000] {processor.py:186} INFO - Started process (PID=90) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:57:58.627+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:57:58.634+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:57:58.633+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:57:58.792+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:57:58.837+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:57:58.837+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:57:58.890+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:57:58.889+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:57:58.910+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.298 seconds
[2024-11-03T00:58:29.054+0000] {processor.py:186} INFO - Started process (PID=91) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:58:29.056+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:58:29.060+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:58:29.060+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:58:29.125+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:58:29.213+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:58:29.213+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:58:29.275+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:58:29.275+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:58:29.295+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.256 seconds
[2024-11-03T00:58:59.899+0000] {processor.py:186} INFO - Started process (PID=92) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:58:59.901+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:58:59.910+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:58:59.909+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:59:00.078+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:59:00.141+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:59:00.140+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:59:00.196+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:59:00.196+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:59:00.235+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.354 seconds
[2024-11-03T00:59:30.382+0000] {processor.py:186} INFO - Started process (PID=93) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:59:30.384+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T00:59:30.389+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:59:30.389+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:59:30.445+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T00:59:30.494+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:59:30.493+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T00:59:30.525+0000] {logging_mixin.py:190} INFO - [2024-11-03T00:59:30.525+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T00:59:30.554+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.199 seconds
[2024-11-03T01:00:00.762+0000] {processor.py:186} INFO - Started process (PID=94) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:00:00.765+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:00:00.771+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:00:00.771+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:00:00.828+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:00:00.898+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:00:00.897+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:00:00.928+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:00:00.928+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:00:00.946+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.203 seconds
[2024-11-03T01:00:31.375+0000] {processor.py:186} INFO - Started process (PID=95) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:00:31.377+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:00:31.382+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:00:31.381+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:00:31.452+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:00:31.492+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:00:31.491+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:00:31.530+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:00:31.530+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:00:31.561+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.199 seconds
[2024-11-03T01:01:01.860+0000] {processor.py:186} INFO - Started process (PID=96) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:01.862+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:01:01.868+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:01.868+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:01.936+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:02.005+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:02.005+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:01:02.037+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:02.037+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:01:02.055+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.212 seconds
[2024-11-03T01:01:13.186+0000] {processor.py:186} INFO - Started process (PID=97) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:13.189+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:01:13.199+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:13.198+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:13.278+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:13.351+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:13.351+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:01:13.381+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:13.381+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:01:13.401+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.256 seconds
[2024-11-03T01:01:43.641+0000] {processor.py:186} INFO - Started process (PID=98) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:43.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:01:43.651+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:43.650+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:43.717+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:01:43.774+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:43.774+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:01:43.808+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:01:43.808+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:01:43.827+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.214 seconds
[2024-11-03T01:02:14.224+0000] {processor.py:186} INFO - Started process (PID=99) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:02:14.234+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:02:14.241+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:02:14.241+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:02:14.315+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:02:14.387+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:02:14.387+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:02:14.424+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:02:14.424+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:02:14.441+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.238 seconds
[2024-11-03T01:02:44.721+0000] {processor.py:186} INFO - Started process (PID=100) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:02:44.722+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:02:44.728+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:02:44.728+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:02:44.836+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:02:44.879+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:02:44.879+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:02:44.921+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:02:44.920+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:02:44.937+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.233 seconds
[2024-11-03T01:03:15.373+0000] {processor.py:186} INFO - Started process (PID=101) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:03:15.375+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:03:15.395+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:03:15.393+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:03:15.461+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:03:15.507+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:03:15.507+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:03:15.563+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:03:15.562+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:03:15.579+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.219 seconds
[2024-11-03T01:03:45.881+0000] {processor.py:186} INFO - Started process (PID=102) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:03:45.891+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:03:45.897+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:03:45.897+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:03:45.966+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:03:46.036+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:03:46.036+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:03:46.068+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:03:46.068+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:03:46.093+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.239 seconds
[2024-11-03T01:04:16.360+0000] {processor.py:186} INFO - Started process (PID=103) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:04:16.362+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:04:16.377+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:04:16.376+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:04:16.446+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:04:16.504+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:04:16.504+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:04:16.536+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:04:16.536+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:04:16.554+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.209 seconds
[2024-11-03T01:04:47.019+0000] {processor.py:186} INFO - Started process (PID=104) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:04:47.022+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:04:47.047+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:04:47.046+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:04:47.119+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:04:47.179+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:04:47.179+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:04:47.209+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:04:47.208+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:04:47.226+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.234 seconds
[2024-11-03T01:05:17.508+0000] {processor.py:186} INFO - Started process (PID=105) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:05:17.511+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:05:17.517+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:05:17.516+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:05:17.617+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:05:17.692+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:05:17.692+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:05:17.727+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:05:17.727+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:05:17.746+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.255 seconds
[2024-11-03T01:05:48.049+0000] {processor.py:186} INFO - Started process (PID=106) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:05:48.051+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:05:48.056+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:05:48.056+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:05:48.125+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:05:48.171+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:05:48.170+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:05:48.221+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:05:48.221+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:05:48.241+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.212 seconds
[2024-11-03T01:06:18.656+0000] {processor.py:186} INFO - Started process (PID=107) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:06:18.658+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:06:18.664+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:06:18.663+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:06:18.769+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:06:18.840+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:06:18.840+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:06:18.872+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:06:18.872+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:06:18.891+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.249 seconds
[2024-11-03T01:06:49.146+0000] {processor.py:186} INFO - Started process (PID=108) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:06:49.148+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:06:49.153+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:06:49.153+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:06:49.252+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:06:49.308+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:06:49.307+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:06:49.351+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:06:49.351+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:06:49.370+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.262 seconds
[2024-11-03T01:07:19.811+0000] {processor.py:186} INFO - Started process (PID=109) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:07:19.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:07:19.831+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:07:19.830+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:07:19.887+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:07:19.969+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:07:19.968+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:07:20.003+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:07:20.003+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:07:20.020+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.238 seconds
[2024-11-03T01:07:50.304+0000] {processor.py:186} INFO - Started process (PID=110) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:07:50.307+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:07:50.311+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:07:50.310+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:07:50.370+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:07:50.434+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:07:50.434+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:07:50.469+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:07:50.469+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:07:50.491+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.217 seconds
[2024-11-03T01:08:21.019+0000] {processor.py:186} INFO - Started process (PID=111) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:08:21.022+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:08:21.026+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:08:21.026+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:08:21.100+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:08:21.151+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:08:21.151+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:08:21.188+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:08:21.188+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:08:21.215+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.213 seconds
[2024-11-03T01:08:51.555+0000] {processor.py:186} INFO - Started process (PID=112) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:08:51.557+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:08:51.564+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:08:51.563+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:08:51.639+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:08:51.698+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:08:51.697+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:08:51.735+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:08:51.735+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:08:51.754+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.217 seconds
[2024-11-03T01:09:22.181+0000] {processor.py:186} INFO - Started process (PID=113) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:09:22.184+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:09:22.189+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:09:22.188+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:09:22.266+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:09:22.313+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:09:22.313+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:09:22.354+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:09:22.354+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:09:22.374+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.206 seconds
[2024-11-03T01:09:52.796+0000] {processor.py:186} INFO - Started process (PID=114) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:09:52.798+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:09:52.803+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:09:52.802+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:09:52.895+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:09:53.039+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:09:53.039+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:09:53.081+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:09:53.080+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:09:53.101+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.319 seconds
[2024-11-03T01:10:23.274+0000] {processor.py:186} INFO - Started process (PID=115) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:10:23.277+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:10:23.282+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:10:23.281+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:10:23.364+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:10:23.437+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:10:23.436+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:10:23.469+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:10:23.469+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:10:23.500+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.244 seconds
[2024-11-03T01:10:53.824+0000] {processor.py:186} INFO - Started process (PID=116) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:10:53.826+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:10:53.831+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:10:53.830+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:10:53.923+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:10:53.977+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:10:53.976+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:10:54.008+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:10:54.007+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:10:54.035+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.225 seconds
[2024-11-03T01:11:24.434+0000] {processor.py:186} INFO - Started process (PID=117) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:11:24.436+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:11:24.440+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:11:24.440+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:11:24.523+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:11:24.584+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:11:24.584+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:11:24.643+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:11:24.642+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:11:24.661+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.240 seconds
[2024-11-03T01:11:54.996+0000] {processor.py:186} INFO - Started process (PID=118) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:11:54.998+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:11:55.005+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:11:55.004+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:11:55.086+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:11:55.136+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:11:55.136+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:11:55.172+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:11:55.172+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:11:55.189+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.217 seconds
[2024-11-03T01:12:25.563+0000] {processor.py:186} INFO - Started process (PID=119) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:12:25.565+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:12:25.570+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:12:25.569+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:12:25.674+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:12:25.720+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:12:25.719+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:12:25.755+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:12:25.754+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:12:25.774+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.223 seconds
[2024-11-03T01:12:56.014+0000] {processor.py:186} INFO - Started process (PID=120) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:12:56.017+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:12:56.024+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:12:56.023+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:12:56.081+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:12:56.135+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:12:56.135+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:12:56.172+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:12:56.172+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:12:56.190+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.207 seconds
[2024-11-03T01:13:26.716+0000] {processor.py:186} INFO - Started process (PID=121) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:13:26.719+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:13:26.727+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:13:26.726+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:13:26.797+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:13:26.864+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:13:26.864+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:13:26.897+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:13:26.896+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:13:26.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.225 seconds
[2024-11-03T01:13:57.182+0000] {processor.py:186} INFO - Started process (PID=122) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:13:57.185+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:13:57.191+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:13:57.191+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:13:57.266+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:13:57.331+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:13:57.331+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:13:57.362+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:13:57.362+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:13:57.381+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.235 seconds
[2024-11-03T01:14:27.738+0000] {processor.py:186} INFO - Started process (PID=123) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:14:27.739+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:14:27.744+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:14:27.743+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:14:27.821+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:14:27.859+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:14:27.859+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:14:27.891+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:14:27.890+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:14:27.909+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.183 seconds
[2024-11-03T01:14:58.129+0000] {processor.py:186} INFO - Started process (PID=124) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:14:58.131+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:14:58.137+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:14:58.137+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:14:58.223+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:14:58.278+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:14:58.278+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:14:58.310+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:14:58.310+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:14:58.329+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.213 seconds
[2024-11-03T01:15:28.724+0000] {processor.py:186} INFO - Started process (PID=125) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:15:28.726+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:15:28.730+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:15:28.730+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:15:28.824+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:15:28.869+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:15:28.869+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:15:28.908+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:15:28.908+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:15:28.927+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.218 seconds
[2024-11-03T01:15:59.201+0000] {processor.py:186} INFO - Started process (PID=126) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:15:59.203+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:15:59.208+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:15:59.208+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:15:59.318+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:15:59.353+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:15:59.352+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:15:59.392+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:15:59.392+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:15:59.412+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.234 seconds
[2024-11-03T01:16:29.742+0000] {processor.py:186} INFO - Started process (PID=127) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:16:29.744+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:16:29.757+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:16:29.755+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:16:29.856+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:16:29.911+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:16:29.911+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:16:29.939+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:16:29.939+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:16:29.960+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.233 seconds
[2024-11-03T01:17:00.288+0000] {processor.py:186} INFO - Started process (PID=128) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:17:00.295+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:17:00.305+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:17:00.305+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:17:00.393+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:17:00.435+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:17:00.435+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:17:00.484+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:17:00.483+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:17:00.502+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.230 seconds
[2024-11-03T01:17:30.735+0000] {processor.py:186} INFO - Started process (PID=129) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:17:30.738+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:17:30.743+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:17:30.742+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:17:30.810+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:17:30.842+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:17:30.842+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:17:30.866+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:17:30.865+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:17:30.883+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.159 seconds
[2024-11-03T01:18:01.484+0000] {processor.py:186} INFO - Started process (PID=130) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:18:01.487+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:18:01.495+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:18:01.494+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:18:01.602+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:18:01.655+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:18:01.654+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:18:01.709+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:18:01.709+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:18:01.732+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.289 seconds
[2024-11-03T01:18:31.917+0000] {processor.py:186} INFO - Started process (PID=131) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:18:31.920+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:18:31.925+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:18:31.924+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:18:32.018+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:18:32.058+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:18:32.058+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:18:32.104+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:18:32.104+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:18:32.119+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.217 seconds
[2024-11-03T01:19:02.516+0000] {processor.py:186} INFO - Started process (PID=132) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:19:02.518+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:19:02.525+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:19:02.524+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:19:02.615+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:19:02.658+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:19:02.658+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:19:02.690+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:19:02.690+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:19:02.709+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.207 seconds
[2024-11-03T01:19:32.957+0000] {processor.py:186} INFO - Started process (PID=133) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:19:32.960+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:19:32.966+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:19:32.966+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:19:33.066+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:19:33.102+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:19:33.102+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:19:33.131+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:19:33.131+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:19:33.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.220 seconds
[2024-11-03T01:20:03.600+0000] {processor.py:186} INFO - Started process (PID=134) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:20:03.610+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:20:03.621+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:20:03.620+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:20:03.690+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:20:03.752+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:20:03.751+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:20:03.782+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:20:03.782+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:20:03.802+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.221 seconds
[2024-11-03T01:20:34.058+0000] {processor.py:186} INFO - Started process (PID=135) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:20:34.061+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:20:34.065+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:20:34.065+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:20:34.145+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:20:34.193+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:20:34.193+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:20:34.224+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:20:34.224+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:20:34.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.197 seconds
[2024-11-03T01:21:04.673+0000] {processor.py:186} INFO - Started process (PID=136) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:21:04.685+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:21:04.703+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:21:04.703+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:21:04.770+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:21:04.805+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:21:04.805+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:21:04.846+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:21:04.845+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:21:04.861+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.202 seconds
[2024-11-03T01:21:35.148+0000] {processor.py:186} INFO - Started process (PID=137) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:21:35.153+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:21:35.158+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:21:35.158+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:21:35.243+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:21:35.280+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:21:35.279+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:21:35.331+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:21:35.331+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:21:35.354+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.224 seconds
[2024-11-03T01:22:05.808+0000] {processor.py:186} INFO - Started process (PID=138) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:22:05.810+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:22:05.816+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:22:05.816+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:22:05.886+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:22:05.935+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:22:05.935+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:22:05.976+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:22:05.976+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:22:05.994+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.202 seconds
[2024-11-03T01:22:36.280+0000] {processor.py:186} INFO - Started process (PID=139) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:22:36.283+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:22:36.289+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:22:36.289+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:22:36.354+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:22:36.347+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data
  File "/opt/airflow/etls/reddit_etl.py", line 31
    post_df['created_utc'] = pd.to_datetime(post_df['created_utc'], unit='s'])
                                                                            ^
SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
[2024-11-03T01:22:36.355+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:22:36.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.141 seconds
[2024-11-03T01:23:06.974+0000] {processor.py:186} INFO - Started process (PID=140) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:23:06.977+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:23:06.984+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:23:06.984+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:23:07.015+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:23:07.010+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 31
    post_df['created_utc'] = pd.to_datetime(post_df['created_utc'], unit='s'])
                                                                            ^
SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
[2024-11-03T01:23:07.016+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:23:07.051+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.110 seconds
[2024-11-03T01:23:37.433+0000] {processor.py:186} INFO - Started process (PID=141) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:23:37.435+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:23:37.440+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:23:37.440+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:23:37.461+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:23:37.459+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 31
    post_df['created_utc'] = pd.to_datetime(post_df['created_utc'], unit='s'])
                                                                            ^
SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
[2024-11-03T01:23:37.462+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:23:37.510+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-11-03T01:24:07.966+0000] {processor.py:186} INFO - Started process (PID=142) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:24:07.969+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:24:07.971+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:24:07.970+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:24:07.989+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:24:07.987+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 31
    post_df['created_utc'] = pd.to_datetime(post_df['created_utc'], unit='s'])
                                                                            ^
SyntaxError: closing parenthesis ']' does not match opening parenthesis '('
[2024-11-03T01:24:07.990+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:24:08.025+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.082 seconds
[2024-11-03T01:24:38.541+0000] {processor.py:186} INFO - Started process (PID=143) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:24:38.543+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:24:38.545+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:24:38.545+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:24:39.052+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:24:39.055+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:24:39.267+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:24:39.268+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:24:40.458+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:24:40.774+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:24:40.773+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:24:40.814+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:24:40.814+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:24:40.846+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 2.329 seconds
[2024-11-03T01:25:11.054+0000] {processor.py:186} INFO - Started process (PID=145) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:25:11.056+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:25:11.060+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:25:11.059+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:25:11.205+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:25:11.206+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:25:11.264+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:25:11.264+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:25:11.460+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:25:11.494+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:25:11.494+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:25:11.534+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:25:11.534+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:25:11.550+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.541 seconds
[2024-11-03T01:25:41.867+0000] {processor.py:186} INFO - Started process (PID=147) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:25:41.869+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:25:41.871+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:25:41.871+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:25:42.000+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:25:42.001+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:25:42.052+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:25:42.053+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:25:42.221+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:25:42.260+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:25:42.259+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:25:42.309+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:25:42.308+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:25:42.324+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.475 seconds
[2024-11-03T01:26:12.443+0000] {processor.py:186} INFO - Started process (PID=149) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:26:12.446+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:26:12.448+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:26:12.447+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:26:12.571+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:26:12.572+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:26:12.621+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:26:12.621+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:26:12.787+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:26:12.820+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:26:12.820+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:26:12.842+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:26:12.841+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:26:12.855+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.431 seconds
[2024-11-03T01:26:43.128+0000] {processor.py:186} INFO - Started process (PID=151) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:26:43.131+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:26:43.134+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:26:43.133+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:26:43.250+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:26:43.251+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:26:43.304+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:26:43.304+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:26:43.476+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:26:43.582+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:26:43.582+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:26:43.619+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:26:43.619+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:26:43.634+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.540 seconds
[2024-11-03T01:27:14.072+0000] {processor.py:186} INFO - Started process (PID=153) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:27:14.075+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:27:14.077+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:27:14.076+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:27:14.196+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:27:14.197+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:27:14.242+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:27:14.242+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:27:14.435+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:27:14.497+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:27:14.497+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:27:14.553+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:27:14.553+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:27:14.568+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.535 seconds
[2024-11-03T01:27:45.078+0000] {processor.py:186} INFO - Started process (PID=155) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:27:45.080+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:27:45.083+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:27:45.082+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:27:45.189+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:27:45.190+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:27:45.254+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:27:45.254+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:27:45.452+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:27:45.495+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:27:45.494+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:27:45.525+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:27:45.525+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:27:45.547+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.513 seconds
[2024-11-03T01:28:15.726+0000] {processor.py:186} INFO - Started process (PID=157) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:28:15.728+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:28:15.730+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:28:15.730+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:28:15.844+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:28:15.845+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:28:15.906+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:28:15.907+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:28:16.098+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:28:16.135+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:28:16.134+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:28:16.156+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:28:16.156+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:28:16.171+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.473 seconds
[2024-11-03T01:28:46.349+0000] {processor.py:186} INFO - Started process (PID=159) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:28:46.352+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:28:46.355+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:28:46.354+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:28:46.481+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:28:46.482+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:28:46.533+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:28:46.533+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:28:46.758+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:28:46.823+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:28:46.823+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:28:46.853+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:28:46.852+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:28:46.867+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.535 seconds
[2024-11-03T01:29:17.272+0000] {processor.py:186} INFO - Started process (PID=161) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:29:17.275+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:29:17.278+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:29:17.277+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:29:17.393+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:29:17.394+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:29:17.441+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:29:17.441+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:29:17.673+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:29:17.714+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:29:17.713+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:29:17.754+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:29:17.753+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:29:17.769+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.526 seconds
[2024-11-03T01:29:48.292+0000] {processor.py:186} INFO - Started process (PID=163) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:29:48.296+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:29:48.306+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:29:48.304+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:29:48.463+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:29:48.463+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:29:48.526+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:29:48.526+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:29:48.756+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:29:48.794+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:29:48.794+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:29:48.836+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:29:48.836+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:29:48.850+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.578 seconds
[2024-11-03T01:30:19.096+0000] {processor.py:186} INFO - Started process (PID=165) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:30:19.112+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:30:19.121+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:30:19.121+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:30:19.231+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:30:19.232+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:30:19.283+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:30:19.284+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:30:19.487+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:30:19.535+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:30:19.535+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:30:19.563+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:30:19.563+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:30:19.580+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.507 seconds
[2024-11-03T01:30:49.722+0000] {processor.py:186} INFO - Started process (PID=167) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:30:49.724+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:30:49.726+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:30:49.726+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:30:49.925+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:30:49.930+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:30:50.067+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:30:50.068+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:30:50.276+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:30:50.329+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:30:50.329+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:30:50.376+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:30:50.376+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:30:50.396+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.731 seconds
[2024-11-03T01:31:20.813+0000] {processor.py:186} INFO - Started process (PID=169) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:31:20.817+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T01:31:20.819+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:31:20.819+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:31:20.953+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:31:20.954+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:31:21.013+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T01:31:21.014+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T01:31:21.258+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T01:31:21.309+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:31:21.308+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T01:31:21.335+0000] {logging_mixin.py:190} INFO - [2024-11-03T01:31:21.335+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T01:31:21.355+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.560 seconds
[2024-11-03T05:47:03.989+0000] {processor.py:186} INFO - Started process (PID=171) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T05:47:03.992+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T05:47:03.994+0000] {logging_mixin.py:190} INFO - [2024-11-03T05:47:03.993+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T05:47:04.129+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T05:47:04.130+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T05:47:04.183+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T05:47:04.183+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T05:47:04.408+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T05:47:04.434+0000] {logging_mixin.py:190} INFO - [2024-11-03T05:47:04.434+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T05:47:04.455+0000] {logging_mixin.py:190} INFO - [2024-11-03T05:47:04.455+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T05:47:04.478+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.506 seconds
[2024-11-03T08:47:45.656+0000] {processor.py:186} INFO - Started process (PID=173) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T08:47:45.659+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T08:47:45.662+0000] {logging_mixin.py:190} INFO - [2024-11-03T08:47:45.661+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T08:47:45.777+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T08:47:45.777+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T08:47:45.841+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T08:47:45.841+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T08:47:46.029+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T08:47:46.054+0000] {logging_mixin.py:190} INFO - [2024-11-03T08:47:46.054+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T08:47:46.092+0000] {logging_mixin.py:190} INFO - [2024-11-03T08:47:46.091+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T08:47:46.107+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.492 seconds
[2024-11-03T11:44:21.747+0000] {processor.py:186} INFO - Started process (PID=175) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T11:44:21.749+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T11:44:21.752+0000] {logging_mixin.py:190} INFO - [2024-11-03T11:44:21.751+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T11:44:21.900+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T11:44:21.901+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T11:44:21.954+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T11:44:21.954+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T11:44:22.133+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T11:44:22.182+0000] {logging_mixin.py:190} INFO - [2024-11-03T11:44:22.181+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T11:44:22.213+0000] {logging_mixin.py:190} INFO - [2024-11-03T11:44:22.213+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T11:44:22.228+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.498 seconds
[2024-11-03T15:06:52.572+0000] {processor.py:186} INFO - Started process (PID=177) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:06:52.574+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T15:06:52.577+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:06:52.576+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:06:52.698+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:06:52.699+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:06:52.752+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:06:52.753+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:06:52.960+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:06:53.017+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:06:53.017+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T15:06:53.046+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:06:53.046+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T15:06:53.062+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.508 seconds
[2024-11-03T15:07:23.255+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:07:23.257+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T15:07:23.259+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:07:23.258+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:07:23.397+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:07:23.398+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:07:23.465+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:07:23.466+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:07:23.674+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:07:23.727+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:07:23.726+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T15:07:23.762+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:07:23.762+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T15:07:23.786+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.549 seconds
[2024-11-03T15:07:54.015+0000] {processor.py:186} INFO - Started process (PID=181) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:07:54.017+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T15:07:54.020+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:07:54.019+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:07:54.165+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:07:54.165+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:07:54.209+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:07:54.209+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:07:54.419+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:07:54.459+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:07:54.459+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T15:07:54.509+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:07:54.509+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T15:07:54.526+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.529 seconds
[2024-11-03T15:08:24.641+0000] {processor.py:186} INFO - Started process (PID=183) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:08:24.643+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T15:08:24.646+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:08:24.645+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:08:24.768+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:08:24.769+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:08:24.836+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T15:08:24.837+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T15:08:25.026+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T15:08:25.093+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:08:25.092+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T15:08:25.121+0000] {logging_mixin.py:190} INFO - [2024-11-03T15:08:25.121+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T15:08:25.137+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.515 seconds
[2024-11-03T16:06:40.586+0000] {processor.py:186} INFO - Started process (PID=185) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:06:40.588+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:06:40.590+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:06:40.589+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:06:40.720+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:06:40.721+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:06:40.785+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:06:40.786+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:06:41.004+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:06:41.142+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:06:41.141+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:06:41.240+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:06:41.239+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:06:41.286+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.733 seconds
[2024-11-03T16:07:11.834+0000] {processor.py:186} INFO - Started process (PID=187) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:07:11.836+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:07:11.838+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:07:11.837+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:07:11.991+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:07:11.992+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:07:12.052+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:07:12.054+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:07:12.430+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:07:12.522+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:07:12.522+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:07:12.568+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:07:12.567+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:07:12.594+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.794 seconds
[2024-11-03T16:07:43.141+0000] {processor.py:186} INFO - Started process (PID=189) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:07:43.144+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:07:43.147+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:07:43.146+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:07:43.274+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:07:43.274+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:07:43.318+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:07:43.318+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:07:43.508+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:07:43.547+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:07:43.547+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:07:43.583+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:07:43.582+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:07:43.603+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.499 seconds
[2024-11-03T16:08:14.059+0000] {processor.py:186} INFO - Started process (PID=191) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:08:14.061+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:08:14.065+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:08:14.064+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:08:14.192+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:08:14.193+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:08:14.239+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:08:14.239+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:08:14.429+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:08:14.487+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:08:14.487+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:08:14.512+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:08:14.512+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:08:14.529+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.521 seconds
[2024-11-03T16:08:44.978+0000] {processor.py:186} INFO - Started process (PID=193) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:08:44.980+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:08:44.982+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:08:44.982+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:08:45.119+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:08:45.120+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:08:45.170+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:08:45.170+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:08:45.353+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:08:45.410+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:08:45.410+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:08:45.438+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:08:45.438+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:08:45.455+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.514 seconds
[2024-11-03T16:09:15.905+0000] {processor.py:186} INFO - Started process (PID=195) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:09:15.909+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:09:15.919+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:09:15.916+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:09:16.029+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:09:16.030+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:09:16.080+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:09:16.081+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:09:16.273+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:09:16.331+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:09:16.331+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:09:16.358+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:09:16.358+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:09:16.374+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.503 seconds
[2024-11-03T16:09:46.818+0000] {processor.py:186} INFO - Started process (PID=197) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:09:46.822+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:09:46.824+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:09:46.824+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:09:46.947+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:09:46.948+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:09:46.994+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:09:46.994+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:09:47.180+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:09:47.241+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:09:47.241+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:09:47.271+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:09:47.270+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:09:47.287+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.495 seconds
[2024-11-03T16:10:17.797+0000] {processor.py:186} INFO - Started process (PID=199) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:10:17.799+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:10:17.811+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:10:17.810+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:10:17.938+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:10:17.939+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:10:17.989+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:10:17.989+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:10:18.200+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:10:18.260+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:10:18.260+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:10:18.291+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:10:18.290+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:10:18.318+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.540 seconds
[2024-11-03T16:10:48.654+0000] {processor.py:186} INFO - Started process (PID=201) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:10:48.657+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:10:48.659+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:10:48.658+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:10:48.788+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:10:48.789+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:10:48.839+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:10:48.839+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:10:49.078+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:10:49.143+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:10:49.142+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:10:49.173+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:10:49.173+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:10:49.187+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.550 seconds
[2024-11-03T16:11:19.655+0000] {processor.py:186} INFO - Started process (PID=203) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:11:19.656+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:11:19.658+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:11:19.658+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:11:19.781+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:11:19.782+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:11:19.830+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:11:19.831+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:11:20.050+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:11:20.121+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:11:20.121+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:11:20.152+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:11:20.151+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:11:20.179+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.543 seconds
[2024-11-03T16:11:50.664+0000] {processor.py:186} INFO - Started process (PID=205) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:11:50.666+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:11:50.668+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:11:50.667+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:11:50.774+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:11:50.775+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:11:50.824+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:11:50.825+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:11:51.047+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:11:51.089+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:11:51.089+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:11:51.114+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:11:51.114+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:11:51.141+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.521 seconds
[2024-11-03T16:12:21.644+0000] {processor.py:186} INFO - Started process (PID=207) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:12:21.646+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:12:21.648+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:12:21.648+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:12:21.780+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:12:21.781+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:12:21.846+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:12:21.846+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:12:22.065+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:12:22.122+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:12:22.122+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:12:22.152+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:12:22.152+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:12:22.168+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.550 seconds
[2024-11-03T16:12:52.536+0000] {processor.py:186} INFO - Started process (PID=209) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:12:52.542+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:12:52.546+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:12:52.545+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:12:52.665+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:12:52.666+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:12:52.715+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:12:52.715+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:12:52.963+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:12:53.005+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:12:53.004+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:12:53.062+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:12:53.062+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:12:53.085+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.578 seconds
[2024-11-03T16:13:23.600+0000] {processor.py:186} INFO - Started process (PID=211) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:13:23.602+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:13:23.604+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:13:23.603+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:13:23.733+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:13:23.734+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:13:23.802+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:13:23.802+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:13:24.008+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:13:24.051+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:13:24.051+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:13:24.078+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:13:24.078+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:13:24.093+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.510 seconds
[2024-11-03T16:13:54.301+0000] {processor.py:186} INFO - Started process (PID=213) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:13:54.303+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:13:54.305+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:13:54.305+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:13:54.404+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:13:54.405+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:13:54.457+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:13:54.457+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:13:54.651+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:13:54.688+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:13:54.687+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:13:54.725+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:13:54.725+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:13:54.740+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.453 seconds
[2024-11-03T16:14:25.047+0000] {processor.py:186} INFO - Started process (PID=215) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:14:25.050+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:14:25.052+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:14:25.051+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:14:25.180+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:14:25.181+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:14:25.231+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:14:25.231+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:14:25.425+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:14:25.492+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:14:25.491+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:14:25.519+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:14:25.519+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:14:25.535+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.507 seconds
[2024-11-03T16:14:55.702+0000] {processor.py:186} INFO - Started process (PID=217) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:14:55.705+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:14:55.707+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:14:55.707+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:14:55.822+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:14:55.831+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:14:55.885+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:14:55.886+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:14:56.064+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:14:56.125+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:14:56.124+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:14:56.153+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:14:56.153+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:14:56.170+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.488 seconds
[2024-11-03T16:15:26.374+0000] {processor.py:186} INFO - Started process (PID=219) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:15:26.377+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:15:26.380+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:15:26.379+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:15:26.506+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:15:26.506+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:15:26.564+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:15:26.565+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:15:26.739+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:15:26.798+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:15:26.797+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:15:26.827+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:15:26.826+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:15:26.851+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.501 seconds
[2024-11-03T16:15:57.188+0000] {processor.py:186} INFO - Started process (PID=221) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:15:57.190+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:15:57.192+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:15:57.191+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:15:57.309+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:15:57.310+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:15:57.352+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:15:57.353+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:15:57.620+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:15:57.686+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:15:57.686+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:15:57.723+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:15:57.723+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:15:57.741+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.570 seconds
[2024-11-03T16:16:28.318+0000] {processor.py:186} INFO - Started process (PID=223) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:16:28.321+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:16:28.323+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:16:28.322+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:16:28.471+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:16:28.477+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:16:28.559+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:16:28.559+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:16:28.746+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:16:28.881+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:16:28.880+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:16:29.003+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:16:29.002+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:16:29.026+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.743 seconds
[2024-11-03T16:16:59.192+0000] {processor.py:186} INFO - Started process (PID=225) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:16:59.195+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:16:59.198+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:16:59.197+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:16:59.328+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:16:59.329+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:16:59.377+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:16:59.378+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:16:59.578+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:16:59.617+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:16:59.616+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:16:59.665+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:16:59.665+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:16:59.681+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.510 seconds
[2024-11-03T16:17:29.936+0000] {processor.py:186} INFO - Started process (PID=227) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:17:29.939+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:17:29.941+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:17:29.941+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:17:30.071+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:17:30.072+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:17:30.118+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:17:30.118+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:17:30.357+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:17:30.399+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:17:30.399+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:17:30.432+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:17:30.432+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:17:30.450+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.534 seconds
[2024-11-03T16:18:00.876+0000] {processor.py:186} INFO - Started process (PID=229) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:18:00.878+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:18:00.881+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:18:00.880+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:18:01.005+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:18:01.006+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:18:01.059+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:18:01.059+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:18:01.265+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:18:01.324+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:18:01.323+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:18:01.355+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:18:01.355+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:18:01.371+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.521 seconds
[2024-11-03T16:18:31.862+0000] {processor.py:186} INFO - Started process (PID=231) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:18:31.864+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:18:31.866+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:18:31.866+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:18:31.990+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:18:31.991+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:18:32.055+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:18:32.055+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:18:32.288+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:18:32.351+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:18:32.351+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:18:32.384+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:18:32.384+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:18:32.400+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.556 seconds
[2024-11-03T16:19:02.700+0000] {processor.py:186} INFO - Started process (PID=233) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:19:02.703+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:19:02.706+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:19:02.705+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:19:02.831+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:19:02.832+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:19:02.890+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:19:02.890+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:19:03.068+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:19:03.124+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:19:03.124+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:19:03.154+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:19:03.153+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:19:03.168+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.487 seconds
[2024-11-03T16:19:33.594+0000] {processor.py:186} INFO - Started process (PID=235) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:19:33.605+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:19:33.609+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:19:33.608+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:19:33.727+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:19:33.730+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:19:33.786+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:19:33.787+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:19:34.003+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:19:34.061+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:19:34.060+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:19:34.112+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:19:34.112+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:19:34.128+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.563 seconds
[2024-11-03T16:20:04.562+0000] {processor.py:186} INFO - Started process (PID=237) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:20:04.564+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:20:04.566+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:20:04.566+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:20:04.685+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:20:04.686+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:20:04.744+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:20:04.745+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:20:04.946+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:20:05.060+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:20:05.060+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:20:05.093+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:20:05.093+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:20:05.107+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.565 seconds
[2024-11-03T16:20:35.603+0000] {processor.py:186} INFO - Started process (PID=239) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:20:35.611+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:20:35.614+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:20:35.613+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:20:35.733+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:20:35.735+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:20:35.789+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:20:35.789+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:20:35.984+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:20:36.030+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:20:36.029+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:20:36.061+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:20:36.061+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:20:36.078+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.498 seconds
[2024-11-03T16:21:06.410+0000] {processor.py:186} INFO - Started process (PID=241) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:21:06.412+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:21:06.415+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:21:06.414+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:21:06.540+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:21:06.541+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:21:06.586+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:21:06.587+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:21:06.769+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:21:06.822+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:21:06.822+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:21:06.850+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:21:06.850+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:21:06.866+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.487 seconds
[2024-11-03T16:21:37.268+0000] {processor.py:186} INFO - Started process (PID=243) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:21:37.270+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:21:37.272+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:21:37.272+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:21:37.399+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:21:37.400+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:21:37.456+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:21:37.457+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:21:37.622+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:21:37.684+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:21:37.684+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:21:37.713+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:21:37.713+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:21:37.730+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.481 seconds
[2024-11-03T16:22:08.176+0000] {processor.py:186} INFO - Started process (PID=245) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:22:08.179+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:22:08.181+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:22:08.181+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:22:08.327+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:22:08.329+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:22:08.377+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:22:08.377+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:22:08.563+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:22:08.630+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:22:08.630+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:22:08.659+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:22:08.659+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:22:08.675+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.517 seconds
[2024-11-03T16:22:39.125+0000] {processor.py:186} INFO - Started process (PID=247) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:22:39.128+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:22:39.130+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:22:39.129+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:22:39.243+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:22:39.244+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:22:39.293+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:22:39.294+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:22:39.462+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:22:39.520+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:22:39.519+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:22:39.552+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:22:39.552+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:22:39.569+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.459 seconds
[2024-11-03T16:23:10.103+0000] {processor.py:186} INFO - Started process (PID=249) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:23:10.107+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:23:10.110+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:23:10.109+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:23:10.326+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:23:10.328+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:23:10.492+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:23:10.495+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:23:10.970+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:23:11.084+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:23:11.083+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:23:11.135+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:23:11.135+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:23:11.188+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 1.126 seconds
[2024-11-03T16:23:41.590+0000] {processor.py:186} INFO - Started process (PID=251) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:23:41.592+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:23:41.595+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:23:41.594+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:23:41.716+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:23:41.717+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:23:41.774+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:23:41.775+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:23:41.952+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:23:41.996+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:23:41.996+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:23:42.033+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:23:42.032+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:23:42.048+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.477 seconds
[2024-11-03T16:24:12.660+0000] {processor.py:186} INFO - Started process (PID=253) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:24:12.663+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:24:12.665+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:24:12.665+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:24:12.802+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:24:12.803+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:24:12.852+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:24:12.853+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:24:13.046+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:24:13.086+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:24:13.086+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:24:13.128+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:24:13.128+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:24:13.146+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.507 seconds
[2024-11-03T16:24:43.495+0000] {processor.py:186} INFO - Started process (PID=255) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:24:43.499+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:24:43.501+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:24:43.501+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:24:43.624+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:24:43.625+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:24:43.676+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:24:43.676+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:24:43.888+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:24:43.930+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:24:43.930+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:24:43.967+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:24:43.967+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:24:43.988+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.514 seconds
[2024-11-03T16:25:14.458+0000] {processor.py:186} INFO - Started process (PID=257) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:25:14.460+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:25:14.464+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:25:14.463+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:25:14.607+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:25:14.608+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:25:14.653+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:25:14.653+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:25:14.823+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:25:14.880+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:25:14.880+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:25:14.911+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:25:14.911+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:25:14.927+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.489 seconds
[2024-11-03T16:25:45.362+0000] {processor.py:186} INFO - Started process (PID=259) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:25:45.365+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:25:45.367+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:25:45.367+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:25:45.510+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:25:45.511+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:25:45.566+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:25:45.566+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:25:45.768+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:25:45.808+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:25:45.808+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:25:45.837+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:25:45.837+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:25:45.854+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.508 seconds
[2024-11-03T16:26:16.323+0000] {processor.py:186} INFO - Started process (PID=261) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:26:16.325+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:26:16.327+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:26:16.327+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:26:16.446+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:26:16.447+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:26:16.498+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:26:16.499+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:26:16.706+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:26:16.744+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:26:16.744+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:26:16.791+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:26:16.791+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:26:16.808+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.503 seconds
[2024-11-03T16:26:47.208+0000] {processor.py:186} INFO - Started process (PID=263) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:26:47.226+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:26:47.239+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:26:47.236+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:26:47.351+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:26:47.354+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:26:47.411+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:26:47.411+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:26:47.616+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:26:47.654+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:26:47.653+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:26:47.680+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:26:47.680+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:26:47.694+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.506 seconds
[2024-11-03T16:27:18.209+0000] {processor.py:186} INFO - Started process (PID=265) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:27:18.212+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:27:18.215+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:27:18.214+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:27:18.330+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:27:18.331+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:27:18.380+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:27:18.381+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:27:18.568+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:27:18.609+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:27:18.609+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:27:18.642+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:27:18.642+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:27:18.666+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.497 seconds
[2024-11-03T16:27:49.208+0000] {processor.py:186} INFO - Started process (PID=267) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:27:49.211+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:27:49.214+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:27:49.213+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:27:49.346+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:27:49.347+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:27:49.398+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:27:49.399+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:27:49.584+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:27:49.631+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:27:49.630+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:27:49.659+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:27:49.659+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:27:49.674+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.485 seconds
[2024-11-03T16:28:20.120+0000] {processor.py:186} INFO - Started process (PID=269) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:28:20.131+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:28:20.133+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:28:20.132+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:28:20.255+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:28:20.256+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:28:20.304+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:28:20.304+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:28:20.583+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:28:20.643+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:28:20.642+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:28:20.672+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:28:20.671+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:28:20.687+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.588 seconds
[2024-11-03T16:28:51.142+0000] {processor.py:186} INFO - Started process (PID=271) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:28:51.145+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:28:51.147+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:28:51.146+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:28:51.274+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:28:51.274+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:28:51.319+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:28:51.319+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:28:51.502+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:28:51.542+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:28:51.542+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:28:51.584+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:28:51.584+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:28:51.600+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.474 seconds
[2024-11-03T16:29:22.086+0000] {processor.py:186} INFO - Started process (PID=273) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:29:22.094+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:29:22.097+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:29:22.097+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:29:22.220+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:29:22.223+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:29:22.275+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:29:22.276+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:29:22.471+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:29:22.520+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:29:22.520+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:29:22.552+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:29:22.551+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:29:22.568+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.519 seconds
[2024-11-03T16:29:52.995+0000] {processor.py:186} INFO - Started process (PID=275) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:29:52.998+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:29:53.000+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:29:52.999+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:29:53.108+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:29:53.108+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:29:53.179+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:29:53.179+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:29:53.354+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:29:53.412+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:29:53.412+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:29:53.440+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:29:53.440+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:29:53.456+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.505 seconds
[2024-11-03T16:30:23.920+0000] {processor.py:186} INFO - Started process (PID=277) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:30:23.923+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:30:23.926+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:30:23.925+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:30:24.044+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:30:24.045+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:30:24.102+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:30:24.102+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:30:24.319+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:30:24.356+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:30:24.356+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:30:24.382+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:30:24.381+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:30:24.396+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.516 seconds
[2024-11-03T16:30:54.791+0000] {processor.py:186} INFO - Started process (PID=279) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:30:54.793+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:30:54.795+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:30:54.795+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:30:54.918+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:30:54.919+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:30:54.978+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:30:54.979+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:30:55.198+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:30:55.254+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:30:55.254+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:30:55.297+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:30:55.297+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:30:55.328+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.564 seconds
[2024-11-03T16:31:25.755+0000] {processor.py:186} INFO - Started process (PID=281) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:31:25.766+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:31:25.771+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:31:25.771+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:31:25.901+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:31:25.902+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:31:25.965+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:31:25.966+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:31:26.194+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:31:26.233+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:31:26.232+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:31:26.269+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:31:26.268+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:31:26.283+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.555 seconds
[2024-11-03T16:31:56.692+0000] {processor.py:186} INFO - Started process (PID=283) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:31:56.696+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:31:56.698+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:31:56.697+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:31:56.790+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:31:56.791+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:31:56.866+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:31:56.867+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:31:57.091+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:31:57.120+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:31:57.120+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:31:57.164+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:31:57.164+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:31:57.179+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.514 seconds
[2024-11-03T16:32:27.587+0000] {processor.py:186} INFO - Started process (PID=285) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:32:27.590+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:32:27.592+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:32:27.592+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:32:27.699+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:32:27.701+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:32:27.782+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:32:27.783+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:32:28.001+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:32:28.034+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:32:28.034+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:32:28.060+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:32:28.060+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:32:28.075+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.530 seconds
[2024-11-03T16:32:58.528+0000] {processor.py:186} INFO - Started process (PID=287) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:32:58.531+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:32:58.533+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:32:58.532+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:32:58.644+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:32:58.645+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:32:58.699+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:32:58.699+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:32:58.933+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:32:58.965+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:32:58.964+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:32:58.987+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:32:58.987+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:32:59.001+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.506 seconds
[2024-11-03T16:33:29.428+0000] {processor.py:186} INFO - Started process (PID=289) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:33:29.431+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:33:29.433+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:33:29.432+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:33:29.541+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:33:29.542+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:33:29.594+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:33:29.594+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:33:29.811+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:33:29.845+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:33:29.845+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:33:29.882+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:33:29.882+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:33:29.900+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.484 seconds
[2024-11-03T16:34:00.325+0000] {processor.py:186} INFO - Started process (PID=291) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:34:00.328+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:34:00.331+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:34:00.330+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:34:00.455+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:34:00.459+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:34:00.525+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:34:00.526+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:34:00.726+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:34:00.759+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:34:00.759+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:34:00.783+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:34:00.783+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:34:00.797+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-11-03T16:34:31.243+0000] {processor.py:186} INFO - Started process (PID=293) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:34:31.245+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:34:31.247+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:34:31.247+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:34:31.371+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:34:31.372+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:34:31.435+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:34:31.436+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:34:31.655+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:34:31.690+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:34:31.689+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:34:31.721+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:34:31.720+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:34:31.752+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.522 seconds
[2024-11-03T16:35:02.265+0000] {processor.py:186} INFO - Started process (PID=295) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:35:02.269+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:35:02.272+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:35:02.271+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:35:02.394+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:35:02.395+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:35:02.467+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:35:02.468+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:35:02.685+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:35:02.726+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:35:02.726+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:35:02.750+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:35:02.750+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:35:02.764+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.520 seconds
[2024-11-03T16:35:33.153+0000] {processor.py:186} INFO - Started process (PID=297) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:35:33.156+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:35:33.159+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:35:33.158+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:35:33.250+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:35:33.251+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:35:33.330+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:35:33.330+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:35:33.550+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:35:33.612+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:35:33.611+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:35:33.636+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:35:33.636+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:35:33.652+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.530 seconds
[2024-11-03T16:36:04.139+0000] {processor.py:186} INFO - Started process (PID=299) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:36:04.140+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:36:04.142+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:36:04.142+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:36:04.280+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:36:04.281+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:36:04.334+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:36:04.335+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:36:04.530+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:36:04.566+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:36:04.566+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:36:04.600+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:36:04.600+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:36:04.619+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.494 seconds
[2024-11-03T16:36:35.148+0000] {processor.py:186} INFO - Started process (PID=301) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:36:35.150+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:36:35.152+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:36:35.151+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:36:35.289+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:36:35.290+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:36:35.358+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:36:35.358+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:36:35.565+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:36:35.620+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:36:35.620+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:36:35.643+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:36:35.643+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:36:35.656+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.525 seconds
[2024-11-03T16:37:06.156+0000] {processor.py:186} INFO - Started process (PID=303) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:37:06.158+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:37:06.170+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:37:06.167+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:37:06.312+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:37:06.314+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:37:06.370+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:37:06.370+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:37:06.574+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:37:06.607+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:37:06.607+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:37:06.633+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:37:06.633+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:37:06.646+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.505 seconds
[2024-11-03T16:37:36.864+0000] {processor.py:186} INFO - Started process (PID=305) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:37:36.866+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:37:36.868+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:37:36.868+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:37:36.980+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:37:36.982+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:37:37.052+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:37:37.053+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:37:37.274+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:37:37.311+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:37:37.311+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:37:37.334+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:37:37.334+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:37:37.348+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.502 seconds
[2024-11-03T16:38:07.490+0000] {processor.py:186} INFO - Started process (PID=307) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:38:07.491+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:38:07.494+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:38:07.493+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:38:07.633+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:38:07.634+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:38:07.718+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:38:07.718+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:38:07.915+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:38:07.969+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:38:07.968+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:38:07.993+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:38:07.993+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:38:08.010+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.536 seconds
[2024-11-03T16:38:38.328+0000] {processor.py:186} INFO - Started process (PID=309) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:38:38.332+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:38:38.335+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:38:38.334+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:38:38.494+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:38:38.495+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:38:38.596+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:38:38.597+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:38:38.837+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:38:38.872+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:38:38.871+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:38:38.899+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:38:38.899+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:38:38.916+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.619 seconds
[2024-11-03T16:39:09.493+0000] {processor.py:186} INFO - Started process (PID=311) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:39:09.495+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:39:09.498+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:39:09.497+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:39:09.646+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:39:09.646+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:39:09.703+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:39:09.704+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:39:09.924+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:39:09.963+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:39:09.963+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:39:10.007+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:39:10.007+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:39:10.024+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.614 seconds
[2024-11-03T16:39:40.455+0000] {processor.py:186} INFO - Started process (PID=313) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:39:40.457+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:39:40.466+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:39:40.463+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:39:40.589+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:39:40.592+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:39:40.652+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:39:40.653+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:39:40.892+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:39:40.936+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:39:40.935+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:39:40.971+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:39:40.970+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:39:40.985+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.546 seconds
[2024-11-03T16:40:11.401+0000] {processor.py:186} INFO - Started process (PID=315) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:40:11.404+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:40:11.406+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:40:11.405+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:40:11.524+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:40:11.525+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:40:11.590+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:40:11.591+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:40:11.812+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:40:11.868+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:40:11.867+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:40:11.897+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:40:11.896+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:40:11.915+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.532 seconds
[2024-11-03T16:40:42.311+0000] {processor.py:186} INFO - Started process (PID=317) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:40:42.314+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:40:42.316+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:40:42.316+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:40:42.444+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:40:42.444+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:40:42.508+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:40:42.509+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:40:42.725+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:40:42.762+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:40:42.761+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:40:42.789+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:40:42.789+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:40:42.821+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.525 seconds
[2024-11-03T16:41:13.299+0000] {processor.py:186} INFO - Started process (PID=319) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:41:13.302+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:41:13.304+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:41:13.304+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:41:13.429+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:41:13.431+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:41:13.491+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:41:13.492+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:41:13.712+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:41:13.752+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:41:13.752+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:41:13.775+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:41:13.775+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:41:13.790+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.527 seconds
[2024-11-03T16:41:44.244+0000] {processor.py:186} INFO - Started process (PID=321) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:41:44.246+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:41:44.249+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:41:44.248+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:41:44.351+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:41:44.351+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:41:44.414+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:41:44.415+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:41:44.621+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:41:44.665+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:41:44.664+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:41:44.700+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:41:44.700+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:41:44.717+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.489 seconds
[2024-11-03T16:42:15.166+0000] {processor.py:186} INFO - Started process (PID=323) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:42:15.168+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:42:15.171+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:42:15.170+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:42:15.291+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:42:15.291+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:42:15.360+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:42:15.361+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:42:15.582+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:42:15.623+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:42:15.622+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:42:15.670+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:42:15.669+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:42:15.687+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.540 seconds
[2024-11-03T16:42:46.074+0000] {processor.py:186} INFO - Started process (PID=325) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:42:46.078+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:42:46.083+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:42:46.081+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:42:46.205+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:42:46.207+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:42:46.262+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:42:46.262+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:42:46.471+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:42:46.523+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:42:46.523+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:42:46.551+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:42:46.551+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:42:46.569+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.535 seconds
[2024-11-03T16:43:17.042+0000] {processor.py:186} INFO - Started process (PID=327) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:43:17.045+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:43:17.047+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:43:17.046+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:43:17.171+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:43:17.172+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:43:17.239+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:43:17.240+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:43:17.461+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:43:17.522+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:43:17.522+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:43:17.558+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:43:17.558+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:43:17.575+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.571 seconds
[2024-11-03T16:43:47.958+0000] {processor.py:186} INFO - Started process (PID=329) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:43:47.960+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:43:47.962+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:43:47.962+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:43:48.074+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:43:48.075+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:43:48.127+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:43:48.128+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:43:48.360+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:43:48.400+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:43:48.399+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:43:48.444+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:43:48.443+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:43:48.460+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.516 seconds
[2024-11-03T16:44:18.891+0000] {processor.py:186} INFO - Started process (PID=331) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:44:18.894+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:44:18.897+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:44:18.896+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:44:19.012+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:44:19.014+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:44:19.074+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:44:19.075+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:44:19.315+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:44:19.386+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:44:19.385+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:44:19.410+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:44:19.410+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:44:19.426+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.572 seconds
[2024-11-03T16:44:49.794+0000] {processor.py:186} INFO - Started process (PID=333) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:44:49.798+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:44:49.799+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:44:49.799+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:44:49.929+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:44:49.930+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:44:49.984+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:44:49.985+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:44:50.195+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:44:50.250+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:44:50.250+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:44:50.280+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:44:50.280+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:44:50.294+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.516 seconds
[2024-11-03T16:45:20.778+0000] {processor.py:186} INFO - Started process (PID=335) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:45:20.781+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:45:20.784+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:45:20.783+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:45:20.927+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:45:20.928+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:45:20.984+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:45:20.985+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:45:21.240+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:45:21.328+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:45:21.327+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:45:21.402+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:45:21.402+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:45:21.430+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.671 seconds
[2024-11-03T16:45:51.671+0000] {processor.py:186} INFO - Started process (PID=337) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:45:51.674+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:45:51.678+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:45:51.677+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:45:51.800+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:45:51.801+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:45:51.872+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:45:51.873+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:45:52.081+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:45:52.137+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:45:52.136+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:45:52.163+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:45:52.163+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:45:52.178+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.525 seconds
[2024-11-03T16:46:22.614+0000] {processor.py:186} INFO - Started process (PID=339) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:46:22.623+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:46:22.629+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:46:22.628+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:46:22.748+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:46:22.749+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:46:22.802+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:46:22.803+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:46:23.043+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:46:23.083+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:46:23.082+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:46:23.110+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:46:23.110+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:46:23.127+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.547 seconds
[2024-11-03T16:46:53.681+0000] {processor.py:186} INFO - Started process (PID=341) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:46:53.685+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:46:53.688+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:46:53.687+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:46:53.820+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:46:53.821+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:46:53.883+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:46:53.883+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:46:54.104+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:46:54.139+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:46:54.139+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:46:54.176+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:46:54.175+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:46:54.192+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.550 seconds
[2024-11-03T16:47:24.415+0000] {processor.py:186} INFO - Started process (PID=343) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:47:24.418+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:47:24.420+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:47:24.420+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:47:24.547+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:47:24.552+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:47:24.614+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:47:24.614+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:47:24.802+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:47:24.852+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:47:24.852+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:47:24.875+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:47:24.875+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:47:24.888+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-11-03T16:47:54.996+0000] {processor.py:186} INFO - Started process (PID=345) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:47:54.999+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:47:55.001+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:47:55.000+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:47:55.148+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:47:55.149+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:47:55.203+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:47:55.203+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:47:55.401+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:47:55.459+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:47:55.459+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:47:55.487+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:47:55.487+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:47:55.505+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.525 seconds
[2024-11-03T16:48:25.707+0000] {processor.py:186} INFO - Started process (PID=347) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:48:25.710+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:48:25.712+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:48:25.711+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:48:25.838+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:48:25.839+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:48:25.917+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:48:25.918+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:48:26.138+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:48:26.172+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:48:26.172+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:48:26.203+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:48:26.202+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:48:26.226+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.536 seconds
[2024-11-03T16:48:56.366+0000] {processor.py:186} INFO - Started process (PID=349) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:48:56.369+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:48:56.372+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:48:56.371+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:48:56.484+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:48:56.485+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:48:56.571+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:48:56.572+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:48:56.779+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:48:56.812+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:48:56.811+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:48:56.834+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:48:56.833+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:48:56.848+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.515 seconds
[2024-11-03T16:49:27.003+0000] {processor.py:186} INFO - Started process (PID=351) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:49:27.004+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:49:27.006+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:49:27.006+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:49:27.157+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:49:27.158+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:49:27.238+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:49:27.239+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:49:27.453+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:49:27.489+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:49:27.489+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:49:27.512+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:49:27.512+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:49:27.529+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.542 seconds
[2024-11-03T16:49:57.703+0000] {processor.py:186} INFO - Started process (PID=353) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:49:57.705+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:49:57.707+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:49:57.707+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:49:57.858+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:49:57.858+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:49:57.910+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:49:57.910+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:49:58.113+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:49:58.158+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:49:58.157+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:49:58.183+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:49:58.182+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:49:58.196+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.505 seconds
[2024-11-03T16:50:28.636+0000] {processor.py:186} INFO - Started process (PID=355) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:50:28.644+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:50:28.653+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:50:28.650+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:50:28.801+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:50:28.802+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:50:28.857+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:50:28.858+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:50:29.047+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:50:29.078+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:50:29.077+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:50:29.099+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:50:29.099+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:50:29.112+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-11-03T16:50:59.606+0000] {processor.py:186} INFO - Started process (PID=357) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:50:59.608+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:50:59.610+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:50:59.610+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:50:59.747+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:50:59.748+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:50:59.809+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:50:59.810+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:51:00.019+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:51:00.060+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:51:00.060+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:51:00.083+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:51:00.082+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:51:00.097+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.524 seconds
[2024-11-03T16:51:30.471+0000] {processor.py:186} INFO - Started process (PID=359) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:51:30.474+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:51:30.475+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:51:30.475+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:51:30.606+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:51:30.607+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:51:30.695+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:51:30.696+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:51:30.892+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:51:30.923+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:51:30.923+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:51:30.946+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:51:30.946+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:51:30.961+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.506 seconds
[2024-11-03T16:52:01.335+0000] {processor.py:186} INFO - Started process (PID=361) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:52:01.338+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:52:01.340+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:52:01.339+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:52:01.467+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:52:01.468+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:52:01.549+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:52:01.550+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:52:01.766+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:52:01.802+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:52:01.801+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:52:01.825+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:52:01.825+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:52:01.856+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.561 seconds
[2024-11-03T16:52:32.379+0000] {processor.py:186} INFO - Started process (PID=363) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:52:32.383+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:52:32.386+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:52:32.386+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:52:32.516+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:52:32.517+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:52:32.580+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:52:32.581+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:52:32.761+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:52:32.816+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:52:32.816+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:52:32.838+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:52:32.838+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:52:32.853+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-11-03T16:53:03.061+0000] {processor.py:186} INFO - Started process (PID=365) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:53:03.063+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:53:03.065+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:53:03.065+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:53:03.199+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:53:03.201+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:53:03.259+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:53:03.260+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:53:03.470+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:53:03.522+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:53:03.521+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:53:03.548+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:53:03.547+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:53:03.562+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.517 seconds
[2024-11-03T16:53:34.032+0000] {processor.py:186} INFO - Started process (PID=367) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:53:34.036+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:53:34.040+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:53:34.039+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:53:34.180+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:53:34.181+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:53:34.250+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:53:34.250+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:53:34.428+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:53:34.462+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:53:34.462+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:53:34.484+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:53:34.484+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:53:34.498+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.490 seconds
[2024-11-03T16:54:04.992+0000] {processor.py:186} INFO - Started process (PID=369) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:54:04.995+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:54:04.997+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:54:04.997+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:54:05.146+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:54:05.148+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:54:05.239+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:54:05.240+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:54:05.458+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:54:05.494+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:54:05.493+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:54:05.516+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:54:05.515+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:54:05.537+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.596 seconds
[2024-11-03T16:54:35.859+0000] {processor.py:186} INFO - Started process (PID=371) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:54:35.861+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:54:35.863+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:54:35.863+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:54:35.967+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:54:35.968+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:54:36.017+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:54:36.017+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:54:36.195+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:54:36.222+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:54:36.222+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:54:36.254+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:54:36.254+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:54:36.273+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.430 seconds
[2024-11-03T16:55:06.738+0000] {processor.py:186} INFO - Started process (PID=373) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:55:06.740+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:55:06.741+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:55:06.741+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:55:06.873+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:55:06.874+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:55:06.950+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:55:06.951+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:55:07.146+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:55:07.182+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:55:07.182+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:55:07.208+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:55:07.208+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:55:07.221+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.498 seconds
[2024-11-03T16:55:37.660+0000] {processor.py:186} INFO - Started process (PID=375) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:55:37.663+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:55:37.665+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:55:37.664+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:55:37.794+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:55:37.795+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:55:37.873+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:55:37.874+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:55:38.110+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:55:38.155+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:55:38.155+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:55:38.179+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:55:38.179+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:55:38.194+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.548 seconds
[2024-11-03T16:56:08.663+0000] {processor.py:186} INFO - Started process (PID=377) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:56:08.666+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:56:08.668+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:56:08.667+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:56:08.802+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:56:08.803+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:56:08.881+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:56:08.881+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:56:09.068+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:56:09.113+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:56:09.112+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:56:09.147+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:56:09.147+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:56:09.163+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.516 seconds
[2024-11-03T16:56:39.505+0000] {processor.py:186} INFO - Started process (PID=379) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:56:39.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:56:39.510+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:56:39.509+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:56:39.637+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:56:39.638+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:56:39.715+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:56:39.715+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:56:39.923+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:56:39.954+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:56:39.953+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:56:39.975+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:56:39.975+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:56:39.989+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.501 seconds
[2024-11-03T16:57:10.453+0000] {processor.py:186} INFO - Started process (PID=381) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:57:10.456+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:57:10.457+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:57:10.457+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:57:10.577+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:57:10.578+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:57:10.644+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:57:10.644+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:57:10.831+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:57:10.877+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:57:10.876+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:57:10.899+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:57:10.899+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:57:10.913+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.479 seconds
[2024-11-03T16:57:41.378+0000] {processor.py:186} INFO - Started process (PID=383) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:57:41.381+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:57:41.382+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:57:41.382+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:57:41.511+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:57:41.512+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:57:41.573+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:57:41.573+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:57:41.790+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:57:41.827+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:57:41.827+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:57:41.849+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:57:41.849+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:57:41.862+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.513 seconds
[2024-11-03T16:58:12.243+0000] {processor.py:186} INFO - Started process (PID=385) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:58:12.245+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:58:12.250+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:58:12.249+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:58:12.388+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:58:12.390+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:58:12.455+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:58:12.455+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:58:12.671+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:58:12.709+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:58:12.709+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:58:12.732+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:58:12.732+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:58:12.756+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.527 seconds
[2024-11-03T16:58:43.218+0000] {processor.py:186} INFO - Started process (PID=387) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:58:43.221+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:58:43.223+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:58:43.223+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:58:43.331+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:58:43.332+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:58:43.386+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:58:43.387+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:58:43.578+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:58:43.611+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:58:43.611+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:58:43.632+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:58:43.632+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:58:43.645+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.444 seconds
[2024-11-03T16:59:14.112+0000] {processor.py:186} INFO - Started process (PID=389) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:59:14.114+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:59:14.117+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:59:14.116+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:59:14.233+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:59:14.234+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:59:14.301+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:59:14.302+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:59:14.502+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:59:14.547+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:59:14.547+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:59:14.571+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:59:14.571+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:59:14.586+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.494 seconds
[2024-11-03T16:59:45.014+0000] {processor.py:186} INFO - Started process (PID=391) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:59:45.016+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T16:59:45.019+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:59:45.018+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:59:45.146+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:59:45.147+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:59:45.208+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T16:59:45.209+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T16:59:45.404+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T16:59:45.433+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:59:45.433+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T16:59:45.455+0000] {logging_mixin.py:190} INFO - [2024-11-03T16:59:45.454+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T16:59:45.468+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.466 seconds
[2024-11-03T17:00:15.919+0000] {processor.py:186} INFO - Started process (PID=393) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:00:15.926+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:00:15.928+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:00:15.927+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:00:16.010+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:00:16.010+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:00:16.046+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:00:16.046+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:00:16.187+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:00:16.210+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:00:16.210+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:00:16.227+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:00:16.227+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:00:16.240+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.341 seconds
[2024-11-03T17:00:46.309+0000] {processor.py:186} INFO - Started process (PID=395) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:00:46.311+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:00:46.313+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:00:46.312+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:00:46.432+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:00:46.433+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:00:46.497+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:00:46.498+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:00:46.717+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:00:46.754+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:00:46.754+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:00:46.779+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:00:46.779+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:00:46.793+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.493 seconds
[2024-11-03T17:01:17.029+0000] {processor.py:186} INFO - Started process (PID=397) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:01:17.030+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:01:17.032+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:01:17.031+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:01:17.114+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:01:17.115+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:01:17.166+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:01:17.167+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:01:17.325+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:01:17.350+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:01:17.349+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:01:17.368+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:01:17.367+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:01:17.380+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.362 seconds
[2024-11-03T17:01:47.568+0000] {processor.py:186} INFO - Started process (PID=399) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:01:47.570+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:01:47.590+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:01:47.588+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:01:47.703+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:01:47.704+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:01:47.758+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:01:47.759+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:01:47.965+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:01:48.018+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:01:48.018+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:01:48.041+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:01:48.041+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:01:48.055+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.504 seconds
[2024-11-03T17:02:18.203+0000] {processor.py:186} INFO - Started process (PID=401) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:02:18.204+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:02:18.206+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:02:18.206+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:02:18.324+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:02:18.325+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:02:18.391+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:02:18.392+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:02:18.574+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:02:18.607+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:02:18.606+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:02:18.633+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:02:18.633+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:02:18.647+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.459 seconds
[2024-11-03T17:02:48.925+0000] {processor.py:186} INFO - Started process (PID=403) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:02:48.928+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:02:48.929+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:02:48.929+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:02:49.054+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:02:49.055+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:02:49.117+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:02:49.118+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:02:49.364+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:02:49.432+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:02:49.431+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:02:49.458+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:02:49.458+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:02:49.473+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.567 seconds
[2024-11-03T17:03:19.929+0000] {processor.py:186} INFO - Started process (PID=405) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:03:19.939+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:03:19.942+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:03:19.941+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:03:20.052+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:03:20.053+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:03:20.124+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:03:20.125+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:03:20.337+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:03:20.369+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:03:20.369+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:03:20.391+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:03:20.391+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:03:20.406+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.499 seconds
[2024-11-03T17:03:50.671+0000] {processor.py:186} INFO - Started process (PID=407) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:03:50.673+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:03:50.675+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:03:50.674+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:03:50.805+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:03:50.806+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:03:50.859+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:03:50.860+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:03:51.103+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:03:51.144+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:03:51.143+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:03:51.168+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:03:51.168+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:03:51.181+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.526 seconds
[2024-11-03T17:04:21.639+0000] {processor.py:186} INFO - Started process (PID=409) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:04:21.643+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:04:21.645+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:04:21.644+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:04:21.762+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:04:21.762+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:04:21.835+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:04:21.836+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:04:22.037+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:04:22.073+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:04:22.072+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:04:22.095+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:04:22.095+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:04:22.108+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.488 seconds
[2024-11-03T17:04:52.594+0000] {processor.py:186} INFO - Started process (PID=411) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:04:52.596+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:04:52.598+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:04:52.597+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:04:52.735+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:04:52.736+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:04:52.795+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:04:52.795+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:04:53.010+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:04:53.044+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:04:53.044+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:04:53.065+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:04:53.065+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:04:53.078+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.502 seconds
[2024-11-03T17:05:23.590+0000] {processor.py:186} INFO - Started process (PID=413) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:05:23.591+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:05:23.594+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:05:23.593+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:05:23.691+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:05:23.691+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:05:23.743+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:05:23.743+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:05:23.936+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:05:23.969+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:05:23.969+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:05:23.994+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:05:23.994+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:05:24.008+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.434 seconds
[2024-11-03T17:05:54.516+0000] {processor.py:186} INFO - Started process (PID=415) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:05:54.519+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:05:54.521+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:05:54.520+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:05:54.642+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:05:54.643+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:05:54.698+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:05:54.698+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:05:54.919+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:05:54.953+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:05:54.952+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:05:54.991+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:05:54.991+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:05:55.008+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.504 seconds
[2024-11-03T17:06:25.517+0000] {processor.py:186} INFO - Started process (PID=417) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:06:25.519+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:06:25.521+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:06:25.521+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:06:25.651+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:06:25.652+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:06:25.711+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:06:25.712+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:06:25.938+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:06:25.998+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:06:25.998+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:06:26.021+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:06:26.021+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:06:26.035+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.536 seconds
[2024-11-03T17:06:56.598+0000] {processor.py:186} INFO - Started process (PID=419) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:06:56.600+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:06:56.602+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:06:56.602+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:06:56.719+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:06:56.720+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:06:56.783+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:06:56.784+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:06:56.970+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:06:57.003+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:06:57.003+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:06:57.025+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:06:57.025+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:06:57.038+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.457 seconds
[2024-11-03T17:07:27.558+0000] {processor.py:186} INFO - Started process (PID=421) to work on /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:07:27.564+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-11-03T17:07:27.578+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:07:27.576+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:07:27.717+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:07:27.718+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:07:27.770+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 173, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 602, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 341, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 413, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 3, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, transform_data, load_data_to_csv
  File "/opt/airflow/etls/reddit_etl.py", line 5, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-11-03T17:07:27.770+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-11-03T17:07:27.969+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-11-03T17:07:28.014+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:07:28.014+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2024-11-03T17:07:28.040+0000] {logging_mixin.py:190} INFO - [2024-11-03T17:07:28.040+0000] {dag.py:4180} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-11-03 00:00:00+00:00, run_after=2024-11-04 00:00:00+00:00
[2024-11-03T17:07:28.053+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.509 seconds
